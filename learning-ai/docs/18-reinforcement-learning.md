# ğŸ¤– Reinforcement Learning (RL) - Há»c tÄƒng cÆ°á»ng

> **Má»¥c tiÃªu**: Hiá»ƒu sÃ¢u vá» cÃ¡c khÃ¡i niá»‡m cá»‘t lÃµi cá»§a Há»c tÄƒng cÆ°á»ng, tá»« ná»n táº£ng lÃ½ thuyáº¿t (MDPs, Bellman) Ä‘áº¿n cÃ¡c thuáº­t toÃ¡n kinh Ä‘iá»ƒn (Q-learning, Policy Gradients) vÃ  á»©ng dá»¥ng thá»±c táº¿.

## ğŸ“‹ Tá»•ng quan ná»™i dung

```mermaid
graph TD
    A[ğŸ¤– Reinforcement Learning] --> B[ğŸ§  Ná»n táº£ng lÃ½ thuyáº¿t]
    A --> C[âš™ï¸ Thuáº­t toÃ¡n Value-Based]
    A --> D[ğŸ“ˆ Thuáº­t toÃ¡n Policy-Based]
    A --> E[ğŸ­ Thuáº­t toÃ¡n Actor-Critic]
    A --> F[ğŸŒ á»¨ng dá»¥ng]
    
    B --> B1[Markov Decision Process (MDP)]
    B --> B2[Bellman Equations]
    B --> B3[Value & Policy Iteration]
    B --> B4[Exploration vs. Exploitation]
    
    C --> C1[Q-Learning]
    C --> C2[Deep Q-Networks (DQN)]
    C --> C3[Double DQN & Dueling DQN]
    
    D --> D1[Policy Gradients]
    D --> D2[REINFORCE]
    
    E --> E1[Advantage Actor-Critic (A2C)]
    E --> E2[Asynchronous A3C]
    E --> E3[Proximal Policy Optimization (PPO)]
    
    F --> F1[ChÆ¡i game (Atari, AlphaGo)]
    F --> F2[Robotics]
    F --> F3[Tá»‘i Æ°u hÃ³a tÃ i nguyÃªn]
    F --> F4[Há»‡ thá»‘ng gá»£i Ã½]
```

## ğŸ“š 1. Báº£ng kÃ½ hiá»‡u (Notation)

- **Agent**: TÃ¡c nhÃ¢n, thá»±c thá»ƒ ra quyáº¿t Ä‘á»‹nh.
- **Environment**: MÃ´i trÆ°á»ng, nÆ¡i agent tÆ°Æ¡ng tÃ¡c.
- **State ($s \in S$)**: Tráº¡ng thÃ¡i cá»§a mÃ´i trÆ°á»ng.
- **Action ($a \in A$)**: HÃ nh Ä‘á»™ng mÃ  agent cÃ³ thá»ƒ thá»±c hiá»‡n.
- **Reward ($r$)**: Pháº§n thÆ°á»Ÿng (hoáº·c pháº¡t) mÃ  agent nháº­n Ä‘Æ°á»£c tá»« mÃ´i trÆ°á»ng.
- **Policy ($\pi(a|s)$)**: ChÃ­nh sÃ¡ch, chiáº¿n lÆ°á»£c cá»§a agent. ÄÃ¢y lÃ  má»™t hÃ m xÃ¡c suáº¥t chá»n hÃ nh Ä‘á»™ng `a` khi Ä‘ang á»Ÿ tráº¡ng thÃ¡i `s`.
- **Value Function ($V^\pi(s)$)**: HÃ m giÃ¡ trá»‹, Æ°á»›c tÃ­nh tá»•ng pháº§n thÆ°á»Ÿng ká»³ vá»ng trong tÆ°Æ¡ng lai khi báº¯t Ä‘áº§u tá»« tráº¡ng thÃ¡i `s` vÃ  Ä‘i theo chÃ­nh sÃ¡ch $\pi$.
- **Q-Value Function ($Q^\pi(s, a)$)**: HÃ m giÃ¡ trá»‹ hÃ nh Ä‘á»™ng, Æ°á»›c tÃ­nh tá»•ng pháº§n thÆ°á»Ÿng ká»³ vá»ng khi thá»±c hiá»‡n hÃ nh Ä‘á»™ng `a` táº¡i tráº¡ng thÃ¡i `s` rá»“i sau Ä‘Ã³ Ä‘i theo chÃ­nh sÃ¡ch $\pi$.
- **Discount Factor ($\gamma$)**: Há»‡ sá»‘ chiáº¿t kháº¥u ($0 \le \gamma \le 1$), thá»ƒ hiá»‡n táº§m quan trá»ng cá»§a pháº§n thÆ°á»Ÿng trong tÆ°Æ¡ng lai so vá»›i pháº§n thÆ°á»Ÿng trÆ°á»›c máº¯t.

## ğŸ“– 2. Glossary (Äá»‹nh nghÄ©a cá»‘t lÃµi)

-   **Markov Decision Process (MDP)**: Má»™t khuÃ´n khá»• toÃ¡n há»c Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a viá»‡c ra quyáº¿t Ä‘á»‹nh trong mÃ´i trÆ°á»ng mÃ  káº¿t quáº£ vá»«a ngáº«u nhiÃªn, vá»«a chá»‹u sá»± kiá»ƒm soÃ¡t cá»§a agent.
-   **Bellman Equations**: Há»‡ phÆ°Æ¡ng trÃ¬nh Ä‘á»‡ quy mÃ´ táº£ má»‘i quan há»‡ giá»¯a giÃ¡ trá»‹ cá»§a má»™t tráº¡ng thÃ¡i vÃ  giÃ¡ trá»‹ cá»§a cÃ¡c tráº¡ng thÃ¡i káº¿ tiáº¿p. LÃ  ná»n táº£ng cho háº§u háº¿t cÃ¡c thuáº­t toÃ¡n RL.
-   **Exploration vs. Exploitation Tradeoff**: Sá»± Ä‘Ã¡nh Ä‘á»•i kinh Ä‘iá»ƒn trong RL.
    -   **Exploitation (Khai thÃ¡c)**: Chá»n hÃ nh Ä‘á»™ng tá»‘t nháº¥t dá»±a trÃªn nhá»¯ng gÃ¬ Ä‘Ã£ biáº¿t.
    -   **Exploration (KhÃ¡m phÃ¡)**: Thá»­ cÃ¡c hÃ nh Ä‘á»™ng má»›i Ä‘á»ƒ cÃ³ thá»ƒ tÃ¬m ra nhá»¯ng lá»±a chá»n tá»‘t hÆ¡n trong tÆ°Æ¡ng lai.
-   **On-Policy vs. Off-Policy**:
    -   **On-Policy**: Agent há»c vÃ  hÃ nh Ä‘á»™ng theo cÃ¹ng má»™t chÃ­nh sÃ¡ch.
    -   **Off-Policy**: Agent há»c má»™t chÃ­nh sÃ¡ch tá»‘i Æ°u trong khi Ä‘ang hÃ nh Ä‘á»™ng theo má»™t chÃ­nh sÃ¡ch khÃ¡c (thÆ°á»ng lÃ  chÃ­nh sÃ¡ch cÃ³ tÃ­nh khÃ¡m phÃ¡ cao hÆ¡n). Q-Learning lÃ  má»™t vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh.

---
## ğŸ§  3. Ná»n táº£ng lÃ½ thuyáº¿t

### 3.1 Markov Decision Process (MDP)

MDP lÃ  "sÃ¢n chÆ¡i" mÃ  cÃ¡c agent RL hoáº¡t Ä‘á»™ng trong Ä‘Ã³. NÃ³ Ä‘á»‹nh nghÄ©a cÃ¡c quy táº¯c cá»§a trÃ² chÆ¡i. Má»™t MDP Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi má»™t bá»™ 5 thÃ nh pháº§n `(S, A, P, R, Î³)`:

1.  **S (States)**: Má»™t táº­p há»£p táº¥t cáº£ cÃ¡c tráº¡ng thÃ¡i cÃ³ thá»ƒ cÃ³ cá»§a mÃ´i trÆ°á»ng.
    -   *VÃ­ dá»¥ (Cá» vua)*: ToÃ n bá»™ cÃ¡c cÃ¡ch sáº¯p xáº¿p quÃ¢n cá» trÃªn bÃ n cá».
2.  **A (Actions)**: Má»™t táº­p há»£p táº¥t cáº£ cÃ¡c hÃ nh Ä‘á»™ng mÃ  agent cÃ³ thá»ƒ thá»±c hiá»‡n.
    -   *VÃ­ dá»¥ (Cá» vua)*: Táº¥t cáº£ cÃ¡c nÆ°á»›c Ä‘i há»£p lá»‡ táº¡i má»™t tráº¡ng thÃ¡i bÃ n cá».
3.  **P (Transition Probability Function - HÃ m xÃ¡c suáº¥t chuyá»ƒn Ä‘á»•i)**: $P(s'|s, a)$ lÃ  xÃ¡c suáº¥t chuyá»ƒn Ä‘áº¿n tráº¡ng thÃ¡i má»›i $s'$ sau khi thá»±c hiá»‡n hÃ nh Ä‘á»™ng $a$ táº¡i tráº¡ng thÃ¡i $s$.
    -   *VÃ­ dá»¥ (Robot di chuyá»ƒn)*: Náº¿u robot ra lá»‡nh "Ä‘i tháº³ng", cÃ³ 80% xÃ¡c suáº¥t nÃ³ sáº½ Ä‘i tháº³ng, 10% trÆ°á»£t sang trÃ¡i, vÃ  10% trÆ°á»£t sang pháº£i.
4.  **R (Reward Function - HÃ m pháº§n thÆ°á»Ÿng)**: $R(s, a, s')$ lÃ  pháº§n thÆ°á»Ÿng agent nháº­n Ä‘Æ°á»£c khi chuyá»ƒn tá»« $s$ Ä‘áº¿n $s'$ báº±ng hÃ nh Ä‘á»™ng $a$.
    -   *VÃ­ dá»¥ (Game Pac-Man)*: +10 Ä‘iá»ƒm khi Äƒn má»™t viÃªn thá»©c Äƒn, -500 Ä‘iá»ƒm khi bá»‹ ma Ä‘uá»•i, +1 khi sá»‘ng sÃ³t qua má»—i bÆ°á»›c.
5.  **Î³ (Discount Factor)**: Há»‡ sá»‘ chiáº¿t kháº¥u.
    -   *VÃ­ dá»¥*: Náº¿u $\gamma = 0.9$, pháº§n thÆ°á»Ÿng nháº­n Ä‘Æ°á»£c á»Ÿ 1 bÆ°á»›c trong tÆ°Æ¡ng lai chá»‰ cÃ³ giÃ¡ trá»‹ báº±ng 90% so vá»›i pháº§n thÆ°á»Ÿng nháº­n Ä‘Æ°á»£c ngay láº­p tá»©c.

**TÃ­nh cháº¥t Markov ("Memoryless")**: TÆ°Æ¡ng lai chá»‰ phá»¥ thuá»™c vÃ o hiá»‡n táº¡i, khÃ´ng phá»¥ thuá»™c vÃ o quÃ¡ khá»©. $P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1}|s_t, a_t)$. Tráº¡ng thÃ¡i $s_t$ Ä‘Ã£ chá»©a táº¥t cáº£ thÃ´ng tin cáº§n thiáº¿t.

### 3.2 PhÆ°Æ¡ng trÃ¬nh Bellman (Bellman Equations)

PhÆ°Æ¡ng trÃ¬nh Bellman lÃ  cÃ´ng thá»©c Ä‘á»‡ quy dÃ¹ng Ä‘á»ƒ tÃ­nh toÃ¡n giÃ¡ trá»‹ cá»§a má»™t tráº¡ng thÃ¡i hoáº·c má»™t cáº·p tráº¡ng thÃ¡i-hÃ nh Ä‘á»™ng. ChÃºng káº¿t ná»‘i giÃ¡ trá»‹ cá»§a má»™t tráº¡ng thÃ¡i vá»›i giÃ¡ trá»‹ cá»§a cÃ¡c tráº¡ng thÃ¡i káº¿ tiáº¿p.

**Bellman Equation cho Value Function (V-function)**:
> "GiÃ¡ trá»‹ cá»§a tráº¡ng thÃ¡i hiá»‡n táº¡i (`s`) báº±ng pháº§n thÆ°á»Ÿng trÆ°á»›c máº¯t cá»™ng vá»›i giÃ¡ trá»‹ (Ä‘Ã£ chiáº¿t kháº¥u) cá»§a tráº¡ng thÃ¡i tiáº¿p theo mÃ  báº¡n cÃ³ kháº£ nÄƒng sáº½ Ä‘áº¿n."

$$ V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s, a) [R(s, a, s') + \gamma V^\pi(s')] $$

**Bellman Equation cho Q-Value Function (Q-function)**:
> "GiÃ¡ trá»‹ cá»§a viá»‡c thá»±c hiá»‡n hÃ nh Ä‘á»™ng `a` táº¡i tráº¡ng thÃ¡i `s` báº±ng pháº§n thÆ°á»Ÿng trÆ°á»›c máº¯t cá»™ng vá»›i giÃ¡ trá»‹ (Ä‘Ã£ chiáº¿t kháº¥u) cá»§a cáº·p (tráº¡ng thÃ¡i, hÃ nh Ä‘á»™ng) tá»‘t nháº¥t á»Ÿ bÆ°á»›c tiáº¿p theo."

$$ Q^\pi(s, a) = \sum_{s' \in S} P(s'|s, a) [R(s, a, s') + \gamma \sum_{a' \in A} \pi(a'|s') Q^\pi(s', a')] $$

**PhÆ°Æ¡ng trÃ¬nh Bellman tá»‘i Æ°u (Bellman Optimality Equations)**:
ÄÃ¢y lÃ  trÆ°á»ng há»£p Ä‘áº·c biá»‡t khi chÃºng ta Ä‘i theo chÃ­nh sÃ¡ch tá»‘i Æ°u (chá»n hÃ nh Ä‘á»™ng tá»‘t nháº¥t á»Ÿ má»—i bÆ°á»›c).

$$ V^*(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s, a) [R(s, a, s') + \gamma V^*(s')] $$
$$ Q^*(s, a) = \sum_{s' \in S} P(s'|s, a) [R(s, a, s') + \gamma \max_{a' \in A} Q^*(s', a')] $$

ÄÃ¢y chÃ­nh lÃ  cÃ´ng thá»©c ná»n táº£ng cho thuáº­t toÃ¡n **Q-Learning**. NÃ³ cho phÃ©p chÃºng ta cáº­p nháº­t giÃ¡ trá»‹ Q cá»§a má»™t cáº·p `(s, a)` dá»±a trÃªn giÃ¡ trá»‹ Q tá»‘i Ä‘a cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c á»Ÿ tráº¡ng thÃ¡i tiáº¿p theo `s'`.

---

## âš™ï¸ 4. Tháº» thuáº­t toÃ¡n - Q-Learning

### 1. BÃ i toÃ¡n & dá»¯ liá»‡u
- **BÃ i toÃ¡n**: TÃ¬m ra chÃ­nh sÃ¡ch tá»‘i Æ°u $\pi^*$ trong má»™t mÃ´i trÆ°á»ng MDP mÃ  khÃ´ng cáº§n biáº¿t trÆ°á»›c mÃ´ hÃ¬nh cá»§a mÃ´i trÆ°á»ng (Transition-Probabilities `P` vÃ  Reward Function `R`).
- **Dá»¯ liá»‡u**: CÃ¡c bá»™ `(state, action, reward, next_state)` mÃ  agent thu tháº­p Ä‘Æ°á»£c qua quÃ¡ trÃ¬nh tÆ°Æ¡ng tÃ¡c (thá»­ vÃ  sai).
- **á»¨ng dá»¥ng**: CÃ¡c bÃ i toÃ¡n Ä‘iá»u khiá»ƒn Ä‘Æ¡n giáº£n, game, robot tÃ¬m Ä‘Æ°á»ng.

### 2. MÃ´ hÃ¬nh & cÃ´ng thá»©c
- **MÃ´ hÃ¬nh**: Má»™t báº£ng (Q-table) lÆ°u trá»¯ giÃ¡ trá»‹ $Q(s, a)$ cho má»i cáº·p (tráº¡ng thÃ¡i, hÃ nh Ä‘á»™ng).
- **CÃ´ng thá»©c cáº­p nháº­t (dá»±a trÃªn Bellman Optimality)**:
$$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \right] $$

### 3. Loss & má»¥c tiÃªu
- **Má»¥c tiÃªu**: LÃ m cho $Q(s, a)$ há»™i tá»¥ vá» giÃ¡ trá»‹ tá»‘i Æ°u $Q^*(s, a)$.
- **Loss (Temporal Difference Error)**:
  $$ \text{TD Error} = \underbrace{r_t + \gamma \max_{a} Q(s_{t+1}, a)}_{\text{TD Target}} - \underbrace{Q(s_t, a_t)}_{\text{Old Value}} $$
  Má»¥c tiÃªu lÃ  giáº£m thiá»ƒu sai sá»‘ nÃ y.

### 4. Tá»‘i Æ°u hoÃ¡ & cáº­p nháº­t
- **Algorithm**: Temporal Difference (TD) Learning. Cáº­p nháº­t giÃ¡ trá»‹ Q dá»±a trÃªn Æ°á»›c tÃ­nh hiá»‡n táº¡i, khÃ´ng cáº§n Ä‘á»£i Ä‘áº¿n cuá»‘i má»™t episode.
- **ChÃ­nh sÃ¡ch hÃ nh Ä‘á»™ng**: ThÆ°á»ng lÃ  **Epsilon-Greedy** Ä‘á»ƒ cÃ¢n báº±ng giá»¯a Exploration vÃ  Exploitation.
  - Vá»›i xÃ¡c suáº¥t `1-Îµ`: Chá»n hÃ nh Ä‘á»™ng tá»‘t nháº¥t (khai thÃ¡c).
  - Vá»›i xÃ¡c suáº¥t `Îµ`: Chá»n má»™t hÃ nh Ä‘á»™ng ngáº«u nhiÃªn (khÃ¡m phÃ¡).

### 5. Hyperparams
- **Learning rate ($\alpha$)**: Tá»‘c Ä‘á»™ há»c (0.01-0.1).
- **Discount factor ($\gamma$)**: Táº§m quan trá»ng cá»§a pháº§n thÆ°á»Ÿng tÆ°Æ¡ng lai (0.9-0.99).
- **Epsilon ($\epsilon$)**: Tá»· lá»‡ khÃ¡m phÃ¡, thÆ°á»ng giáº£m dáº§n theo thá»i gian.

### 6. Äá»™ phá»©c táº¡p
- **Time**: $O(A)$ cho má»—i bÆ°á»›c cáº­p nháº­t (Ä‘á»ƒ tÃ¬m `max Q`).
- **Space**: $O(S \times A)$ Ä‘á»ƒ lÆ°u Q-table. ÄÃ¢y lÃ  háº¡n cháº¿ lá»›n nháº¥t.

### 7. Metrics Ä‘Ã¡nh giÃ¡
- **Tá»•ng pháº§n thÆ°á»Ÿng má»—i episode**: Pháº£i cÃ³ xu hÆ°á»›ng tÄƒng lÃªn.
- **Sá»‘ bÆ°á»›c Ä‘á»ƒ hoÃ n thÃ nh episode**: Pháº£i cÃ³ xu hÆ°á»›ng giáº£m Ä‘i (cho cÃ¡c bÃ i toÃ¡n cÃ³ má»¥c tiÃªu).
- **Sá»± há»™i tá»¥ cá»§a Q-table**: CÃ¡c giÃ¡ trá»‹ Q cÃ³ á»•n Ä‘á»‹nh sau má»™t thá»i gian khÃ´ng.

### 8. Æ¯u / NhÆ°á»£c Ä‘iá»ƒm
**Æ¯u Ä‘iá»ƒm**:
- ÄÆ¡n giáº£n, dá»… hiá»ƒu.
- **Off-policy**: Ráº¥t máº¡nh máº½, cho phÃ©p há»c tá»« kinh nghiá»‡m cÅ© hoáº·c tá»« cÃ¡c agent khÃ¡c.
- Äáº£m báº£o há»™i tá»¥ náº¿u cÃ¡c cáº·p (s, a) Ä‘Æ°á»£c ghÃ© thÄƒm Ä‘á»§ nhiá»u.

**NhÆ°á»£c Ä‘iá»ƒm**:
- KhÃ´ng thá»ƒ hoáº¡t Ä‘á»™ng vá»›i khÃ´ng gian tráº¡ng thÃ¡i/hÃ nh Ä‘á»™ng lá»›n hoáº·c liÃªn tá»¥c (do Q-table quÃ¡ lá»›n).
- Gáº·p khÃ³ khÄƒn trong mÃ´i trÆ°á»ng cÃ³ tÃ­nh ngáº«u nhiÃªn cao.

### 9. Báº«y & máº¹o
- **Báº«y**: Learning rate quÃ¡ lá»›n cÃ³ thá»ƒ lÃ m giÃ¡ trá»‹ Q khÃ´ng á»•n Ä‘á»‹nh.
- **Máº¹o**: Giáº£m dáº§n `epsilon` theo thá»i gian. Ban Ä‘áº§u khÃ¡m phÃ¡ nhiá»u, sau Ä‘Ã³ táº­p trung khai thÃ¡c.
- **Máº¹o**: Khá»Ÿi táº¡o Q-table má»™t cÃ¡ch láº¡c quan (vá»›i giÃ¡ trá»‹ cao) Ä‘á»ƒ khuyáº¿n khÃ­ch khÃ¡m phÃ¡.

### 10. Pseudocode:
```python
initialize Q(s, a) arbitrarily
for each episode:
    initialize s
    for each step of episode:
        choose a from s using policy derived from Q (e.g., Îµ-greedy)
        take action a, observe r, s'
        Q(s, a) <- Q(s, a) + Î±[r + Î³ * max_a'(Q(s', a')) - Q(s, a)]
        s <- s'
    until s is terminal
```

## âš™ï¸ 5. Thuáº­t toÃ¡n dá»±a trÃªn giÃ¡ trá»‹ (Value-Based Algorithms)

> **TÆ° tÆ°á»Ÿng cá»‘t lÃµi**: Thay vÃ¬ cá»‘ gáº¯ng há»c trá»±c tiáº¿p má»™t chÃ­nh sÃ¡ch (policy), cÃ¡c thuáº­t toÃ¡n nÃ y táº­p trung vÃ o viá»‡c há»c má»™t **hÃ m giÃ¡ trá»‹**. Sau khi cÃ³ Ä‘Æ°á»£c hÃ m giÃ¡ trá»‹ tá»‘i Æ°u, chÃ­nh sÃ¡ch tá»‘i Æ°u sáº½ tá»± Ä‘á»™ng xuáº¥t hiá»‡n: chá»‰ cáº§n chá»n hÃ nh Ä‘á»™ng dáº«n Ä‘áº¿n tráº¡ng thÃ¡i cÃ³ giÃ¡ trá»‹ cao nháº¥t.

### 5.1 Q-Learning
Q-Learning lÃ  thuáº­t toÃ¡n RL kinh Ä‘iá»ƒn. Má»¥c tiÃªu cá»§a nÃ³ lÃ  há»c hÃ m **$Q^*(s, a)$**, lÃ  giÃ¡ trá»‹ tá»‘i Æ°u cá»§a viá»‡c thá»±c hiá»‡n hÃ nh Ä‘á»™ng `a` trong tráº¡ng thÃ¡i `s`.

-   **Q-Table**: Trong cÃ¡c mÃ´i trÆ°á»ng Ä‘Æ¡n giáº£n, ta cÃ³ thá»ƒ dÃ¹ng má»™t báº£ng Ä‘á»ƒ lÆ°u giÃ¡ trá»‹ Q cho má»i cáº·p (tráº¡ng thÃ¡i, hÃ nh Ä‘á»™ng).
-   **CÃ´ng thá»©c cáº­p nháº­t**: TrÃ¡i tim cá»§a Q-learning, dá»±a trÃªn phÆ°Æ¡ng trÃ¬nh Bellman:
    `New_Q(s, a) = Old_Q(s, a) + Î± * [Reward + Î³ * max_Q(s', a') - Old_Q(s, a)]`
-   **Off-Policy**: Äiá»ƒm máº¡nh nháº¥t cá»§a Q-Learning. NÃ³ cÃ³ thá»ƒ há»c chÃ­nh sÃ¡ch tá»‘i Æ°u (luÃ´n chá»n hÃ nh Ä‘á»™ng `max_Q`) trong khi Ä‘ang thá»±c thi má»™t chÃ­nh sÃ¡ch khÃ¡c Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u (vÃ­ dá»¥: Îµ-greedy Ä‘á»ƒ khÃ¡m phÃ¡). Äiá»u nÃ y giá»‘ng nhÆ° viá»‡c xem ngÆ°á»i khÃ¡c chÆ¡i cá» Ä‘á»ƒ há»c nÆ°á»›c Ä‘i hay nháº¥t, trong khi chÃ­nh báº¡n thá»‰nh thoáº£ng láº¡i Ä‘i nhá»¯ng nÆ°á»›c ngá»› ngáº©n Ä‘á»ƒ thá»­ nghiá»‡m.

### 5.2 Deep Q-Networks (DQN)

-   **Váº¥n Ä‘á» vá»›i Q-Learning**: Q-table trá»Ÿ nÃªn báº¥t kháº£ thi khi khÃ´ng gian tráº¡ng thÃ¡i quÃ¡ lá»›n (vÃ­ dá»¥: mÃ n hÃ¬nh game Atari cÃ³ hÃ ng triá»‡u pixel).
-   **Giáº£i phÃ¡p cá»§a DQN**: DÃ¹ng má»™t **máº¡ng nÆ¡-ron** Ä‘á»ƒ **xáº¥p xá»‰** hÃ m Q-value. Máº¡ng nÃ y nháº­n Ä‘áº§u vÃ o lÃ  tráº¡ng thÃ¡i `s` vÃ  tráº£ vá» má»™t vector cÃ¡c giÃ¡ trá»‹ Q cho táº¥t cáº£ cÃ¡c hÃ nh Ä‘á»™ng cÃ³ thá»ƒ cÃ³.
    $$ Q(s, a; \theta) \approx Q^*(s, a) $$
-   **Hai cáº£i tiáº¿n Ä‘á»™t phÃ¡ Ä‘á»ƒ á»•n Ä‘á»‹nh training**:
    1.  **Experience Replay**: Thay vÃ¬ há»c ngay tá»« tráº£i nghiá»‡m vá»«a cÃ³, agent lÆ°u láº¡i cÃ¡c transition `(s, a, r, s')` vÃ o má»™t "bá»™ nhá»›" (replay buffer). Khi training, nÃ³ láº¥y ra má»™t mini-batch ngáº«u nhiÃªn tá»« bá»™ nhá»› nÃ y.
        *   **Táº¡i sao?** Viá»‡c nÃ y phÃ¡ vá»¡ sá»± tÆ°Æ¡ng quan giá»¯a cÃ¡c máº«u dá»¯ liá»‡u liÃªn tiáº¿p, giÃºp quÃ¡ trÃ¬nh há»c á»•n Ä‘á»‹nh hÆ¡n vÃ  hiá»‡u quáº£ hÆ¡n vá» máº·t dá»¯ liá»‡u.
    2.  **Fixed Q-Targets**: Sá»­ dá»¥ng hai máº¡ng nÆ¡-ron: má»™t máº¡ng chÃ­nh (`Q_online`) Ä‘Æ°á»£c cáº­p nháº­t liÃªn tá»¥c, vÃ  má»™t máº¡ng má»¥c tiÃªu (`Q_target`) Ä‘Æ°á»£c sao chÃ©p tá»« máº¡ng chÃ­nh sau má»—i `C` bÆ°á»›c.
        *   **Táº¡i sao?** Khi tÃ­nh toÃ¡n TD Target (`r + Î³ * max_Q(s')`), ta dÃ¹ng máº¡ng `Q_target` cÅ© vÃ  á»•n Ä‘á»‹nh. Äiá»u nÃ y ngÄƒn cháº·n viá»‡c "má»¥c tiÃªu" liÃªn tá»¥c thay Ä‘á»•i, giá»‘ng nhÆ° viá»‡c báº¡n cá»‘ báº¯n vÃ o má»™t táº¥m bia Ä‘ang di chuyá»ƒn. NÃ³ giÃºp quÃ¡ trÃ¬nh training á»•n Ä‘á»‹nh hÆ¡n ráº¥t nhiá»u.
    *   **Loss Function**: ThÆ°á»ng lÃ  Mean Squared Error giá»¯a TD Target vÃ  dá»± Ä‘oÃ¡n cá»§a máº¡ng online.
        $$ L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( \underbrace{r + \gamma \max_{a'} Q_{\text{target}}(s', a'; \theta^-)}_{\text{TD Target}} - \underbrace{Q_{\text{online}}(s, a; \theta)}_{\text{Prediction}} \right)^2 \right] $$

## ğŸ“ˆ 6. Thuáº­t toÃ¡n dá»±a trÃªn chÃ­nh sÃ¡ch (Policy-Based Algorithms)

> **TÆ° tÆ°á»Ÿng cá»‘t lÃµi**: Thay vÃ¬ há»c hÃ m giÃ¡ trá»‹, cÃ¡c thuáº­t toÃ¡n nÃ y **há»c trá»±c tiáº¿p chÃ­nh sÃ¡ch (policy) $\pi(a|s; \theta)$**. MÃ´ hÃ¬nh sáº½ lÃ  má»™t hÃ m nháº­n vÃ o tráº¡ng thÃ¡i `s` vÃ  tráº£ vá» má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t trÃªn cÃ¡c hÃ nh Ä‘á»™ng `a`.

-   **Æ¯u Ä‘iá»ƒm**:
    -   Hoáº¡t Ä‘á»™ng tá»‘t trong khÃ´ng gian hÃ nh Ä‘á»™ng liÃªn tá»¥c hoáº·c xÃ¡c suáº¥t.
    -   CÃ³ thá»ƒ há»c cÃ¡c chÃ­nh sÃ¡ch ngáº«u nhiÃªn (stochastic policies), há»¯u Ã­ch trong má»™t sá»‘ mÃ´i trÆ°á»ng.
-   **ThÃ¡ch thá»©c**: PhÆ°Æ¡ng sai (variance) cá»§a gradient thÆ°á»ng ráº¥t cao, khiáº¿n viá»‡c training khÃ´ng á»•n Ä‘á»‹nh.

### 6.1 Policy Gradient Theorem
ÄÃ¢y lÃ  Ä‘á»‹nh lÃ½ ná»n táº£ng cho cÃ¡c thuáº­t toÃ¡n Policy-Based. NÃ³ cung cáº¥p má»™t cÃ¡ch Ä‘á»ƒ tÃ­nh gradient cá»§a tá»•ng pháº§n thÆ°á»Ÿng ká»³ vá»ng theo cÃ¡c tham sá»‘ $\theta$ cá»§a chÃ­nh sÃ¡ch.

-   **TÆ° duy trá»±c quan**: "TÄƒng xÃ¡c suáº¥t cá»§a nhá»¯ng hÃ nh Ä‘á»™ng dáº«n Ä‘áº¿n pháº§n thÆ°á»Ÿng cao, vÃ  giáº£m xÃ¡c suáº¥t cá»§a nhá»¯ng hÃ nh Ä‘á»™ng dáº«n Ä‘áº¿n pháº§n thÆ°á»Ÿng tháº¥p."
-   **CÃ´ng thá»©c Gradient**:
    $$ \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \left( \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \right) \left( \sum_{t=0}^{T} r(s_t, a_t) \right) \right] $$
    -   $\nabla_\theta \log \pi_\theta(a_t|s_t)$: Cho biáº¿t hÆ°á»›ng Ä‘á»ƒ tÄƒng xÃ¡c suáº¥t cá»§a hÃ nh Ä‘á»™ng $a_t$ táº¡i tráº¡ng thÃ¡i $s_t$.
    -   $\sum r(s_t, a_t)$: Tá»•ng pháº§n thÆ°á»Ÿng cá»§a cáº£ má»™t episode (trajectory $\tau$).
    -   Vá» cÆ¡ báº£n, ta Ä‘iá»u chá»‰nh tham sá»‘ theo hÆ°á»›ng `log-probability` cá»§a cÃ¡c hÃ nh Ä‘á»™ng, Ä‘Æ°á»£c "cÃ¢n" bá»Ÿi tá»•ng pháº§n thÆ°á»Ÿng nháº­n Ä‘Æ°á»£c.

### 6.2 REINFORCE Algorithm
REINFORCE lÃ  thuáº­t toÃ¡n Policy Gradient Ä‘Æ¡n giáº£n nháº¥t.
1.  Cháº¡y má»™t episode hoÃ n chá»‰nh theo chÃ­nh sÃ¡ch hiá»‡n táº¡i $\pi_\theta$ Ä‘á»ƒ thu tháº­p má»™t trajectory `(s0, a0, r1, s1, a1, ...)`
2.  TÃ­nh tá»•ng pháº§n thÆ°á»Ÿng (return) $G_t$ tá»« má»—i time step `t` Ä‘áº¿n cuá»‘i.
3.  Cáº­p nháº­t tham sá»‘ $\theta$ báº±ng cÃ¡ch sá»­ dá»¥ng gradient ascent:
    $$ \theta \leftarrow \theta + \alpha \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) G_t $$

## ğŸ­ 7. Thuáº­t toÃ¡n Actor-Critic

> **TÆ° tÆ°á»Ÿng cá»‘t lÃµi**: Káº¿t há»£p nhá»¯ng Ä‘iá»ƒm máº¡nh nháº¥t cá»§a hai phÆ°Æ¡ng phÃ¡p Value-Based vÃ  Policy-Based.

MÃ´ hÃ¬nh Actor-Critic cÃ³ hai "bá»™ nÃ£o" riÃªng biá»‡t:
1.  **The Actor (Diá»…n viÃªn)**: LÃ  **policy** $\pi(a|s; \theta)$. NÃ³ chá»‹u trÃ¡ch nhiá»‡m chá»n hÃ nh Ä‘á»™ng.
2.  **The Critic (NhÃ  phÃª bÃ¬nh)**: LÃ  **value function** $V(s; w)$ hoáº·c $Q(s, a; w)$. NÃ³ chá»‹u trÃ¡ch nhiá»‡m "phÃª bÃ¬nh" hÃ nh Ä‘á»™ng cá»§a Actor báº±ng cÃ¡ch Ä‘Ã¡nh giÃ¡ xem hÃ nh Ä‘á»™ng Ä‘Ã³ tá»‘t Ä‘áº¿n Ä‘Ã¢u.

**Luá»“ng hoáº¡t Ä‘á»™ng**:
1.  **Actor** chá»n hÃ nh Ä‘á»™ng $a$ táº¡i tráº¡ng thÃ¡i $s$.
2.  Agent thá»±c hiá»‡n hÃ nh Ä‘á»™ng, nháº­n Ä‘Æ°á»£c pháº§n thÆ°á»Ÿng $r$ vÃ  tráº¡ng thÃ¡i má»›i $s'$.
3.  **Critic** tÃ­nh toÃ¡n "TD Error": $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$.
    -   TD Error cho biáº¿t hÃ nh Ä‘á»™ng vá»«a rá»“i "tá»‘t hÆ¡n" hay "tá»‡ hÆ¡n" so vá»›i ká»³ vá»ng.
4.  **Cáº­p nháº­t Critic**: DÃ¹ng TD Error Ä‘á»ƒ cáº­p nháº­t Critic, giÃºp nÃ³ Ä‘Æ°a ra nhá»¯ng lá»i phÃª bÃ¬nh chÃ­nh xÃ¡c hÆ¡n trong tÆ°Æ¡ng lai.
5.  **Cáº­p nháº­t Actor**: DÃ¹ng TD Error Ä‘á»ƒ cáº­p nháº­t Actor.
    -   Náº¿u $\delta_t > 0$ (hÃ nh Ä‘á»™ng tá»‘t hÆ¡n ká»³ vá»ng), tÄƒng xÃ¡c suáº¥t chá»n hÃ nh Ä‘á»™ng Ä‘Ã³.
    -   Náº¿u $\delta_t < 0$ (hÃ nh Ä‘á»™ng tá»‡ hÆ¡n ká»³ vá»ng), giáº£m xÃ¡c suáº¥t chá»n hÃ nh Ä‘á»™ng Ä‘Ã³.

### 7.1 PPO (Proximal Policy Optimization)
PPO lÃ  má»™t thuáº­t toÃ¡n Actor-Critic hiá»‡n Ä‘áº¡i vÃ  lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i nháº¥t hiá»‡n nay.

-   **Váº¥n Ä‘á» cá»§a Policy Gradient truyá»n thá»‘ng**: Viá»‡c cáº­p nháº­t policy cÃ³ thá»ƒ quÃ¡ lá»›n, khiáº¿n chÃ­nh sÃ¡ch má»›i hoÃ n toÃ n khÃ¡c chÃ­nh sÃ¡ch cÅ©, gÃ¢y ra sá»± sá»¥p Ä‘á»• trong quÃ¡ trÃ¬nh training.
-   **Giáº£i phÃ¡p cá»§a PPO**: Sá»­ dá»¥ng má»™t "Clipped Surrogate Objective Function" Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng má»—i láº§n cáº­p nháº­t policy chá»‰ diá»…n ra trong má»™t "vÃ¹ng an toÃ n" nhá». NÃ³ ngÄƒn khÃ´ng cho chÃ­nh sÃ¡ch thay Ä‘á»•i quÃ¡ Ä‘á»™t ngá»™t, giÃºp quÃ¡ trÃ¬nh há»c á»•n Ä‘á»‹nh hÆ¡n ráº¥t nhiá»u.

## ğŸŒ 8. á»¨ng dá»¥ng, BÃ i táº­p vÃ  Tham kháº£o

### 8.1 á»¨ng dá»¥ng thá»±c táº¿
-   **ChÆ¡i game**: AlphaGo cá»§a DeepMind Ä‘Ã¡nh báº¡i ká»³ thá»§ cá» vÃ¢y tháº¿ giá»›i; cÃ¡c agent chÆ¡i game Atari, Dota 2, StarCraft.
-   **Robotics**: Dáº¡y robot cÃ¡ch Ä‘i láº¡i, cáº§m náº¯m Ä‘á»“ váº­t, láº¯p rÃ¡p.
-   **TÃ i chÃ­nh**: Tá»‘i Æ°u hÃ³a danh má»¥c Ä‘áº§u tÆ°, giao dá»‹ch tá»± Ä‘á»™ng.
-   **Quáº£n lÃ½ tÃ i nguyÃªn**: Tá»‘i Æ°u hÃ³a hoáº¡t Ä‘á»™ng cá»§a cÃ¡c trung tÃ¢m dá»¯ liá»‡u (Google DeepMind), quáº£n lÃ½ lÆ°á»›i Ä‘iá»‡n.

### 8.2 BÃ i táº­p thá»±c hÃ nh
1.  **Grid World**: Implement thuáº­t toÃ¡n Q-Learning tá»« Ä‘áº§u Ä‘á»ƒ tÃ¬m Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t trong má»™t mÃª cung Ä‘Æ¡n giáº£n.
2.  **CartPole**: Sá»­ dá»¥ng thÆ° viá»‡n `gymnasium`, huáº¥n luyá»‡n má»™t agent DQN Ä‘á»ƒ giá»¯ thÄƒng báº±ng cho cÃ¢y cá»™t.
3.  **Policy Gradient**: Implement thuáº­t toÃ¡n REINFORCE cho bÃ i toÃ¡n CartPole vÃ  so sÃ¡nh káº¿t quáº£ vá»›i DQN.
4.  **PPO**: Sá»­ dá»¥ng má»™t thÆ° viá»‡n RL (nhÆ° `stable-baselines3`) Ä‘á»ƒ huáº¥n luyá»‡n má»™t agent PPO trÃªn má»™t mÃ´i trÆ°á»ng phá»©c táº¡p hÆ¡n (vÃ­ dá»¥: BipedalWalker).

### 8.3 TÃ i liá»‡u tham kháº£o
-   **SÃ¡ch**: "Reinforcement Learning: An Introduction" cá»§a Sutton vÃ  Barto (Ä‘Æ°á»£c coi lÃ  "kinh thÃ¡nh" cá»§a RL).
-   **KhÃ³a há»c**:
    -   David Silver's Reinforcement Learning Course (DeepMind/UCL).
    -   CS285 Deep Reinforcement Learning (UC Berkeley).
-   **ThÆ° viá»‡n**: `gymnasium`, `stable-baselines3`, `rl-baselines3-zoo`.
-   **BÃ i bÃ¡o quan trá»ng**:
    -   "Playing Atari with Deep Reinforcement Learning" (DQN paper).
    -   "Asynchronous Methods for Deep Reinforcement Learning" (A3C paper).
    -   "Proximal Policy Optimization Algorithms" (PPO paper).

---
*ChÃºc báº¡n há»c táº­p hiá»‡u quáº£! ğŸš€*

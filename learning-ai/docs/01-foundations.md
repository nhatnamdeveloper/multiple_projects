# üöÄ N·ªÅn t·∫£ng b·∫Øt bu·ªôc cho AI/ML/Data Science

> **M·ª•c ti√™u**: X√¢y d·ª±ng n·ªÅn t·∫£ng v·ªØng ch·∫Øc v·ªÅ l·∫≠p tr√¨nh, to√°n h·ªçc v√† c√¥ng c·ª• c·∫ßn thi·∫øt ƒë·ªÉ tr·ªü th√†nh chuy√™n gia AI/ML

## üìã T·ªïng quan n·ªôi dung

```mermaid
graph TD
    A[üéØ N·ªÅn t·∫£ng b·∫Øt bu·ªôc] --> B[üêç Python N√¢ng cao]
    A --> C[üìä To√°n h·ªçc c∆° b·∫£n]
    A --> D[üóÑÔ∏è SQL & Database]
    A --> E[üé® Tr·ª±c quan h√≥a d·ªØ li·ªáu]
    A --> F[üîß Git & CLI Tools]
    
    B --> B1[C·∫•u tr√∫c d·ªØ li·ªáu & Algorithms]
    B --> B2[OOP & Design Patterns]
    B --> B3[Packaging & Testing]
    B --> B4[Performance & Optimization]
    
    C --> C1[ƒê·∫°i s·ªë tuy·∫øn t√≠nh]
    C --> C2[Gi·∫£i t√≠ch & T·ªëi ∆∞u h√≥a]
    C --> C3[X√°c su·∫•t & Th·ªëng k√™]
    
    D --> D1[Queries & JOINs]
    D --> D2[Schema Design]
    D --> D3[Performance Tuning]
    
    E --> E1[Matplotlib/Seaborn]
    E --> E2[Plotly & Interactive]
    E --> E3[Storytelling & Dashboard]
    
    F --> F1[Version Control]
    F --> F2[Command Line]
    F --> F3[Automation Scripts]
```

![Foundations Overview](assets/foundations-overview.svg)

![Foundations Overview PNG](assets/foundations-overview.png)

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/foundations-overview.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/foundations-overview.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/foundations-overview.png)**

## üß© Ch∆∞∆°ng tr√¨nh 50/50 (L√Ω thuy·∫øt : Th·ª±c h√†nh)

- M·ª•c ti√™u: 50% l√Ω thuy·∫øt (Python advanced, Math, SQL, Visualization, Git/CLI), 50% th·ª±c h√†nh (notebook/b√†i t·∫≠p/mini-project)

| M√¥-ƒëun | L√Ω thuy·∫øt (50%) | Th·ª±c h√†nh (50%) |
|---|---|---|
| Python | Structures, OOP, packaging/testing | Th∆∞ vi·ªán nh·ªè + pytest + publish TestPyPI |
| To√°n | LA/Calc/Stats c·ªët l√µi | B√†i t·∫≠p PCA, CI, ki·ªÉm ƒë·ªãnh |
| SQL | JOIN/CTE/Window & Indexing | Truy v·∫•n t·ªëi ∆∞u + explain |
| Viz | Nguy√™n t·∫Øc ch·ªçn bi·ªÉu ƒë·ªì | Dashboard Plotly ƒë∆°n gi·∫£n |
| Git/CLI | Branch/PR, shell tools | Repo + script t·ª± ƒë·ªông ho√° |

Rubric (100ƒë/module): L√Ω thuy·∫øt 30 | Code 30 | K·∫øt qu·∫£ 30 | B√°o c√°o 10

---

## üêç 1. Python N√¢ng cao

### 1.1 C·∫•u tr√∫c d·ªØ li·ªáu v√† Algorithms

> **T·∫°i sao c·∫ßn h·ªçc?** Python l√† ng√¥n ng·ªØ ch√≠nh trong AI/ML. Hi·ªÉu s√¢u v·ªÅ c·∫•u tr√∫c d·ªØ li·ªáu gi√∫p t·ªëi ∆∞u hi·ªáu su·∫•t v√† vi·∫øt code s·∫°ch h∆°n.

#### List comprehensions v√† Generator expressions

**List comprehension** - C√°ch vi·∫øt ng·∫Øn g·ªçn ƒë·ªÉ t·∫°o list t·ª´ iterable:
```python
# List comprehension - t·∫°o list c√°c s·ªë ch·∫µn b√¨nh ph∆∞∆°ng
squares = [x**2 for x in range(10) if x % 2 == 0]
# K·∫øt qu·∫£: [0, 4, 16, 36, 64]

# Generator expression - ti·∫øt ki·ªám b·ªô nh·ªõ, ch·ªâ t√≠nh khi c·∫ßn
squares_gen = (x**2 for x in range(10) if x % 2 == 0)
# K·∫øt qu·∫£: generator object, kh√¥ng chi·∫øm b·ªô nh·ªõ
```

**L√Ω thuy·∫øt c∆° b·∫£n:**
- **List comprehension**: T·∫°o to√†n b·ªô list trong b·ªô nh·ªõ ngay l·∫≠p t·ª©c
- **Generator expression**: T·∫°o t·ª´ng ph·∫ßn t·ª≠ khi c·∫ßn, ti·∫øt ki·ªám b·ªô nh·ªõ

**Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p:**
- **Time Complexity**: O(n) cho c·∫£ hai
- **Space Complexity**: 
  - List comprehension: O(n) - l∆∞u to√†n b·ªô list
  - Generator: O(1) - ch·ªâ l∆∞u iterator state

**·ª®ng d·ª•ng trong ML:**
```python
# Feature engineering v·ªõi generator - ti·∫øt ki·ªám memory
def feature_generator(data_stream):
    """T·∫°o features t·ª´ data stream m√† kh√¥ng load to√†n b·ªô v√†o memory"""
    for batch in data_stream:
        features = [extract_feature(x) for x in batch]
        yield features

# Memory-efficient data processing
large_dataset = (process_row(row) for row in read_large_file())
```

**Best Practices:**
- D√πng list comprehension khi c·∫ßn random access ho·∫∑c multiple iterations
- D√πng generator khi x·ª≠ l√Ω large datasets ho·∫∑c streaming data
- K·∫øt h·ª£p v·ªõi `itertools.islice()` ƒë·ªÉ pagination

#### itertools module - B·ªô c√¥ng c·ª• m·∫°nh m·∫Ω

```python
from itertools import combinations, permutations, product, chain

# Combinations - t·ªï h·ª£p kh√¥ng l·∫∑p l·∫°i
list(combinations([1,2,3], 2))  
# K·∫øt qu·∫£: [(1,2), (1,3), (2,3)]
# C√¥ng th·ª©c: C(n,r) = n!/(r!(n-r)!)

# Permutations - ho√°n v·ªã c√≥ th·ª© t·ª±
list(permutations([1,2,3], 2))  
# K·∫øt qu·∫£: [(1,2), (1,3), (2,1), (2,3), (3,1), (3,2)]
# C√¥ng th·ª©c: P(n,r) = n!/(n-r)!

# Product - t√≠ch Descartes (t·∫•t c·∫£ t·ªï h·ª£p c√≥ th·ªÉ)
list(product([1,2], ['a','b']))  
# K·∫øt qu·∫£: [(1,'a'), (1,'b'), (2,'a'), (2,'b')]
# C√¥ng th·ª©c: n1 √ó n2 √ó ... √ó nk
```

**L√Ω thuy·∫øt to√°n h·ªçc:**
- **Combinations**: C(n,r) = n!/(r!(n-r)!) - s·ªë c√°ch ch·ªçn r ph·∫ßn t·ª≠ t·ª´ n ph·∫ßn t·ª≠ kh√¥ng quan t√¢m th·ª© t·ª±
- **Permutations**: P(n,r) = n!/(n-r)! - s·ªë c√°ch s·∫Øp x·∫øp r ph·∫ßn t·ª≠ t·ª´ n ph·∫ßn t·ª≠ c√≥ quan t√¢m th·ª© t·ª±
- **Product**: n1 √ó n2 √ó ... √ó nk - t√≠ch Descartes c·ªßa c√°c t·∫≠p h·ª£p

**Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p:**
```python
# Time complexity analysis
from time import time
import matplotlib.pyplot as plt

def benchmark_combinatorics():
    """Benchmark performance c·ªßa c√°c h√†m itertools"""
    sizes = range(5, 21)
    times = {'combinations': [], 'permutations': [], 'product': []}
    
    for n in sizes:
        # Test combinations
        start = time()
        list(combinations(range(n), n//2))
        times['combinations'].append(time() - start)
        
        # Test permutations  
        start = time()
        list(permutations(range(n), n//2))
        times['permutations'].append(time() - start)
        
        # Test product
        start = time()
        list(product(range(n//2), repeat=2))
        times['product'].append(time() - start)
    
    return sizes, times

# Plot performance comparison
sizes, times = benchmark_combinatorics()
plt.figure(figsize=(10, 6))
for name, time_data in times.items():
    plt.plot(sizes, time_data, label=name, marker='o')
plt.xlabel('Input Size (n)')
plt.ylabel('Time (seconds)')
plt.title('Performance Comparison: itertools functions')
plt.legend()
plt.grid(True)
plt.show()
```

**·ª®ng d·ª•ng th·ª±c t·∫ø trong ML:**
- **Combinations**: 
  - Feature selection: C(n,k) combinations cho k features t·ª´ n total features
  - Subset selection: T√¨m optimal feature subset
- **Permutations**: 
  - Hyperparameter tuning: P(n,k) orders cho k hyperparameters
  - Sequence modeling: T·∫°o training sequences
- **Product**: 
  - Grid search: Cartesian product c·ªßa hyperparameter ranges
  - Cross-validation: T·∫•t c·∫£ combinations c·ªßa train/validation splits

**Memory Optimization:**
```python
# Lazy evaluation v·ªõi generators
def lazy_feature_combinations(features, k):
    """T·∫°o feature combinations m√† kh√¥ng load t·∫•t c·∫£ v√†o memory"""
    for combo in combinations(features, k):
        yield list(combo)

# Batch processing cho large datasets
def batch_combinations(items, k, batch_size=1000):
    """Process combinations theo batches ƒë·ªÉ tr√°nh memory overflow"""
    combo_gen = combinations(items, k)
    batch = []
    for combo in combo_gen:
        batch.append(combo)
        if len(batch) >= batch_size:
            yield batch
            batch = []
    if batch:  # Yield remaining items
        yield batch
```

#### OOP Patterns - Thi·∫øt k·∫ø h∆∞·ªõng ƒë·ªëi t∆∞·ª£ng

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Protocol, Generic, TypeVar

# Abstract Base Class - l·ªõp c∆° s·ªü tr·ª´u t∆∞·ª£ng
class DataProcessor(ABC):
    @abstractmethod
    def process(self, data: bytes) -> str:
        """Ph∆∞∆°ng th·ª©c b·∫Øt bu·ªôc ph·∫£i implement"""
        pass

# Protocol - typing c·∫•u tr√∫c (structural typing)
class Serializable(Protocol):
    def serialize(self) -> bytes: ...

# Generic types - ki·ªÉu d·ªØ li·ªáu t·ªïng qu√°t
T = TypeVar('T')  # Type variable
class Container(Generic[T]):
    def __init__(self, item: T):
        self.item = item
    
    def get_item(self) -> T:
        return self.item

# Dataclass - t·ª± ƒë·ªông t·∫°o __init__, __repr__, etc.
@dataclass
class DataPoint:
    x: float
    y: float
    label: str = "unknown"
```

**L√Ω thuy·∫øt OOP v√† Type Systems:**

**1. Abstract Base Classes (ABC):**
- **Purpose**: ƒê·ªãnh nghƒ©a interface m√† kh√¥ng implement
- **Benefits**: 
  - Enforce contract implementation
  - Polymorphism v√† dependency injection
  - Testability v√† mockability
- **Design Pattern**: Template Method Pattern

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class MLModel(ABC):
    """Abstract base class cho t·∫•t c·∫£ ML models"""
    
    @abstractmethod
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'MLModel':
        """Train model - must be implemented by subclasses"""
        pass
    
    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions - must be implemented by subclasses"""
        pass
    
    @abstractmethod
    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """Evaluate model performance - must be implemented by subclasses"""
        pass

# Concrete implementation
class LinearRegression(MLModel):
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'LinearRegression':
        # Implementation here
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        # Implementation here
        pass
    
    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        # Implementation here
        pass
```

**2. Structural Typing v·ªõi Protocol:**
- **Concept**: "Duck typing" v·ªõi type checking
- **Benefits**: 
  - Flexible interfaces
  - No inheritance coupling
  - Runtime type safety

```python
from typing import Protocol, runtime_checkable

@runtime_checkable
class DataProcessor(Protocol):
    """Protocol cho data processing - structural typing"""
    def process(self, data: bytes) -> str: ...
    def validate(self, data: bytes) -> bool: ...

# Any class implementing these methods satisfies the protocol
class TextProcessor:
    def process(self, data: bytes) -> str:
        return data.decode('utf-8')
    
    def validate(self, data: bytes) -> bool:
        return len(data) > 0

# Type checker accepts this
def process_data(processor: DataProcessor, data: bytes) -> str:
    if processor.validate(data):
        return processor.process(data)
    raise ValueError("Invalid data")

# This works at runtime
text_proc = TextProcessor()
result = process_data(text_proc, b"Hello World")
```

**3. Generic Types v√† Type Variables:**
- **Purpose**: Type-safe generic programming
- **Benefits**: 
  - Reusable code v·ªõi different types
  - Compile-time type checking
  - Better IDE support

```python
from typing import TypeVar, Generic, List, Dict, Union
from dataclasses import dataclass

# Type variables
T = TypeVar('T')  # Unbounded type variable
N = TypeVar('N', bound=Union[int, float])  # Bounded type variable
K = TypeVar('K')  # Key type
V = TypeVar('V')  # Value type

@dataclass
class DataContainer(Generic[T]):
    """Generic container cho any data type"""
    data: T
    metadata: Dict[str, Any]
    
    def get_data(self) -> T:
        return self.data
    
    def set_data(self, new_data: T) -> None:
        self.data = new_data

# Usage examples
int_container = DataContainer[int](data=42, metadata={"type": "integer"})
str_container = DataContainer[str](data="hello", metadata={"type": "string"})

# Generic collections
class FeatureStore(Generic[K, V]):
    """Generic feature store v·ªõi key-value pairs"""
    
    def __init__(self):
        self._store: Dict[K, V] = {}
    
    def set_feature(self, key: K, value: V) -> None:
        self._store[key] = value
    
    def get_feature(self, key: K) -> V:
        return self._store[key]
    
    def get_all_features(self) -> Dict[K, V]:
        return self._store.copy()

# Usage v·ªõi different types
feature_store = FeatureStore[str, np.ndarray]()
feature_store.set_feature("user_embedding", np.random.randn(128))
```

**4. Advanced Design Patterns:**
```python
from typing import Callable, Optional
from functools import wraps

# Decorator Pattern
def retry(max_attempts: int = 3, delay: float = 1.0):
    """Retry decorator cho unreliable operations"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_attempts - 1:
                        time.sleep(delay * (2 ** attempt))  # Exponential backoff
            raise last_exception
        return wrapper
    return decorator

# Factory Pattern
class ModelFactory:
    """Factory cho creating ML models"""
    
    _models: Dict[str, Type[MLModel]] = {}
    
    @classmethod
    def register(cls, name: str, model_class: Type[MLModel]) -> None:
        """Register a new model class"""
        cls._models[name] = model_class
    
    @classmethod
    def create(cls, name: str, **kwargs) -> MLModel:
        """Create model instance by name"""
        if name not in cls._models:
            raise ValueError(f"Unknown model: {name}")
        return cls._models[name](**kwargs)

# Register models
ModelFactory.register("linear", LinearRegression)
ModelFactory.register("random_forest", RandomForestClassifier)

# Create models
linear_model = ModelFactory.create("linear")
rf_model = ModelFactory.create("random_forest")
```

**5. Memory Management v√† Performance:**
```python
import weakref
from contextlib import contextmanager

class CacheManager:
    """Memory-efficient cache v·ªõi weak references"""
    
    def __init__(self):
        self._cache = weakref.WeakValueDictionary()
    
    def get(self, key: str) -> Optional[Any]:
        return self._cache.get(key)
    
    def set(self, key: str, value: Any) -> None:
        self._cache[key] = value
    
    def clear(self) -> None:
        self._cache.clear()

# Context manager cho resource management
@contextmanager
def timed_operation(operation_name: str):
    """Context manager ƒë·ªÉ measure operation time"""
    start_time = time.time()
    try:
        yield
    finally:
        elapsed = time.time() - start_time
        print(f"{operation_name} took {elapsed:.4f} seconds")

# Usage
with timed_operation("Model Training"):
    model.fit(X_train, y_train)
```

### 1.2 Packaging v√† Testing

> **T·∫°i sao c·∫ßn h·ªçc?** Package management gi√∫p chia s·∫ª code, dependency management. Testing ƒë·∫£m b·∫£o code ho·∫°t ƒë·ªông ƒë√∫ng v√† d·ªÖ maintain.

#### pyproject.toml - C·∫•u h√¨nh package hi·ªán ƒë·∫°i

```toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "my-ai-package"
version = "0.1.0"
description = "AI/ML package for data analysis"
requires-python = ">=3.8"
dependencies = [
    "numpy>=1.21.0",      # Th∆∞ vi·ªán t√≠nh to√°n s·ªë h·ªçc
    "pandas>=1.3.0",      # Th∆∞ vi·ªán x·ª≠ l√Ω d·ªØ li·ªáu
    "scikit-learn>=1.0",  # Th∆∞ vi·ªán machine learning
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",      # Framework testing
    "black>=22.0.0",      # Code formatter
    "mypy>=0.950",        # Type checker
    "flake8>=4.0.0",      # Linter
]

[tool.pytest.ini_options]
testpaths = ["tests"]           # Th∆∞ m·ª•c ch·ª©a tests
python_files = ["test_*.py"]    # Pattern t√™n file test
addopts = "-v --tb=short"       # Options m·∫∑c ƒë·ªãnh
```

**Gi·∫£i th√≠ch c√°c ph·∫ßn:**
- **build-system**: C√¥ng c·ª• ƒë·ªÉ build package
- **dependencies**: C√°c th∆∞ vi·ªán b·∫Øt bu·ªôc khi c√†i ƒë·∫∑t
- **optional-dependencies**: C√°c th∆∞ vi·ªán ch·ªâ c·∫ßn cho development
- **tool.pytest.ini_options**: C·∫•u h√¨nh cho pytest

#### pytest patterns - C√°c m·∫´u testing hi·ªáu qu·∫£

```python
import pytest
from unittest.mock import Mock, patch

# Fixtures - d·ªØ li·ªáu test ƒë∆∞·ª£c t√°i s·ª≠ d·ª•ng
@pytest.fixture
def sample_data():
    """T·∫°o d·ªØ li·ªáu m·∫´u cho testing"""
    return {"a": 1, "b": 2, "c": 3}

@pytest.fixture
def mock_model():
    """Mock model ƒë·ªÉ test m√† kh√¥ng c·∫ßn train th·∫≠t"""
    model = Mock()
    model.predict.return_value = [0.1, 0.9, 0.3]
    return model

# Parametrized tests - test nhi·ªÅu tr∆∞·ªùng h·ª£p c√πng l√∫c
@pytest.mark.parametrize("input_data,expected", [
    ([1,2,3], 6),      # Test case 1: t·ªïng c√°c s·ªë d∆∞∆°ng
    ([0,0,0], 0),      # Test case 2: t·ªïng c√°c s·ªë 0
    ([-1,1], 0),       # Test case 3: t·ªïng c√°c s·ªë √¢m v√† d∆∞∆°ng
])
def test_sum_function(input_data, expected):
    """Test function t√≠nh t·ªïng"""
    assert sum(input_data) == expected

# Mock v√† patch - gi·∫£ l·∫≠p external dependencies
def test_data_loading(mock_model):
    """Test vi·ªác load d·ªØ li·ªáu v·ªõi mock model"""
    with patch('pandas.read_csv') as mock_read:
        mock_read.return_value = sample_data()
        # Test logic c·ªßa b·∫°n ·ªü ƒë√¢y
        assert mock_model.predict.called
```

**C√°c kh√°i ni·ªám testing:**
- **Fixture**: D·ªØ li·ªáu ho·∫∑c object ƒë∆∞·ª£c t√°i s·ª≠ d·ª•ng trong nhi·ªÅu test
- **Parametrized test**: Ch·∫°y c√πng m·ªôt test v·ªõi nhi·ªÅu b·ªô d·ªØ li·ªáu kh√°c nhau
- **Mock**: Gi·∫£ l·∫≠p object ƒë·ªÉ test m√† kh√¥ng c·∫ßn dependency th·∫≠t
- **Patch**: Thay th·∫ø t·∫°m th·ªùi m·ªôt object trong qu√° tr√¨nh test

### 1.3 To√°n h·ªçc c∆° b·∫£n

> **T·∫°i sao c·∫ßn h·ªçc?** To√°n h·ªçc l√† n·ªÅn t·∫£ng c·ªßa AI/ML. Hi·ªÉu c√°c kh√°i ni·ªám c∆° b·∫£n gi√∫p hi·ªÉu s√¢u thu·∫≠t to√°n v√† t·ªëi ∆∞u h√≥a.

#### ƒê·∫°i s·ªë tuy·∫øn t√≠nh

```python
import numpy as np

# Vector operations
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

# Dot product (t√≠ch v√¥ h∆∞·ªõng)
dot_product = np.dot(v1, v2)  # 1√ó4 + 2√ó5 + 3√ó6 = 32

# Matrix operations
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Matrix multiplication
C = A @ B  # Ho·∫∑c np.matmul(A, B)

# Eigenvalues v√† Eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(A)
```

**Gi·∫£i th√≠ch kh√°i ni·ªám:**
- **Dot product**: ƒêo ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa hai vector
- **Matrix multiplication**: K·∫øt h·ª£p th√¥ng tin t·ª´ hai ma tr·∫≠n
- **Eigenvalues/Eigenvectors**: ƒê·∫∑c tr∆∞ng quan tr·ªçng c·ªßa ma tr·∫≠n, d√πng trong PCA

#### X√°c su·∫•t v√† Th·ªëng k√™

```python
import scipy.stats as stats

# Ph√¢n ph·ªëi chu·∫©n (Normal distribution)
# Œº (mu) = mean, œÉ (sigma) = standard deviation
normal_dist = stats.norm(loc=0, scale=1)  # loc=Œº, scale=œÉ

# X√°c su·∫•t P(X < x)
prob_less_than_1 = normal_dist.cdf(1)  # P(X < 1)

# Confidence interval (kho·∫£ng tin c·∫≠y)
# 95% confidence interval cho mean
confidence_interval = stats.t.interval(0.95, df=len(data)-1, 
                                     loc=np.mean(data), 
                                     scale=stats.sem(data))
```

**Gi·∫£i th√≠ch kh√°i ni·ªám:**
- **Œº (mu)**: Gi√° tr·ªã trung b√¨nh c·ªßa ph√¢n ph·ªëi
- **œÉ (sigma)**: ƒê·ªô l·ªách chu·∫©n, ƒëo ƒë·ªô ph√¢n t√°n c·ªßa d·ªØ li·ªáu
- **Confidence interval**: Kho·∫£ng ch·ª©a tham s·ªë th·∫≠t v·ªõi x√°c su·∫•t tin c·∫≠y

### 1.4 SQL v√† Database

> **T·∫°i sao c·∫ßn h·ªçc?** H·∫ßu h·∫øt d·ªØ li·ªáu th·ª±c t·∫ø ƒë∆∞·ª£c l∆∞u trong database. SQL gi√∫p truy v·∫•n v√† x·ª≠ l√Ω d·ªØ li·ªáu hi·ªáu qu·∫£.

#### JOINs v√† Window Functions

```sql
-- INNER JOIN: Ch·ªâ l·∫•y d·ªØ li·ªáu c√≥ trong c·∫£ hai b·∫£ng
SELECT u.name, o.order_id, o.amount
FROM users u
INNER JOIN orders o ON u.user_id = o.user_id;

-- LEFT JOIN: L·∫•y t·∫•t c·∫£ t·ª´ b·∫£ng tr√°i, NULL n·∫øu kh√¥ng c√≥ trong b·∫£ng ph·∫£i
SELECT u.name, o.order_id
FROM users u
LEFT JOIN orders o ON u.user_id = o.user_id;

-- Window function: T√≠nh to√°n tr√™n t·∫≠p con d·ªØ li·ªáu
SELECT 
    user_id,
    order_date,
    amount,
    AVG(amount) OVER (
        PARTITION BY user_id 
        ORDER BY order_date 
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    ) as moving_avg_3_months
FROM orders;
```

**Gi·∫£i th√≠ch JOINs:**
- **INNER JOIN**: Giao c·ªßa hai b·∫£ng (ch·ªâ d·ªØ li·ªáu chung)
- **LEFT JOIN**: T·∫•t c·∫£ t·ª´ b·∫£ng tr√°i + d·ªØ li·ªáu chung t·ª´ b·∫£ng ph·∫£i
- **Window function**: T√≠nh to√°n tr√™n "c·ª≠a s·ªï" d·ªØ li·ªáu (v√≠ d·ª•: moving average)

### 1.5 Tr·ª±c quan h√≥a d·ªØ li·ªáu

> **T·∫°i sao c·∫ßn h·ªçc?** Tr·ª±c quan h√≥a gi√∫p hi·ªÉu d·ªØ li·ªáu, ph√°t hi·ªán pattern v√† truy·ªÅn ƒë·∫°t k·∫øt qu·∫£ hi·ªáu qu·∫£.

#### Matplotlib v√† Seaborn

```python
import matplotlib.pyplot as plt
import seaborn as sns

# T·∫°o figure v·ªõi subplots
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Histogram - ph√¢n ph·ªëi d·ªØ li·ªáu
axes[0,0].hist(data, bins=30, alpha=0.7, color='skyblue')
axes[0,0].set_title('Ph√¢n ph·ªëi d·ªØ li·ªáu')
axes[0,0].set_xlabel('Gi√° tr·ªã')
axes[0,0].set_ylabel('T·∫ßn su·∫•t')

# Box plot - ph√¢n ph·ªëi v√† outliers
sns.boxplot(data=data, ax=axes[0,1])
axes[0,1].set_title('Box Plot - Ph√°t hi·ªán outliers')

# Scatter plot - m·ªëi quan h·ªá gi·ªØa hai bi·∫øn
axes[1,0].scatter(x, y, alpha=0.6)
axes[1,0].set_title('M·ªëi quan h·ªá X vs Y')

# Heatmap - ma tr·∫≠n t∆∞∆°ng quan
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ax=axes[1,1])
axes[1,1].set_title('Ma tr·∫≠n t∆∞∆°ng quan')

plt.tight_layout()
plt.show()
```

**Gi·∫£i th√≠ch c√°c lo·∫°i bi·ªÉu ƒë·ªì:**
- **Histogram**: Hi·ªÉn th·ªã ph√¢n ph·ªëi t·∫ßn su·∫•t c·ªßa d·ªØ li·ªáu
- **Box plot**: Hi·ªÉn th·ªã median, quartiles v√† outliers
- **Scatter plot**: Hi·ªÉn th·ªã m·ªëi quan h·ªá gi·ªØa hai bi·∫øn s·ªë
- **Heatmap**: Hi·ªÉn th·ªã ma tr·∫≠n t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn

### 1.6 Git v√† CLI Tools

> **T·∫°i sao c·∫ßn h·ªçc?** Version control gi√∫p qu·∫£n l√Ω code, collaboration. CLI gi√∫p automation v√† t∆∞∆°ng t√°c v·ªõi h·ªá th·ªëng.

#### Git Workflow c∆° b·∫£n

```bash
# Kh·ªüi t·∫°o repository
git init
git remote add origin <repository_url>

# Workflow h√†ng ng√†y
git add .                    # Stage t·∫•t c·∫£ thay ƒë·ªïi
git commit -m "feat: add new ML model"  # Commit v·ªõi message r√µ r√†ng
git push origin main         # Push l√™n remote

# Branch management
git checkout -b feature/new-algorithm    # T·∫°o v√† chuy·ªÉn sang branch m·ªõi
git merge feature/new-algorithm          # Merge branch v√†o main
```

**Gi·∫£i th√≠ch Git concepts:**
- **Stage**: Chu·∫©n b·ªã files ƒë·ªÉ commit
- **Commit**: L∆∞u snapshot c·ªßa code t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
- **Branch**: Nh√°nh ph√°t tri·ªÉn ri√™ng bi·ªát
- **Merge**: K·∫øt h·ª£p code t·ª´ c√°c branch

## üìö T√†i li·ªáu tham kh·∫£o

### Python
- [Python Documentation](https://docs.python.org/3/) - T√†i li·ªáu ch√≠nh th·ª©c
- [Real Python Tutorials](https://realpython.com/) - H∆∞·ªõng d·∫´n th·ª±c t·∫ø
- [Effective Python - Brett Slatkin](https://effectivepython.com/) - Best practices
- [Fluent Python - Luciano Ramalho](https://www.oreilly.com/library/view/fluent-python/9781491946237/) - Python n√¢ng cao

### To√°n h·ªçc
- [Linear Algebra - Gilbert Strang](https://math.mit.edu/~gs/linearalgebra/) - ƒê·∫°i s·ªë tuy·∫øn t√≠nh
- [Probability and Statistics - DeGroot](https://www.pearson.com/en-us/subject-catalog/p/probability-and-statistics/P200000000968/9780134995472) - X√°c su·∫•t th·ªëng k√™

### SQL
- [SQL Tutorial - W3Schools](https://www.w3schools.com/sql/) - H·ªçc SQL c∆° b·∫£n
- [SQL Performance Explained - Markus Winand](https://use-the-index-luke.com/) - T·ªëi ∆∞u hi·ªáu su·∫•t SQL

### Visualization
- [Matplotlib Tutorial](https://matplotlib.org/stable/tutorials/index.html) - H∆∞·ªõng d·∫´n Matplotlib
- [Seaborn Gallery](https://seaborn.pydata.org/examples/index.html) - V√≠ d·ª• Seaborn

## üéØ B√†i t·∫≠p th·ª±c h√†nh

1. **Python**: T·∫°o package Python v·ªõi testing ƒë·∫ßy ƒë·ªß
2. **Data Structures**: Implement c√°c thu·∫≠t to√°n sort v√† search
3. **SQL**: Thi·∫øt k·∫ø database schema cho e-commerce
4. **Visualization**: T·∫°o dashboard cho dataset m·∫´u
5. **Git**: Th·ª±c h√†nh workflow v·ªõi team

## üöÄ B∆∞·ªõc ti·∫øp theo

Sau khi ho√†n th√†nh n·ªÅn t·∫£ng, b·∫°n s·∫Ω:
- Hi·ªÉu s√¢u v·ªÅ Python v√† c√°c c√¥ng c·ª• development
- C√≥ ki·∫øn th·ª©c to√°n h·ªçc c∆° b·∫£n cho ML
- Bi·∫øt c√°ch qu·∫£n l√Ω d·ªØ li·ªáu v·ªõi SQL
- C√≥ th·ªÉ t·∫°o visualization chuy√™n nghi·ªáp
- S·∫µn s√†ng h·ªçc Data Analysis v√† Machine Learning

---

*Ch√∫c b·∫°n h·ªçc t·∫≠p hi·ªáu qu·∫£! üéâ*


# üöÄ N·ªÅn t·∫£ng b·∫Øt bu·ªôc cho AI/ML/Data Science

> **M·ª•c ti√™u**: X√¢y d·ª±ng n·ªÅn t·∫£ng v·ªØng ch·∫Øc v·ªÅ l·∫≠p tr√¨nh, to√°n h·ªçc v√† c√¥ng c·ª• c·∫ßn thi·∫øt ƒë·ªÉ tr·ªü th√†nh chuy√™n gia AI/ML

## üìö **1. B·∫£ng k√Ω hi·ªáu (Notation)**

### **Programming & Data Structures:**
- **Variable**: $x, y, z$ (bi·∫øn trong ch∆∞∆°ng tr√¨nh)
- **Function**: $f(x), g(x, y)$ (h√†m s·ªë)
- **List**: $L = [x_1, x_2, \ldots, x_n]$ (danh s√°ch)
- **Dictionary**: $D = \{k_1: v_1, k_2: v_2, \ldots\}$ (t·ª´ ƒëi·ªÉn)
- **Set**: $S = \{x_1, x_2, \ldots, x_n\}$ (t·∫≠p h·ª£p)

### **Mathematics:**
- **Vector**: $\mathbf{x} = [x_1, x_2, \ldots, x_n]^T$
- **Matrix**: $\mathbf{A} \in \mathbb{R}^{m \times n}$
- **Scalar**: $a, b, c \in \mathbb{R}$
- **Function**: $f: \mathbb{R}^n \rightarrow \mathbb{R}$
- **Gradient**: $\nabla f(\mathbf{x}) = [\frac{\partial f}{\partial x_1}, \ldots, \frac{\partial f}{\partial x_n}]^T$

### **Statistics & Probability:**
- **Mean**: $\mu = \frac{1}{n}\sum_{i=1}^n x_i$
- **Variance**: $\sigma^2 = \frac{1}{n}\sum_{i=1}^n (x_i - \mu)^2$
- **Probability**: $P(A)$ (x√°c su·∫•t c·ªßa s·ª± ki·ªán A)
- **Expectation**: $\mathbb{E}[X] = \sum_x x \cdot P(X=x)$

### **Database:**
- **Table**: $T(A_1, A_2, \ldots, A_n)$ (b·∫£ng v·ªõi attributes)
- **Query**: $Q = \sigma_{condition}(T)$ (selection query)
- **Join**: $T_1 \bowtie_{condition} T_2$ (join operation)

## üìñ **2. Glossary (ƒê·ªãnh nghƒ©a c·ªët l√µi)**

### **Programming Concepts:**
- **Algorithm**: Thu·∫≠t to√°n - t·∫≠p h·ª£p c√°c b∆∞·ªõc ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n
- **Data Structure**: C·∫•u tr√∫c d·ªØ li·ªáu - c√°ch t·ªï ch·ª©c v√† l∆∞u tr·ªØ d·ªØ li·ªáu
- **Time Complexity**: ƒê·ªô ph·ª©c t·∫°p th·ªùi gian - th·ªùi gian ch·∫°y c·ªßa thu·∫≠t to√°n
- **Space Complexity**: ƒê·ªô ph·ª©c t·∫°p kh√¥ng gian - b·ªô nh·ªõ c·∫ßn thi·∫øt

### **Mathematics:**
- **Linear Algebra**: ƒê·∫°i s·ªë tuy·∫øn t√≠nh - nghi√™n c·ª©u v·ªÅ vectors, matrices
- **Calculus**: Gi·∫£i t√≠ch - nghi√™n c·ª©u v·ªÅ derivatives, integrals
- **Statistics**: Th·ªëng k√™ - thu th·∫≠p, ph√¢n t√≠ch v√† di·ªÖn gi·∫£i d·ªØ li·ªáu
- **Probability**: X√°c su·∫•t - nghi√™n c·ª©u v·ªÅ uncertainty v√† randomness

### **Database:**
- **SQL**: Structured Query Language - ng√¥n ng·ªØ truy v·∫•n c∆° s·ªü d·ªØ li·ªáu
- **Index**: Ch·ªâ m·ª•c - c·∫•u tr√∫c d·ªØ li·ªáu ƒë·ªÉ tƒÉng t·ªëc truy v·∫•n
- **Normalization**: Chu·∫©n h√≥a - qu√° tr√¨nh t·ªï ch·ª©c d·ªØ li·ªáu ƒë·ªÉ gi·∫£m redundancy

### **Visualization:**
- **Chart**: Bi·ªÉu ƒë·ªì - c√°ch bi·ªÉu di·ªÖn d·ªØ li·ªáu tr·ª±c quan
- **Dashboard**: B·∫£ng ƒëi·ªÅu khi·ªÉn - t·∫≠p h·ª£p c√°c bi·ªÉu ƒë·ªì v√† metrics
- **Storytelling**: K·ªÉ chuy·ªán b·∫±ng d·ªØ li·ªáu - c√°ch tr√¨nh b√†y insights

## üìê **3. Th·∫ª thu·∫≠t to√°n - List Comprehension**

### **1. B√†i to√°n & d·ªØ li·ªáu:**
- **B√†i to√°n**: T·∫°o list m·ªõi t·ª´ iterable v·ªõi ƒëi·ªÅu ki·ªán v√† transformation
- **D·ªØ li·ªáu**: Iterable $I = \{x_1, x_2, \ldots, x_n\}$, condition $C(x)$, transformation $T(x)$
- **·ª®ng d·ª•ng**: Data processing, feature engineering, filtering

### **2. M√¥ h√¨nh & c√¥ng th·ª©c:**
**List Comprehension:**
$$L = [T(x) \text{ for } x \text{ in } I \text{ if } C(x)]$$

**Generator Expression:**
$$G = (T(x) \text{ for } x \text{ in } I \text{ if } C(x))$$

Trong ƒë√≥:
- $T(x)$: Transformation function
- $C(x)$: Condition function
- $I$: Input iterable

### **3. Loss & m·ª•c ti√™u:**
- **M·ª•c ti√™u**: T·∫°o collection m·ªõi m·ªôt c√°ch concise v√† readable
- **Loss**: Kh√¥ng c√≥ loss, l√† data transformation

### **4. T·ªëi ∆∞u ho√° & c·∫≠p nh·∫≠t:**
- **Algorithm**: Iterate through input v√† apply transformation
- **C·∫≠p nh·∫≠t**: Kh√¥ng c√≥ parameter learning

### **5. Hyperparams:**
- **Input size**: $n$ (s·ªë ph·∫ßn t·ª≠ trong iterable)
- **Condition complexity**: $O(C(x))$ (ƒë·ªô ph·ª©c t·∫°p c·ªßa condition)
- **Transformation complexity**: $O(T(x))$ (ƒë·ªô ph·ª©c t·∫°p c·ªßa transformation)

### **6. ƒê·ªô ph·ª©c t·∫°p:**
- **Time**: $O(n \times (C(x) + T(x)))$
- **Space**: 
  - List comprehension: $O(n)$ (l∆∞u to√†n b·ªô result)
  - Generator: $O(1)$ (ch·ªâ l∆∞u iterator state)

### **7. Metrics ƒë√°nh gi√°:**
- **Readability**: Code c√≥ d·ªÖ ƒë·ªçc kh√¥ng?
- **Performance**: Memory usage v√† execution time
- **Maintainability**: Code c√≥ d·ªÖ maintain kh√¥ng?

### **8. ∆Øu / Nh∆∞·ª£c:**
**∆Øu ƒëi·ªÉm:**
- Concise v√† readable
- Pythonic style
- Memory efficient v·ªõi generator

**Nh∆∞·ª£c ƒëi·ªÉm:**
- C√≥ th·ªÉ kh√≥ debug
- Kh√¥ng ph√π h·ª£p cho complex logic
- Nested comprehension c√≥ th·ªÉ kh√≥ ƒë·ªçc

### **9. B·∫´y & m·∫πo:**
- **B·∫´y**: Nested comprehension qu√° ph·ª©c t·∫°p ‚Üí kh√≥ ƒë·ªçc
- **B·∫´y**: Qu√™n condition ‚Üí t·∫°o list kh√¥ng mong mu·ªën
- **M·∫πo**: D√πng generator cho large datasets
- **M·∫πo**: Break complex logic th√†nh multiple steps

### **10. Pseudocode:**
```python
def list_comprehension(iterable, condition, transform):
    result = []
    for item in iterable:
        if condition(item):
            result.append(transform(item))
    return result

def generator_expression(iterable, condition, transform):
    for item in iterable:
        if condition(item):
            yield transform(item)
```

### **11. Code m·∫´u:**
```python
# List comprehension - t·∫°o list c√°c s·ªë ch·∫µn b√¨nh ph∆∞∆°ng
squares = [x**2 for x in range(10) if x % 2 == 0]
# K·∫øt qu·∫£: [0, 4, 16, 36, 64]

# Generator expression - ti·∫øt ki·ªám b·ªô nh·ªõ, ch·ªâ t√≠nh khi c·∫ßn
squares_gen = (x**2 for x in range(10) if x % 2 == 0)
# K·∫øt qu·∫£: generator object, kh√¥ng chi·∫øm b·ªô nh·ªõ

# Feature engineering v·ªõi generator - ti·∫øt ki·ªám memory
def feature_generator(data_stream):
    """T·∫°o features t·ª´ data stream m√† kh√¥ng load to√†n b·ªô v√†o memory"""
    for batch in data_stream:
        features = [extract_feature(x) for x in batch]
        yield features

# Nested comprehension example
matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
flattened = [item for row in matrix for item in row]
# K·∫øt qu·∫£: [1, 2, 3, 4, 5, 6, 7, 8, 9]

# Dictionary comprehension
squares_dict = {x: x**2 for x in range(5)}
# K·∫øt qu·∫£: {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}
```

### **12. Checklist ki·ªÉm tra nhanh:**
- [ ] Comprehension c√≥ readable?
- [ ] Condition c√≥ ƒë√∫ng logic?
- [ ] Transformation c√≥ efficient?
- [ ] Memory usage c√≥ acceptable?
- [ ] Code c√≥ maintainable?

---

## üìã T·ªïng quan n·ªôi dung

```mermaid
graph TD
    A[üéØ N·ªÅn t·∫£ng b·∫Øt bu·ªôc] --> B[üêç Python N√¢ng cao]
    A --> C[üìä To√°n h·ªçc c∆° b·∫£n]
    A --> D[üóÑÔ∏è SQL & Database]
    A --> E[üé® Tr·ª±c quan h√≥a d·ªØ li·ªáu]
    A --> F[üîß Git & CLI Tools]
    
    B --> B1[C·∫•u tr√∫c d·ªØ li·ªáu & Algorithms]
    B --> B2[OOP & Design Patterns]
    B --> B3[Packaging & Testing]
    B --> B4[Performance & Optimization]
    
    C --> C1[ƒê·∫°i s·ªë tuy·∫øn t√≠nh]
    C --> C2[Gi·∫£i t√≠ch & T·ªëi ∆∞u h√≥a]
    C --> C3[X√°c su·∫•t & Th·ªëng k√™]
    
    D --> D1[Queries & JOINs]
    D --> D2[Schema Design]
    D --> D3[Performance Tuning]
    
    E --> E1[Matplotlib/Seaborn]
    E --> E2[Plotly & Interactive]
    E --> E3[Storytelling & Dashboard]
    
    F --> F1[Version Control]
    F --> F2[Command Line]
    F --> F3[Automation Scripts]
```

![Foundations Overview](assets/foundations-overview.svg)

![Foundations Overview PNG](assets/foundations-overview.png)

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/foundations-overview.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/foundations-overview.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/foundations-overview.png)**

## üß© Ch∆∞∆°ng tr√¨nh 50/50 (L√Ω thuy·∫øt : Th·ª±c h√†nh)

- M·ª•c ti√™u: 50% l√Ω thuy·∫øt (Python advanced, Math, SQL, Visualization, Git/CLI), 50% th·ª±c h√†nh (notebook/b√†i t·∫≠p/mini-project)

| M√¥-ƒëun | L√Ω thuy·∫øt (50%) | Th·ª±c h√†nh (50%) |
|---|---|---|
| Python | Structures, OOP, packaging/testing | Th∆∞ vi·ªán nh·ªè + pytest + publish TestPyPI |
| To√°n | LA/Calc/Stats c·ªët l√µi | B√†i t·∫≠p PCA, CI, ki·ªÉm ƒë·ªãnh |
| SQL | JOIN/CTE/Window & Indexing | Truy v·∫•n t·ªëi ∆∞u + explain |
| Viz | Nguy√™n t·∫Øc ch·ªçn bi·ªÉu ƒë·ªì | Dashboard Plotly ƒë∆°n gi·∫£n |
| Git/CLI | Branch/PR, shell tools | Repo + script t·ª± ƒë·ªông ho√° |

Rubric (100ƒë/module): L√Ω thuy·∫øt 30 | Code 30 | K·∫øt qu·∫£ 30 | B√°o c√°o 10

---

## üêç 1. Python N√¢ng cao

### 1.1 C·∫•u tr√∫c d·ªØ li·ªáu v√† Algorithms

> **T·∫°i sao c·∫ßn h·ªçc?** Python l√† ng√¥n ng·ªØ ch√≠nh trong AI/ML. Hi·ªÉu s√¢u v·ªÅ c·∫•u tr√∫c d·ªØ li·ªáu gi√∫p t·ªëi ∆∞u hi·ªáu su·∫•t v√† vi·∫øt code s·∫°ch h∆°n.

#### List comprehensions v√† Generator expressions

**List comprehension** - C√°ch vi·∫øt ng·∫Øn g·ªçn ƒë·ªÉ t·∫°o list t·ª´ iterable:
```python
# List comprehension - t·∫°o list c√°c s·ªë ch·∫µn b√¨nh ph∆∞∆°ng
squares = [x**2 for x in range(10) if x % 2 == 0]
# K·∫øt qu·∫£: [0, 4, 16, 36, 64]

# Generator expression - ti·∫øt ki·ªám b·ªô nh·ªõ, ch·ªâ t√≠nh khi c·∫ßn
squares_gen = (x**2 for x in range(10) if x % 2 == 0)
# K·∫øt qu·∫£: generator object, kh√¥ng chi·∫øm b·ªô nh·ªõ
```

**L√Ω thuy·∫øt c∆° b·∫£n:**
- **List comprehension**: T·∫°o to√†n b·ªô list trong b·ªô nh·ªõ ngay l·∫≠p t·ª©c
- **Generator expression**: T·∫°o t·ª´ng ph·∫ßn t·ª≠ khi c·∫ßn, ti·∫øt ki·ªám b·ªô nh·ªõ

**Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p:**
- **Time Complexity**: O(n) cho c·∫£ hai
- **Space Complexity**: 
  - List comprehension: O(n) - l∆∞u to√†n b·ªô list
  - Generator: O(1) - ch·ªâ l∆∞u iterator state

**·ª®ng d·ª•ng trong ML:**
```python
# Feature engineering v·ªõi generator - ti·∫øt ki·ªám memory
def feature_generator(data_stream):
    """T·∫°o features t·ª´ data stream m√† kh√¥ng load to√†n b·ªô v√†o memory"""
    for batch in data_stream:
        features = [extract_feature(x) for x in batch]
        yield features

# Memory-efficient data processing
large_dataset = (process_row(row) for row in read_large_file())
```

**Best Practices:**
- D√πng list comprehension khi c·∫ßn random access ho·∫∑c multiple iterations
- D√πng generator khi x·ª≠ l√Ω large datasets ho·∫∑c streaming data
- K·∫øt h·ª£p v·ªõi `itertools.islice()` ƒë·ªÉ pagination

#### itertools module - B·ªô c√¥ng c·ª• m·∫°nh m·∫Ω

```python
from itertools import combinations, permutations, product, chain

# Combinations - t·ªï h·ª£p kh√¥ng l·∫∑p l·∫°i
list(combinations([1,2,3], 2))  
# K·∫øt qu·∫£: [(1,2), (1,3), (2,3)]
# C√¥ng th·ª©c: C(n,r) = n!/(r!(n-r)!)

# Permutations - ho√°n v·ªã c√≥ th·ª© t·ª±
list(permutations([1,2,3], 2))  
# K·∫øt qu·∫£: [(1,2), (1,3), (2,1), (2,3), (3,1), (3,2)]
# C√¥ng th·ª©c: P(n,r) = n!/(n-r)!

# Product - t√≠ch Descartes (t·∫•t c·∫£ t·ªï h·ª£p c√≥ th·ªÉ)
list(product([1,2], ['a','b']))  
# K·∫øt qu·∫£: [(1,'a'), (1,'b'), (2,'a'), (2,'b')]
# C√¥ng th·ª©c: n1 √ó n2 √ó ... √ó nk
```

**L√Ω thuy·∫øt to√°n h·ªçc:**
- **Combinations**: C(n,r) = n!/(r!(n-r)!) - s·ªë c√°ch ch·ªçn r ph·∫ßn t·ª≠ t·ª´ n ph·∫ßn t·ª≠ kh√¥ng quan t√¢m th·ª© t·ª±
- **Permutations**: P(n,r) = n!/(n-r)! - s·ªë c√°ch s·∫Øp x·∫øp r ph·∫ßn t·ª≠ t·ª´ n ph·∫ßn t·ª≠ c√≥ quan t√¢m th·ª© t·ª±
- **Product**: n1 √ó n2 √ó ... √ó nk - t√≠ch Descartes c·ªßa c√°c t·∫≠p h·ª£p

**Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p:**
```python
# Time complexity analysis
from time import time
import matplotlib.pyplot as plt

def benchmark_combinatorics():
    """Benchmark performance c·ªßa c√°c h√†m itertools"""
    sizes = range(5, 21)
    times = {'combinations': [], 'permutations': [], 'product': []}
    
    for n in sizes:
        # Test combinations
        start = time()
        list(combinations(range(n), n//2))
        times['combinations'].append(time() - start)
        
        # Test permutations  
        start = time()
        list(permutations(range(n), n//2))
        times['permutations'].append(time() - start)
        
        # Test product
        start = time()
        list(product(range(n//2), repeat=2))
        times['product'].append(time() - start)
    
    return sizes, times

# Plot performance comparison
sizes, times = benchmark_combinatorics()
plt.figure(figsize=(10, 6))
for name, time_data in times.items():
    plt.plot(sizes, time_data, label=name, marker='o')
plt.xlabel('Input Size (n)')
plt.ylabel('Time (seconds)')
plt.title('Performance Comparison: itertools functions')
plt.legend()
plt.grid(True)
plt.show()
```

**·ª®ng d·ª•ng th·ª±c t·∫ø trong ML:**
- **Combinations**: 
  - Feature selection: C(n,k) combinations cho k features t·ª´ n total features
  - Subset selection: T√¨m optimal feature subset
- **Permutations**: 
  - Hyperparameter tuning: P(n,k) orders cho k hyperparameters
  - Sequence modeling: T·∫°o training sequences
- **Product**: 
  - Grid search: Cartesian product c·ªßa hyperparameter ranges
  - Cross-validation: T·∫•t c·∫£ combinations c·ªßa train/validation splits

**Memory Optimization:**
```python
# Lazy evaluation v·ªõi generators
def lazy_feature_combinations(features, k):
    """T·∫°o feature combinations m√† kh√¥ng load t·∫•t c·∫£ v√†o memory"""
    for combo in combinations(features, k):
        yield list(combo)

# Batch processing cho large datasets
def batch_combinations(items, k, batch_size=1000):
    """Process combinations theo batches ƒë·ªÉ tr√°nh memory overflow"""
    combo_gen = combinations(items, k)
    batch = []
    for combo in combo_gen:
        batch.append(combo)
        if len(batch) >= batch_size:
            yield batch
            batch = []
    if batch:  # Yield remaining items
        yield batch
```

#### OOP Patterns - Thi·∫øt k·∫ø h∆∞·ªõng ƒë·ªëi t∆∞·ª£ng

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Protocol, Generic, TypeVar

# Abstract Base Class - l·ªõp c∆° s·ªü tr·ª´u t∆∞·ª£ng
class DataProcessor(ABC):
    @abstractmethod
    def process(self, data: bytes) -> str:
        """Ph∆∞∆°ng th·ª©c b·∫Øt bu·ªôc ph·∫£i implement"""
        pass

# Protocol - typing c·∫•u tr√∫c (structural typing)
class Serializable(Protocol):
    def serialize(self) -> bytes: ...

# Generic types - ki·ªÉu d·ªØ li·ªáu t·ªïng qu√°t
T = TypeVar('T')  # Type variable
class Container(Generic[T]):
    def __init__(self, item: T):
        self.item = item
    
    def get_item(self) -> T:
        return self.item

# Dataclass - t·ª± ƒë·ªông t·∫°o __init__, __repr__, etc.
@dataclass
class DataPoint:
    x: float
    y: float
    label: str = "unknown"
```

**L√Ω thuy·∫øt OOP v√† Type Systems:**

**1. Abstract Base Classes (ABC):**
- **Purpose**: ƒê·ªãnh nghƒ©a interface m√† kh√¥ng implement
- **Benefits**: 
  - Enforce contract implementation
  - Polymorphism v√† dependency injection
  - Testability v√† mockability
- **Design Pattern**: Template Method Pattern

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class MLModel(ABC):
    """Abstract base class cho t·∫•t c·∫£ ML models"""
    
    @abstractmethod
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'MLModel':
        """Train model - must be implemented by subclasses"""
        pass
    
    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions - must be implemented by subclasses"""
        pass
    
    @abstractmethod
    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """Evaluate model performance - must be implemented by subclasses"""
        pass

# Concrete implementation
class LinearRegression(MLModel):
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'LinearRegression':
        # Implementation here
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        # Implementation here
        pass
    
    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        # Implementation here
        pass
```

**2. Structural Typing v·ªõi Protocol:**
- **Concept**: "Duck typing" v·ªõi type checking
- **Benefits**: 
  - Flexible interfaces
  - No inheritance coupling
  - Runtime type safety

```python
from typing import Protocol, runtime_checkable

@runtime_checkable
class DataProcessor(Protocol):
    """Protocol cho data processing - structural typing"""
    def process(self, data: bytes) -> str: ...
    def validate(self, data: bytes) -> bool: ...

# Any class implementing these methods satisfies the protocol
class TextProcessor:
    def process(self, data: bytes) -> str:
        return data.decode('utf-8')
    
    def validate(self, data: bytes) -> bool:
        return len(data) > 0

# Type checker accepts this
def process_data(processor: DataProcessor, data: bytes) -> str:
    if processor.validate(data):
        return processor.process(data)
    raise ValueError("Invalid data")

# This works at runtime
text_proc = TextProcessor()
result = process_data(text_proc, b"Hello World")
```

**3. Generic Types v√† Type Variables:**
- **Purpose**: Type-safe generic programming
- **Benefits**: 
  - Reusable code v·ªõi different types
  - Compile-time type checking
  - Better IDE support

```python
from typing import TypeVar, Generic, List, Dict, Union
from dataclasses import dataclass

# Type variables
T = TypeVar('T')  # Unbounded type variable
N = TypeVar('N', bound=Union[int, float])  # Bounded type variable
K = TypeVar('K')  # Key type
V = TypeVar('V')  # Value type

@dataclass
class DataContainer(Generic[T]):
    """Generic container cho any data type"""
    data: T
    metadata: Dict[str, Any]
    
    def get_data(self) -> T:
        return self.data
    
    def set_data(self, new_data: T) -> None:
        self.data = new_data

# Usage examples
int_container = DataContainer[int](data=42, metadata={"type": "integer"})
str_container = DataContainer[str](data="hello", metadata={"type": "string"})

# Generic collections
class FeatureStore(Generic[K, V]):
    """Generic feature store v·ªõi key-value pairs"""
    
    def __init__(self):
        self._store: Dict[K, V] = {}
    
    def set_feature(self, key: K, value: V) -> None:
        self._store[key] = value
    
    def get_feature(self, key: K) -> V:
        return self._store[key]
    
    def get_all_features(self) -> Dict[K, V]:
        return self._store.copy()

# Usage v·ªõi different types
feature_store = FeatureStore[str, np.ndarray]()
feature_store.set_feature("user_embedding", np.random.randn(128))
```

**4. Advanced Design Patterns:**
```python
from typing import Callable, Optional
from functools import wraps

# Decorator Pattern
def retry(max_attempts: int = 3, delay: float = 1.0):
    """Retry decorator cho unreliable operations"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_attempts - 1:
                        time.sleep(delay * (2 ** attempt))  # Exponential backoff
            raise last_exception
        return wrapper
    return decorator

# Factory Pattern
class ModelFactory:
    """Factory cho creating ML models"""
    
    _models: Dict[str, Type[MLModel]] = {}
    
    @classmethod
    def register(cls, name: str, model_class: Type[MLModel]) -> None:
        """Register a new model class"""
        cls._models[name] = model_class
    
    @classmethod
    def create(cls, name: str, **kwargs) -> MLModel:
        """Create model instance by name"""
        if name not in cls._models:
            raise ValueError(f"Unknown model: {name}")
        return cls._models[name](**kwargs)

# Register models
ModelFactory.register("linear", LinearRegression)
ModelFactory.register("random_forest", RandomForestClassifier)

# Create models
linear_model = ModelFactory.create("linear")
rf_model = ModelFactory.create("random_forest")
```

**5. Memory Management v√† Performance:**
```python
import weakref
from contextlib import contextmanager

class CacheManager:
    """Memory-efficient cache v·ªõi weak references"""
    
    def __init__(self):
        self._cache = weakref.WeakValueDictionary()
    
    def get(self, key: str) -> Optional[Any]:
        return self._cache.get(key)
    
    def set(self, key: str, value: Any) -> None:
        self._cache[key] = value
    
    def clear(self) -> None:
        self._cache.clear()

# Context manager cho resource management
@contextmanager
def timed_operation(operation_name: str):
    """Context manager ƒë·ªÉ measure operation time"""
    start_time = time.time()
    try:
        yield
    finally:
        elapsed = time.time() - start_time
        print(f"{operation_name} took {elapsed:.4f} seconds")

# Usage
with timed_operation("Model Training"):
    model.fit(X_train, y_train)
```

### 1.2 Packaging v√† Testing

> **T·∫°i sao c·∫ßn h·ªçc?** Package management gi√∫p chia s·∫ª code, dependency management. Testing ƒë·∫£m b·∫£o code ho·∫°t ƒë·ªông ƒë√∫ng v√† d·ªÖ maintain.

#### pyproject.toml - C·∫•u h√¨nh package hi·ªán ƒë·∫°i

```toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "my-ai-package"
version = "0.1.0"
description = "AI/ML package for data analysis"
requires-python = ">=3.8"
dependencies = [
    "numpy>=1.21.0",      # Th∆∞ vi·ªán t√≠nh to√°n s·ªë h·ªçc
    "pandas>=1.3.0",      # Th∆∞ vi·ªán x·ª≠ l√Ω d·ªØ li·ªáu
    "scikit-learn>=1.0",  # Th∆∞ vi·ªán machine learning
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",      # Framework testing
    "black>=22.0.0",      # Code formatter
    "mypy>=0.950",        # Type checker
    "flake8>=4.0.0",      # Linter
]

[tool.pytest.ini_options]
testpaths = ["tests"]           # Th∆∞ m·ª•c ch·ª©a tests
python_files = ["test_*.py"]    # Pattern t√™n file test
addopts = "-v --tb=short"       # Options m·∫∑c ƒë·ªãnh
```

**Gi·∫£i th√≠ch c√°c ph·∫ßn:**
- **build-system**: C√¥ng c·ª• ƒë·ªÉ build package
- **dependencies**: C√°c th∆∞ vi·ªán b·∫Øt bu·ªôc khi c√†i ƒë·∫∑t
- **optional-dependencies**: C√°c th∆∞ vi·ªán ch·ªâ c·∫ßn cho development
- **tool.pytest.ini_options**: C·∫•u h√¨nh cho pytest

#### pytest patterns - C√°c m·∫´u testing hi·ªáu qu·∫£

```python
import pytest
from unittest.mock import Mock, patch

# Fixtures - d·ªØ li·ªáu test ƒë∆∞·ª£c t√°i s·ª≠ d·ª•ng
@pytest.fixture
def sample_data():
    """T·∫°o d·ªØ li·ªáu m·∫´u cho testing"""
    return {"a": 1, "b": 2, "c": 3}

@pytest.fixture
def mock_model():
    """Mock model ƒë·ªÉ test m√† kh√¥ng c·∫ßn train th·∫≠t"""
    model = Mock()
    model.predict.return_value = [0.1, 0.9, 0.3]
    return model

# Parametrized tests - test nhi·ªÅu tr∆∞·ªùng h·ª£p c√πng l√∫c
@pytest.mark.parametrize("input_data,expected", [
    ([1,2,3], 6),      # Test case 1: t·ªïng c√°c s·ªë d∆∞∆°ng
    ([0,0,0], 0),      # Test case 2: t·ªïng c√°c s·ªë 0
    ([-1,1], 0),       # Test case 3: t·ªïng c√°c s·ªë √¢m v√† d∆∞∆°ng
])
def test_sum_function(input_data, expected):
    """Test function t√≠nh t·ªïng"""
    assert sum(input_data) == expected

# Mock v√† patch - gi·∫£ l·∫≠p external dependencies
def test_data_loading(mock_model):
    """Test vi·ªác load d·ªØ li·ªáu v·ªõi mock model"""
    with patch('pandas.read_csv') as mock_read:
        mock_read.return_value = sample_data()
        # Test logic c·ªßa b·∫°n ·ªü ƒë√¢y
        assert mock_model.predict.called
```

**C√°c kh√°i ni·ªám testing:**
- **Fixture**: D·ªØ li·ªáu ho·∫∑c object ƒë∆∞·ª£c t√°i s·ª≠ d·ª•ng trong nhi·ªÅu test
- **Parametrized test**: Ch·∫°y c√πng m·ªôt test v·ªõi nhi·ªÅu b·ªô d·ªØ li·ªáu kh√°c nhau
- **Mock**: Gi·∫£ l·∫≠p object ƒë·ªÉ test m√† kh√¥ng c·∫ßn dependency th·∫≠t
- **Patch**: Thay th·∫ø t·∫°m th·ªùi m·ªôt object trong qu√° tr√¨nh test

### 1.3 To√°n h·ªçc c∆° b·∫£n

> **T·∫°i sao c·∫ßn h·ªçc?** To√°n h·ªçc l√† ng√¥n ng·ªØ c·ªßa AI/ML. Hi·ªÉu c√°c kh√°i ni·ªám c∆° b·∫£n gi√∫p b·∫°n kh√¥ng ch·ªâ *s·ª≠ d·ª•ng* c√°c th∆∞ vi·ªán c√≥ s·∫µn m√† c√≤n *hi·ªÉu s√¢u* c√°ch ch√∫ng ho·∫°t ƒë·ªông, t·ª´ ƒë√≥ c√≥ th·ªÉ t√πy ch·ªânh, t·ªëi ∆∞u h√≥a v√† th·∫≠m ch√≠ ph√°t tri·ªÉn c√°c thu·∫≠t to√°n m·ªõi.

#### ƒê·∫°i s·ªë tuy·∫øn t√≠nh

ƒê·∫°i s·ªë tuy·∫øn t√≠nh l√† nh√°nh to√°n h·ªçc nghi√™n c·ª©u v·ªÅ kh√¥ng gian vector v√† c√°c ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh. Trong AI/ML, d·ªØ li·ªáu th∆∞·ªùng ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng vector v√† ma tr·∫≠n, v√† c√°c thu·∫≠t to√°n ML c·ªët l√µi (nh∆∞ Neural Networks) th·ª±c ch·∫•t l√† m·ªôt chu·ªói c√°c ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh.

##### Vector

- **Vector l√† g√¨?** M·ªôt vector l√† m·ªôt m·∫£ng c√°c con s·ªë, bi·ªÉu di·ªÖn m·ªôt ƒëi·ªÉm trong kh√¥ng gian nhi·ªÅu chi·ªÅu. M·ªói con s·ªë trong vector ƒë·∫°i di·ªán cho m·ªôt chi·ªÅu (m·ªôt thu·ªôc t√≠nh ho·∫∑c feature).
    - V√≠ d·ª•: Vector `[tu·ªïi, thu_nh·∫≠p, s·ªë_nƒÉm_kinh_nghi·ªám]` c√≥ th·ªÉ ƒë·∫°i di·ªán cho m·ªôt ·ª©ng vi√™n.
- **Bi·ªÉu di·ªÖn trong code**:
    ```python
    import numpy as np
    # Vector bi·ªÉu di·ªÖn m·ªôt ng∆∞·ªùi d√πng v·ªõi 3 features
    user_vector = np.array([25, 50000, 3]) 
    ```

##### C√°c ph√©p to√°n c∆° b·∫£n tr√™n Vector
- **C·ªông Vector**: `v1 + v2` - K·∫øt h·ª£p th√¥ng tin. V√≠ d·ª•: c·ªông vector "vua" v√† vector "ph·ª• n·ªØ" c√≥ th·ªÉ cho ra m·ªôt vector g·∫ßn v·ªõi "n·ªØ ho√†ng".
- **Nh√¢n v·ªõi s·ªë v√¥ h∆∞·ªõng (Scalar Multiplication)**: `c * v` - Scale (co gi√£n) m·ªôt vector. V√≠ d·ª•: `2 * user_vector` l√†m tƒÉng g·∫•p ƒë√¥i t·∫•t c·∫£ c√°c thu·ªôc t√≠nh.

##### T√≠ch v√¥ h∆∞·ªõng (Dot Product)

- **C√¥ng th·ª©c**: `v1 ¬∑ v2 = Œ£(v1[i] * v2[i])`
- **√ù nghƒ©a tr·ª±c quan**: T√≠ch v√¥ h∆∞·ªõng ƒëo l∆∞·ªùng m·ª©c ƒë·ªô "c√πng h∆∞·ªõng" (t∆∞∆°ng ƒë·ªìng) c·ªßa hai vector.
    - N·∫øu `v1 ¬∑ v2 > 0`: Hai vector c√πng h∆∞·ªõng.
    - N·∫øu `v1 ¬∑ v2 < 0`: Hai vector ng∆∞·ª£c h∆∞·ªõng.
    - N·∫øu `v1 ¬∑ v2 = 0`: Hai vector vu√¥ng g√≥c (tr·ª±c giao), kh√¥ng c√≥ s·ª± t∆∞∆°ng quan tuy·∫øn t√≠nh.
- **·ª®ng d·ª•ng trong ML**:
    - **ƒêo ƒë·ªô t∆∞∆°ng ƒë·ªìng**: R·∫•t quan tr·ªçng trong c√°c h·ªá th·ªëng g·ª£i √Ω (recommendation systems) v√† t√¨m ki·∫øm ng·ªØ nghƒ©a (semantic search).
    - **T√≠nh to√°n trong Neural Network**: L√† ph√©p to√°n c·ªët l√µi trong m·ªói neuron (t·ªïng tr·ªçng s·ªë c·ªßa c√°c input).

```python
# Vector operations
v1 = np.array([1, 2, 3])  # Vector bi·ªÉu di·ªÖn feature A
v2 = np.array([4, 5, 6])  # Vector bi·ªÉu di·ªÖn feature B

# T√≠ch v√¥ h∆∞·ªõng (Dot product)
dot_product = np.dot(v1, v2)  # 1*4 + 2*5 + 3*6 = 32
print(f"T√≠ch v√¥ h∆∞·ªõng: {dot_product}")
# Gi√° tr·ªã d∆∞∆°ng l·ªõn cho th·∫•y 2 vector kh√° t∆∞∆°ng ƒë·ªìng v·ªÅ h∆∞·ªõng.
```

##### Ma tr·∫≠n (Matrix)

- **Ma tr·∫≠n l√† g√¨?** M·ªôt ma tr·∫≠n l√† m·ªôt m·∫£ng hai chi·ªÅu c√°c con s·ªë, c√≥ th·ªÉ ƒë∆∞·ª£c xem nh∆∞ m·ªôt t·∫≠p h·ª£p c√°c vector.
- **·ª®ng d·ª•ng trong ML**:
    - **Bi·ªÉu di·ªÖn d·ªØ li·ªáu**: M·ªôt ma tr·∫≠n c√≥ th·ªÉ bi·ªÉu di·ªÖn m·ªôt t·∫≠p d·ªØ li·ªáu, trong ƒë√≥ m·ªói h√†ng l√† m·ªôt m·∫´u d·ªØ li·ªáu (data point) v√† m·ªói c·ªôt l√† m·ªôt feature.
    - **Bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh (Linear Transformation)**: M·ªôt ma tr·∫≠n c√≥ th·ªÉ "bi·∫øn ƒë·ªïi" m·ªôt vector t·ª´ kh√¥ng gian n√†y sang kh√¥ng gian kh√°c (xoay, co gi√£n, tr∆∞·ª£t). ƒê√¢y l√† n·ªÅn t·∫£ng c·ªßa Neural Networks.
    - **L∆∞u tr·ªØ tr·ªçng s·ªë**: Trong Neural Networks, c√°c tr·ªçng s·ªë (weights) c·ªßa m·ªôt layer ƒë∆∞·ª£c l∆∞u trong m·ªôt ma tr·∫≠n.

##### Ph√©p nh√¢n ma tr·∫≠n (Matrix Multiplication)

- **C√¥ng th·ª©c**: `C = A @ B` - Nh√¢n ma tr·∫≠n A (k√≠ch th∆∞·ªõc `m x n`) v·ªõi ma tr·∫≠n B (k√≠ch th∆∞·ªõc `n x p`) s·∫Ω cho ra ma tr·∫≠n C (k√≠ch th∆∞·ªõc `m x p`).
- **√ù nghƒ©a tr·ª±c quan**: Nh√¢n m·ªôt vector v·ªõi m·ªôt ma tr·∫≠n (`y = A @ x`) ch√≠nh l√† √°p d·ª•ng m·ªôt ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh l√™n vector ƒë√≥. Chu·ªói c√°c layer trong Neural Network th·ª±c ch·∫•t l√† m·ªôt chu·ªói c√°c ph√©p nh√¢n ma tr·∫≠n.
- **V√≠ d·ª•**:
    ```python
    # Ma tr·∫≠n A c√≥ th·ªÉ ƒë·∫°i di·ªán cho tr·ªçng s·ªë c·ªßa m·ªôt layer trong neural network
    A = np.array([[1, 2], [3, 4]]) 
    # Vector x l√† input c·ªßa layer ƒë√≥
    x = np.array([5, 6])

    # Ph√©p nh√¢n ma tr·∫≠n A @ x bi·∫øn ƒë·ªïi vector x
    transformed_x = A @ x  # (1*5 + 2*6, 3*5 + 4*6) = (17, 39)
    print(f"Vector x sau khi bi·∫øn ƒë·ªïi b·ªüi A: {transformed_x}")
    ```

##### Eigenvalues v√† Eigenvectors (Tr·ªã ri√™ng v√† Vector ri√™ng)

- **√ù nghƒ©a tr·ª±c quan**: Khi m·ªôt ma tr·∫≠n (ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh) t√°c ƒë·ªông l√™n h·∫ßu h·∫øt c√°c vector, n√≥ s·∫Ω l√†m thay ƒë·ªïi h∆∞·ªõng c·ªßa ch√∫ng. Tuy nhi√™n, c√≥ m·ªôt s·ªë vector ƒë·∫∑c bi·ªát ch·ªâ b·ªã co gi√£n (d√†i ra ho·∫∑c ng·∫Øn l·∫°i) m√† kh√¥ng ƒë·ªïi h∆∞·ªõng.
    - **Eigenvector**: L√† nh·ªØng vector kh√¥ng ƒë·ªïi h∆∞·ªõng n√†y. Ch√∫ng ƒë·∫°i di·ªán cho c√°c "tr·ª•c ch√≠nh" c·ªßa ph√©p bi·∫øn ƒë·ªïi.
    - **Eigenvalue**: L√† h·ªá s·ªë co gi√£n t∆∞∆°ng ·ª©ng v·ªõi m·ªói eigenvector. N√≥ cho bi·∫øt m·ª©c ƒë·ªô co gi√£n (quan tr·ªçng) c·ªßa tr·ª•c ƒë√≥.
- **·ª®ng d·ª•ng trong ML**:
    - **PCA (Principal Component Analysis)**: M·ªôt thu·∫≠t to√°n gi·∫£m chi·ªÅu d·ªØ li·ªáu. PCA t√¨m c√°c eigenvectors c·ªßa ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai (covariance matrix) ƒë·ªÉ x√°c ƒë·ªãnh c√°c "th√†nh ph·∫ßn ch√≠nh" (c√°c h∆∞·ªõng c√≥ nhi·ªÅu th√¥ng tin nh·∫•t) c·ªßa d·ªØ li·ªáu. C√°c eigenvalue t∆∞∆°ng ·ª©ng cho bi·∫øt t·∫ßm quan tr·ªçng c·ªßa m·ªói th√†nh ph·∫ßn. B·∫±ng c√°ch gi·ªØ l·∫°i c√°c th√†nh ph·∫ßn c√≥ eigenvalue l·ªõn nh·∫•t, ta c√≥ th·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu m√† m·∫•t √≠t th√¥ng tin nh·∫•t.
    - **Ph√¢n t√≠ch ƒë·ªô ·ªïn ƒë·ªãnh c·ªßa h·ªá th·ªëng**: Trong c√°c h·ªá th·ªëng ƒë·ªông, eigenvalues gi√∫p x√°c ƒë·ªãnh h·ªá th·ªëng c√≥ ·ªïn ƒë·ªãnh hay kh√¥ng.

```python
# Matrix operations
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Matrix multiplication
C = A @ B
print(f"Ph√©p nh√¢n ma tr·∫≠n A @ B:\n{C}")

# Eigenvalues v√† Eigenvectors
# Ch·ªâ √°p d·ª•ng cho ma tr·∫≠n vu√¥ng
eigenvalues, eigenvectors = np.linalg.eig(A)
print(f"\nEigenvalues c·ªßa A: {eigenvalues}")
print(f"Eigenvectors c·ªßa A:\n{eigenvectors}")

# Ki·ªÉm tra t√≠nh ch·∫•t A @ v = Œª * v
for i in range(len(eigenvalues)):
    v = eigenvectors[:, i]
    lambda_v = eigenvalues[i] * v
    Av = A @ v
    # So s√°nh xem Av v√† Œªv c√≥ t∆∞∆°ng ƒë∆∞∆°ng kh√¥ng
    print(f"\nKi·ªÉm tra Eigenvector {i+1}:")
    print(f"A @ v = {Av}")
    print(f"Œª * v = {lambda_v}")
    assert np.allclose(Av, lambda_v) # allclose d√πng ƒë·ªÉ so s√°nh float
```

**Gi·∫£i th√≠ch kh√°i ni·ªám:**
- **Dot product (T√≠ch v√¥ h∆∞·ªõng)**: ƒêo ƒë·ªô t∆∞∆°ng ƒë·ªìng v·ªÅ h∆∞·ªõng v√† ƒë·ªô l·ªõn gi·ªØa hai vector.
- **Matrix multiplication (Ph√©p nh√¢n ma tr·∫≠n)**: √Åp d·ª•ng m·ªôt chu·ªói c√°c ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh. L√† tr√°i tim c·ªßa c√°c m·∫°ng n∆°-ron s√¢u.
- **Eigenvalues/Eigenvectors (Tr·ªã ri√™ng/Vector ri√™ng)**: C√°c "tr·ª•c b·∫•t bi·∫øn" c·ªßa m·ªôt ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh. R·∫•t quan tr·ªçng trong c√°c thu·∫≠t to√°n gi·∫£m chi·ªÅu d·ªØ li·ªáu nh∆∞ PCA v√† trong vi·ªác hi·ªÉu c√°c thu·ªôc t√≠nh c·ªßa ma tr·∫≠n.

#### X√°c su·∫•t v√† Th·ªëng k√™

X√°c su·∫•t v√† Th·ªëng k√™ cung c·∫•p c√°c c√¥ng c·ª• ƒë·ªÉ m√¥ h√¨nh h√≥a s·ª± kh√¥ng ch·∫Øc ch·∫Øn (uncertainty) v√† ƒë·ªÉ r√∫t ra k·∫øt lu·∫≠n t·ª´ d·ªØ li·ªáu.

-   **X√°c su·∫•t (Probability)**: B·∫Øt ƒë·∫ßu v·ªõi m·ªôt m√¥ h√¨nh (v√≠ d·ª•: m·ªôt ƒë·ªìng xu c√¥ng b·∫±ng), v√† d·ª± ƒëo√°n d·ªØ li·ªáu (v√≠ d·ª•: x√°c su·∫•t nh·∫≠n ƒë∆∞·ª£c m·∫∑t ng·ª≠a l√† 50%).
-   **Th·ªëng k√™ (Statistics)**: B·∫Øt ƒë·∫ßu v·ªõi d·ªØ li·ªáu (v√≠ d·ª•: tung ƒë·ªìng xu 100 l·∫ßn, nh·∫≠n ƒë∆∞·ª£c 55 l·∫ßn m·∫∑t ng·ª≠a), v√† suy lu·∫≠n v·ªÅ m√¥ h√¨nh (v√≠ d·ª•: li·ªáu ƒë·ªìng xu c√≥ c√¥ng b·∫±ng kh√¥ng?).

##### C√°c kh√°i ni·ªám x√°c su·∫•t c∆° b·∫£n

-   **X√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán (Conditional Probability)**: $P(A|B)$ - x√°c su·∫•t c·ªßa s·ª± ki·ªán A x·∫£y ra *bi·∫øt r·∫±ng* s·ª± ki·ªán B ƒë√£ x·∫£y ra. V√≠ d·ª•: $P(\text{m∆∞a}|\text{tr·ªùi c√≥ m√¢y})$.
-   **ƒê·ªãnh l√Ω Bayes (Bayes' Theorem)**: L√† n·ªÅn t·∫£ng c·ªßa suy lu·∫≠n th·ªëng k√™ v√† nhi·ªÅu thu·∫≠t to√°n ML. N√≥ cho ph√©p ch√∫ng ta c·∫≠p nh·∫≠t "ni·ªÅm tin" c·ªßa m√¨nh v·ªÅ m·ªôt gi·∫£ thuy·∫øt khi c√≥ d·ªØ li·ªáu m·ªõi.
    $$ P(\text{Gi·∫£ thuy·∫øt | D·ªØ li·ªáu}) = \frac{P(\text{D·ªØ li·ªáu | Gi·∫£ thuy·∫øt}) \times P(\text{Gi·∫£ thuy·∫øt})}{P(\text{D·ªØ li·ªáu})} $$
    - $P(\text{Gi·∫£ thuy·∫øt | D·ªØ li·ªáu})$ (Posterior): Ni·ªÅm tin v√†o gi·∫£ thuy·∫øt *sau khi* th·∫•y d·ªØ li·ªáu.
    - $P(\text{D·ªØ li·ªáu | Gi·∫£ thuy·∫øt})$ (Likelihood): Kh·∫£ nƒÉng c√≥ ƒë∆∞·ª£c d·ªØ li·ªáu n√†y n·∫øu gi·∫£ thuy·∫øt l√† ƒë√∫ng.
    - $P(\text{Gi·∫£ thuy·∫øt})$ (Prior): Ni·ªÅm tin v√†o gi·∫£ thuy·∫øt *tr∆∞·ªõc khi* th·∫•y d·ªØ li·ªáu.

##### Ph√¢n ph·ªëi chu·∫©n (Normal Distribution)

- **L√† g√¨?**: C√≤n g·ªçi l√† ph√¢n ph·ªëi Gauss, c√≥ h√¨nh chu√¥ng ƒë·∫∑c tr∆∞ng. R·∫•t nhi·ªÅu hi·ªán t∆∞·ª£ng trong t·ª± nhi√™n tu√¢n theo ph√¢n ph·ªëi n√†y (chi·ªÅu cao, c√¢n n·∫∑ng, sai s·ªë ƒëo l∆∞·ªùng).
- **T·∫°i sao quan tr·ªçng?**: **ƒê·ªãnh l√Ω gi·ªõi h·∫°n trung t√¢m (Central Limit Theorem)** n√≥i r·∫±ng trung b√¨nh c·ªßa m·ªôt l∆∞·ª£ng l·ªõn c√°c bi·∫øn ng·∫´u nhi√™n ƒë·ªôc l·∫≠p s·∫Ω c√≥ ph√¢n ph·ªëi x·∫•p x·ªâ chu·∫©n, b·∫•t k·ªÉ ph√¢n ph·ªëi g·ªëc c·ªßa ch√∫ng l√† g√¨. ƒêi·ªÅu n√†y l√†m cho ph√¢n ph·ªëi chu·∫©n tr·ªü n√™n c·ª±c k·ª≥ ph·ªï bi·∫øn trong th·ªëng k√™.
- **Tham s·ªë**:
    - **Œº (mu)**: Gi√° tr·ªã trung b√¨nh (mean), ƒë·ªânh c·ªßa h√¨nh chu√¥ng.
    - **œÉ (sigma)**: ƒê·ªô l·ªách chu·∫©n (standard deviation), ƒëo ƒë·ªô "ph√¢n t√°n" hay "d·∫πt" c·ªßa h√¨nh chu√¥ng.

```python
import scipy.stats as stats
import numpy as np

# Ph√¢n ph·ªëi chu·∫©n (Normal distribution) v·ªõi trung b√¨nh 0 v√† ƒë·ªô l·ªách chu·∫©n 1
# Œº (mu) = mean, œÉ (sigma) = standard deviation
normal_dist = stats.norm(loc=0, scale=1)  # loc=Œº, scale=œÉ

# T√≠nh x√°c su·∫•t P(X < 1), t·ª©c l√† di·ªán t√≠ch d∆∞·ªõi ƒë∆∞·ªùng cong b√™n tr√°i c·ªßa x=1
prob_less_than_1 = normal_dist.cdf(1)  # cdf: Cumulative Distribution Function
print(f"P(X < 1) trong ph√¢n ph·ªëi chu·∫©n (0,1): {prob_less_than_1:.4f}")
# Kho·∫£ng 84.13% gi√° tr·ªã s·∫Ω nh·ªè h∆°n 1.
```

##### Kho·∫£ng tin c·∫≠y (Confidence Interval)

- **√ù nghƒ©a tr·ª±c quan**: Thay v√¨ ∆∞·ªõc l∆∞·ª£ng m·ªôt tham s·ªë (v√≠ d·ª•: chi·ªÅu cao trung b√¨nh c·ªßa ng∆∞·ªùi Vi·ªát Nam) b·∫±ng m·ªôt con s·ªë duy nh·∫•t, ta ƒë∆∞a ra m·ªôt *kho·∫£ng* v√† n√≥i r·∫±ng ta "tin t∆∞·ªüng 95%" r·∫±ng gi√° tr·ªã th·∫≠t n·∫±m trong kho·∫£ng ƒë√≥.
- **"Tin t∆∞·ªüng 95%" nghƒ©a l√† g√¨?**: N·∫øu ta l·∫∑p l·∫°i quy tr√¨nh l·∫•y m·∫´u v√† t√≠nh kho·∫£ng tin c·∫≠y n√†y 100 l·∫ßn, th√¨ kho·∫£ng 95 trong s·ªë c√°c kho·∫£ng tin c·∫≠y ƒë√≥ s·∫Ω ch·ª©a gi√° tr·ªã th·∫≠t c·ªßa tham s·ªë. N√≥ kh√¥ng c√≥ nghƒ©a l√† c√≥ 95% x√°c su·∫•t gi√° tr·ªã th·∫≠t n·∫±m trong m·ªôt kho·∫£ng tin c·∫≠y c·ª• th·ªÉ.

```python
# Gi·∫£ s·ª≠ ta c√≥ m·ªôt m·∫´u d·ªØ li·ªáu
data = np.random.normal(loc=170, scale=5, size=100) # M·∫´u 100 ng∆∞·ªùi c√≥ chi·ªÅu cao trung b√¨nh 170cm

# T√≠nh kho·∫£ng tin c·∫≠y 95% cho gi√° tr·ªã trung b√¨nh
# Ta d√πng t-distribution v√¨ ta ƒëang ∆∞·ªõc l∆∞·ª£ng t·ª´ m·ªôt m·∫´u
confidence_interval = stats.t.interval(0.95, df=len(data)-1, 
                                     loc=np.mean(data), 
                                     scale=stats.sem(data)) # sem: Standard Error of the Mean

print(f"Chi·ªÅu cao trung b√¨nh c·ªßa m·∫´u: {np.mean(data):.2f} cm")
print(f"Kho·∫£ng tin c·∫≠y 95% cho chi·ªÅu cao trung b√¨nh th·∫≠t: [{confidence_interval[0]:.2f}, {confidence_interval[1]:.2f}] cm")
```

##### Ki·ªÉm ƒë·ªãnh gi·∫£ thuy·∫øt (Hypothesis Testing)
- **M·ª•c ƒë√≠ch**: D√πng d·ªØ li·ªáu t·ª´ m·∫´u ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh v·ªÅ m·ªôt gi·∫£ thuy·∫øt n√†o ƒë√≥ v·ªÅ t·ªïng th·ªÉ.
- **C√°c b∆∞·ªõc c∆° b·∫£n**:
    1.  **Ph√°t bi·ªÉu gi·∫£ thuy·∫øt kh√¥ng (Null Hypothesis, H‚ÇÄ)**: Th∆∞·ªùng l√† gi·∫£ thuy·∫øt "kh√¥ng c√≥ g√¨ x·∫£y ra" (v√≠ d·ª•: thu·ªëc m·ªõi kh√¥ng c√≥ t√°c d·ª•ng, hai nh√≥m l√† nh∆∞ nhau).
    2.  **Ph√°t bi·ªÉu gi·∫£ thuy·∫øt ƒë·ªëi (Alternative Hypothesis, H‚ÇÅ)**: ƒêi·ªÅu b·∫°n mu·ªën ch·ª©ng minh (v√≠ d·ª•: thu·ªëc m·ªõi c√≥ t√°c d·ª•ng).
    3.  **T√≠nh to√°n p-value**: L√† x√°c su·∫•t quan s√°t ƒë∆∞·ª£c k·∫øt qu·∫£ hi·ªán t·∫°i (ho·∫∑c c·ª±c ƒëoan h∆°n) *n·∫øu gi·∫£ thuy·∫øt kh√¥ng l√† ƒë√∫ng*.
    4.  **K·∫øt lu·∫≠n**: N·∫øu p-value r·∫•t nh·ªè (th∆∞·ªùng < 0.05), ta c√≥ b·∫±ng ch·ª©ng ƒë·ªÉ b√°c b·ªè gi·∫£ thuy·∫øt kh√¥ng v√† ch·∫•p nh·∫≠n gi·∫£ thuy·∫øt ƒë·ªëi.

**Gi·∫£i th√≠ch kh√°i ni·ªám:**
- **Œº (mu)**: Gi√° tr·ªã trung b√¨nh c·ªßa ph√¢n ph·ªëi, th·ªÉ hi·ªán "trung t√¢m" c·ªßa d·ªØ li·ªáu.
- **œÉ (sigma)**: ƒê·ªô l·ªách chu·∫©n, ƒëo ƒë·ªô ph√¢n t√°n c·ªßa d·ªØ li·ªáu quanh gi√° tr·ªã trung b√¨nh.
- **Confidence interval (Kho·∫£ng tin c·∫≠y)**: M·ªôt kho·∫£ng ∆∞·ªõc l∆∞·ª£ng cho m·ªôt tham s·ªë c·ªßa t·ªïng th·ªÉ. N√≥ cho bi·∫øt m·ª©c ƒë·ªô kh√¥ng ch·∫Øc ch·∫Øn c·ªßa ∆∞·ªõc l∆∞·ª£ng.
- **Hypothesis Testing (Ki·ªÉm ƒë·ªãnh gi·∫£ thuy·∫øt)**: M·ªôt quy tr√¨nh th·ªëng k√™ ƒë·ªÉ quy·∫øt ƒë·ªãnh xem c√≥ ƒë·ªß b·∫±ng ch·ª©ng trong m·ªôt m·∫´u d·ªØ li·ªáu ƒë·ªÉ suy ra m·ªôt k·∫øt lu·∫≠n n√†o ƒë√≥ v·ªÅ t·ªïng th·ªÉ hay kh√¥ng.

#### Gi·∫£i t√≠ch v√† T·ªëi ∆∞u h√≥a

> **T·∫°i sao c·∫ßn h·ªçc?** H·∫ßu h·∫øt c√°c thu·∫≠t to√°n h·ªçc m√°y (ƒë·∫∑c bi·ªát l√† Deep Learning) ƒë·ªÅu l√† b√†i to√°n t·ªëi ∆∞u h√≥a. Ch√∫ng ta c·∫ßn t√¨m b·ªô tham s·ªë (weights) cho m√¥ h√¨nh ƒë·ªÉ h√†m m·∫•t m√°t (loss function) l√† nh·ªè nh·∫•t. Gi·∫£i t√≠ch cung c·∫•p c√¥ng c·ª• ƒë·ªÉ th·ª±c hi·ªán vi·ªác n√†y, ƒë√≥ l√† **ƒë·∫°o h√†m** v√† **gradient**.

##### ƒê·∫°o h√†m (Derivative)
- **√ù nghƒ©a tr·ª±c quan**: ƒê·∫°o h√†m c·ªßa m·ªôt h√†m s·ªë t·∫°i m·ªôt ƒëi·ªÉm cho bi·∫øt "t·ªëc ƒë·ªô thay ƒë·ªïi" hay "ƒë·ªô d·ªëc" c·ªßa h√†m s·ªë t·∫°i ƒëi·ªÉm ƒë√≥.
    - N·∫øu ƒë·∫°o h√†m > 0: h√†m s·ªë ƒëang ƒëi l√™n.
    - N·∫øu ƒë·∫°o h√†m < 0: h√†m s·ªë ƒëang ƒëi xu·ªëng.
    - N·∫øu ƒë·∫°o h√†m = 0: h√†m s·ªë ƒë·∫°t ƒëi·ªÉm c·ª±c tr·ªã (c·ª±c ƒë·∫°i ho·∫∑c c·ª±c ti·ªÉu).

##### Gradient (ÂãæÈÖç)
- **Gradient l√† g√¨?**: Trong kh√¥ng gian nhi·ªÅu chi·ªÅu, Gradient l√† m·ªôt vector ch·ª©a t·∫•t c·∫£ c√°c ƒë·∫°o h√†m ri√™ng (partial derivatives) c·ªßa h√†m s·ªë.
- **√ù nghƒ©a tr·ª±c quan**: Vector Gradient t·∫°i m·ªôt ƒëi·ªÉm lu√¥n **ch·ªâ v·ªÅ h∆∞·ªõng d·ªëc nh·∫•t** (h∆∞·ªõng m√† h√†m s·ªë tƒÉng nhanh nh·∫•t).
- **·ª®ng d·ª•ng**: ƒê·ªÉ t√¨m ƒëi·ªÉm c·ª±c ti·ªÉu c·ªßa h√†m m·∫•t m√°t, ta ch·ªâ c·∫ßn ƒëi ng∆∞·ª£c l·∫°i h∆∞·ªõng c·ªßa gradient. ƒê√¢y ch√≠nh l√† √Ω t∆∞·ªüng c·ªët l√µi c·ªßa thu·∫≠t to√°n **Gradient Descent**.

##### T·ªëi ∆∞u h√≥a v·ªõi Gradient Descent
- **T∆∞ t∆∞·ªüng**: Gi·ªëng nh∆∞ b·∫°n ƒëang ƒë·ª©ng tr√™n m·ªôt ng·ªçn ƒë·ªìi trong s∆∞∆°ng m√π v√† mu·ªën ƒëi xu·ªëng thung l≈©ng (ƒëi·ªÉm th·∫•p nh·∫•t). B·∫°n s·∫Ω nh√¨n xu·ªëng ch√¢n m√¨nh, xem h∆∞·ªõng n√†o l√† d·ªëc nh·∫•t v√† b∆∞·ªõc m·ªôt b∆∞·ªõc nh·ªè theo h∆∞·ªõng ƒë√≥. L·∫∑p l·∫°i qu√° tr√¨nh n√†y, b·∫°n s·∫Ω d·∫ßn d·∫ßn ƒëi ƒë·∫øn ƒë√°y thung l≈©ng.
- **C√¥ng th·ª©c c·∫≠p nh·∫≠t**:
    $$ \theta_{\text{m·ªõi}} = \theta_{\text{c≈©}} - \alpha \nabla L(\theta_{\text{c≈©}}) $$
    - $\theta$: Tham s·ªë c·ªßa m√¥ h√¨nh (v√≠ d·ª•: weights).
    - $L(\theta)$: H√†m m·∫•t m√°t.
    - $\nabla L(\theta)$: Gradient c·ªßa h√†m m·∫•t m√°t.
    - $\alpha$ (Learning Rate): "K√≠ch th∆∞·ªõc b∆∞·ªõc ch√¢n" c·ªßa b·∫°n.
        - N·∫øu $\alpha$ qu√° l·ªõn, b·∫°n c√≥ th·ªÉ "v∆∞·ª£t" qua ƒë√°y thung l≈©ng.
        - N·∫øu $\alpha$ qu√° nh·ªè, b·∫°n s·∫Ω ƒëi r·∫•t ch·∫≠m.

```python
# V√≠ d·ª• minh h·ªça Gradient Descent cho h√†m f(x) = x^2
def gradient_descent_example():
    # H√†m s·ªë c·∫ßn t·ªëi ∆∞u
    f = lambda x: x**2
    # ƒê·∫°o h√†m c·ªßa h√†m s·ªë
    gradient = lambda x: 2*x

    # Kh·ªüi t·∫°o gi√° tr·ªã ban ƒë·∫ßu
    x_current = 10.0
    learning_rate = 0.1
    epochs = 50

    print("B·∫Øt ƒë·∫ßu Gradient Descent:")
    for i in range(epochs):
        grad = gradient(x_current)
        x_current = x_current - learning_rate * grad
        if (i+1) % 5 == 0:
            print(f"Epoch {i+1}: x = {x_current:.4f}, f(x) = {f(x_current):.4f}")

    print(f"\nGi√° tr·ªã c·ª±c ti·ªÉu t√¨m ƒë∆∞·ª£c x = {x_current:.4f}")

gradient_descent_example()
```

### 1.4 SQL v√† Database

> **T·∫°i sao c·∫ßn h·ªçc?** H·∫ßu h·∫øt d·ªØ li·ªáu trong th·∫ø gi·ªõi th·ª±c ƒë∆∞·ª£c l∆∞u tr·ªØ trong c√°c c∆° s·ªü d·ªØ li·ªáu quan h·ªá. SQL (Structured Query Language) l√† ng√¥n ng·ªØ ti√™u chu·∫©n ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi ch√∫ng. Vi·ªác th√†nh th·∫°o SQL cho ph√©p b·∫°n tr√≠ch xu·∫•t, t·ªïng h·ª£p v√† chu·∫©n b·ªã d·ªØ li·ªáu m·ªôt c√°ch hi·ªáu qu·∫£ cho c√°c m√¥ h√¨nh machine learning.

#### M√¥ h√¨nh d·ªØ li·ªáu quan h·ªá
- **B·∫£ng (Table)**: D·ªØ li·ªáu ƒë∆∞·ª£c t·ªï ch·ª©c th√†nh c√°c b·∫£ng, gi·ªëng nh∆∞ c√°c trang t√≠nh Excel. V√≠ d·ª•: b·∫£ng `users`, `products`, `orders`.
- **C·ªôt (Column/Attribute)**: M·ªói c·ªôt ƒë·∫°i di·ªán cho m·ªôt thu·ªôc t√≠nh c·ªßa d·ªØ li·ªáu. V√≠ d·ª•: trong b·∫£ng `users`, c√≥ th·ªÉ c√≥ c√°c c·ªôt `user_id`, `name`, `email`.
- **H√†ng (Row/Record)**: M·ªói h√†ng ƒë·∫°i di·ªán cho m·ªôt th·ª±c th·ªÉ d·ªØ li·ªáu c·ª• th·ªÉ. V√≠ d·ª•: m·ªôt ng∆∞·ªùi d√πng c·ª• th·ªÉ.
- **Kh√≥a ch√≠nh (Primary Key)**: M·ªôt ho·∫∑c nhi·ªÅu c·ªôt ƒë·ªãnh danh duy nh·∫•t cho m·ªói h√†ng trong b·∫£ng. V√≠ d·ª•: `user_id` trong b·∫£ng `users`.
- **Kh√≥a ngo·∫°i (Foreign Key)**: M·ªôt c·ªôt trong m·ªôt b·∫£ng tham chi·∫øu ƒë·∫øn kh√≥a ch√≠nh c·ªßa m·ªôt b·∫£ng kh√°c, t·∫°o ra m·ªëi quan h·ªá gi·ªØa hai b·∫£ng. V√≠ d·ª•: c·ªôt `user_id` trong b·∫£ng `orders` tham chi·∫øu ƒë·∫øn b·∫£ng `users`.

#### C√°c lo·∫°i JOINs

JOINs ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ k·∫øt h·ª£p d·ªØ li·ªáu t·ª´ hai hay nhi·ªÅu b·∫£ng d·ª±a tr√™n m·ªôt c·ªôt chung. H√£y t∆∞·ªüng t∆∞·ª£ng ch√∫ng nh∆∞ c√°c ph√©p to√°n tr√™n t·∫≠p h·ª£p (bi·ªÉu ƒë·ªì Venn).

- **INNER JOIN**: Ch·ªâ tr·∫£ v·ªÅ c√°c h√†ng c√≥ gi√° tr·ªã kh·ªõp ·ªü c·∫£ hai b·∫£ng (ph·∫ßn giao c·ªßa hai t·∫≠p h·ª£p).
    ```sql
    -- L·∫•y th√¥ng tin ng∆∞·ªùi d√πng v√† c√°c ƒë∆°n h√†ng c·ªßa h·ªç
    SELECT u.name, o.order_id, o.amount
    FROM users u
    INNER JOIN orders o ON u.user_id = o.user_id;
    ```
- **LEFT JOIN (ho·∫∑c LEFT OUTER JOIN)**: Tr·∫£ v·ªÅ t·∫•t c·∫£ c√°c h√†ng t·ª´ b·∫£ng b√™n tr√°i v√† c√°c h√†ng kh·ªõp t·ª´ b·∫£ng b√™n ph·∫£i. N·∫øu kh√¥ng c√≥ h√†ng kh·ªõp, c√°c c·ªôt c·ªßa b·∫£ng b√™n ph·∫£i s·∫Ω c√≥ gi√° tr·ªã `NULL`.
    ```sql
    -- L·∫•y t·∫•t c·∫£ ng∆∞·ªùi d√πng, k·ªÉ c·∫£ nh·ªØng ng∆∞·ªùi ch∆∞a c√≥ ƒë∆°n h√†ng n√†o
    SELECT u.name, o.order_id
    FROM users u
    LEFT JOIN orders o ON u.user_id = o.user_id;
    ```
- **RIGHT JOIN (ho·∫∑c RIGHT OUTER JOIN)**: Ng∆∞·ª£c l·∫°i v·ªõi `LEFT JOIN`. Tr·∫£ v·ªÅ t·∫•t c·∫£ c√°c h√†ng t·ª´ b·∫£ng b√™n ph·∫£i.
- **FULL OUTER JOIN**: Tr·∫£ v·ªÅ t·∫•t c·∫£ c√°c h√†ng khi c√≥ s·ª± tr√πng kh·ªõp ·ªü m·ªôt trong hai b·∫£ng. N·∫øu kh√¥ng c√≥ s·ª± tr√πng kh·ªõp, c√°c c·ªôt c·ªßa b·∫£ng kh√¥ng kh·ªõp s·∫Ω l√† `NULL`.

#### Common Table Expressions (CTEs)
- **CTE** (s·ª≠ d·ª•ng m·ªánh ƒë·ªÅ `WITH`) cho ph√©p b·∫°n t·∫°o m·ªôt b·∫£ng t·∫°m th·ªùi, c√≥ t√™n, m√† b·∫°n c√≥ th·ªÉ tham chi·∫øu trong c√¢u l·ªánh `SELECT`, `INSERT`, `UPDATE`, ho·∫∑c `DELETE` ti·∫øp theo.
- **T·∫°i sao d√πng?** Gi√∫p chia nh·ªè c√°c truy v·∫•n ph·ª©c t·∫°p th√†nh c√°c b∆∞·ªõc logic, d·ªÖ ƒë·ªçc v√† d·ªÖ b·∫£o tr√¨ h∆°n.

```sql
-- V√≠ d·ª•: T√¨m nh·ªØng ng∆∞·ªùi d√πng c√≥ t·ªïng chi ti√™u tr√™n 1000
WITH UserSpending AS (
    -- B∆∞·ªõc 1: T√≠nh t·ªïng chi ti√™u cho m·ªói ng∆∞·ªùi d√πng
    SELECT
        user_id,
        SUM(amount) AS total_spent
    FROM orders
    GROUP BY user_id
)
-- B∆∞·ªõc 2: L·ªçc nh·ªØng ng∆∞·ªùi d√πng c√≥ total_spent > 1000
SELECT
    u.name,
    us.total_spent
FROM users u
JOIN UserSpending us ON u.user_id = us.user_id
WHERE us.total_spent > 1000;
```

#### Window Functions

- **L√† g√¨?** Window functions th·ª±c hi·ªán c√°c ph√©p t√≠nh tr√™n m·ªôt t·∫≠p h·ª£p c√°c h√†ng (m·ªôt "c·ª≠a s·ªï") c√≥ li√™n quan ƒë·∫øn h√†ng hi·ªán t·∫°i. Kh√¥ng gi·ªëng nh∆∞ `GROUP BY`, ch√∫ng kh√¥ng g·ªôp c√°c h√†ng l·∫°i m√† tr·∫£ v·ªÅ m·ªôt gi√° tr·ªã cho m·ªói h√†ng.
- **C√∫ ph√°p**: `FUNCTION() OVER (PARTITION BY ... ORDER BY ...)`
    - `PARTITION BY user_id`: Chia d·ªØ li·ªáu th√†nh c√°c "ph√¢n v√πng" cho m·ªói `user_id`. Ph√©p t√≠nh s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán ri√™ng bi·ªát trong m·ªói ph√¢n v√πng n√†y.
    - `ORDER BY order_date`: S·∫Øp x·∫øp c√°c h√†ng trong m·ªói ph√¢n v√πng theo `order_date`.
    - `ROWS BETWEEN 2 PRECEDING AND CURRENT ROW`: ƒê·ªãnh nghƒ©a "c·ª≠a s·ªï" l√† h√†ng hi·ªán t·∫°i v√† 2 h√†ng tr∆∞·ªõc n√≥.
- **·ª®ng d·ª•ng**:
    - T√≠nh to√°n running total, moving average.
    - X·∫øp h·∫°ng (ranking) d·ªØ li·ªáu.
    - So s√°nh gi√° tr·ªã c·ªßa h√†ng hi·ªán t·∫°i v·ªõi c√°c h√†ng l√¢n c·∫≠n.

```sql
-- V√≠ d·ª•: T√≠nh trung b√¨nh tr∆∞·ª£t 3 th√°ng g·∫ßn nh·∫•t cho m·ªói ng∆∞·ªùi d√πng
SELECT 
    user_id,
    order_date,
    amount,
    -- T√≠nh gi√° tr·ªã trung b√¨nh c·ªßa c·ªôt 'amount'
    -- tr√™n c·ª≠a s·ªï bao g·ªìm h√†ng hi·ªán t·∫°i v√† 2 h√†ng tr∆∞·ªõc ƒë√≥
    -- trong c√πng m·ªôt ph√¢n v√πng user_id
    AVG(amount) OVER (
        PARTITION BY user_id 
        ORDER BY order_date 
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    ) as moving_avg_3_orders
FROM orders;
```

**Gi·∫£i th√≠ch c√°c kh√°i ni·ªám:**
- **JOINs**: C√°c ph√©p n·ªëi b·∫£ng ƒë·ªÉ k·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn.
- **CTE (Common Table Expression)**: Gi√∫p c·∫•u tr√∫c c√°c truy v·∫•n SQL ph·ª©c t·∫°p tr·ªü n√™n r√µ r√†ng, d·ªÖ ƒë·ªçc h∆°n.
- **Window function**: Th·ª±c hi·ªán c√°c ph√©p t√≠nh ph·ª©c t·∫°p tr√™n m·ªôt t·∫≠p h·ª£p c√°c h√†ng m√† kh√¥ng l√†m thay ƒë·ªïi s·ªë l∆∞·ª£ng h√†ng c·ªßa k·∫øt qu·∫£. R·∫•t h·ªØu √≠ch cho vi·ªác t·∫°o feature trong ML.

### 1.5 Tr·ª±c quan h√≥a d·ªØ li·ªáu

> **T·∫°i sao c·∫ßn h·ªçc?** Tr·ª±c quan h√≥a gi√∫p hi·ªÉu d·ªØ li·ªáu, ph√°t hi·ªán pattern v√† truy·ªÅn ƒë·∫°t k·∫øt qu·∫£ hi·ªáu qu·∫£.

#### Matplotlib v√† Seaborn

```python
import matplotlib.pyplot as plt
import seaborn as sns

# T·∫°o figure v·ªõi subplots
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Histogram - ph√¢n ph·ªëi d·ªØ li·ªáu
axes[0,0].hist(data, bins=30, alpha=0.7, color='skyblue')
axes[0,0].set_title('Ph√¢n ph·ªëi d·ªØ li·ªáu')
axes[0,0].set_xlabel('Gi√° tr·ªã')
axes[0,0].set_ylabel('T·∫ßn su·∫•t')

# Box plot - ph√¢n ph·ªëi v√† outliers
sns.boxplot(data=data, ax=axes[0,1])
axes[0,1].set_title('Box Plot - Ph√°t hi·ªán outliers')

# Scatter plot - m·ªëi quan h·ªá gi·ªØa hai bi·∫øn
axes[1,0].scatter(x, y, alpha=0.6)
axes[1,0].set_title('M·ªëi quan h·ªá X vs Y')

# Heatmap - ma tr·∫≠n t∆∞∆°ng quan
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ax=axes[1,1])
axes[1,1].set_title('Ma tr·∫≠n t∆∞∆°ng quan')

plt.tight_layout()
plt.show()
```

**Gi·∫£i th√≠ch c√°c lo·∫°i bi·ªÉu ƒë·ªì:**
- **Histogram**: Hi·ªÉn th·ªã ph√¢n ph·ªëi t·∫ßn su·∫•t c·ªßa d·ªØ li·ªáu
- **Box plot**: Hi·ªÉn th·ªã median, quartiles v√† outliers
- **Scatter plot**: Hi·ªÉn th·ªã m·ªëi quan h·ªá gi·ªØa hai bi·∫øn s·ªë
- **Heatmap**: Hi·ªÉn th·ªã ma tr·∫≠n t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn

### 1.6 Git v√† CLI Tools

> **T·∫°i sao c·∫ßn h·ªçc?** Version control gi√∫p qu·∫£n l√Ω code, collaboration. CLI gi√∫p automation v√† t∆∞∆°ng t√°c v·ªõi h·ªá th·ªëng.

#### Git Workflow c∆° b·∫£n

```bash
# Kh·ªüi t·∫°o repository
git init
git remote add origin <repository_url>

# Workflow h√†ng ng√†y
git add .                    # Stage t·∫•t c·∫£ thay ƒë·ªïi
git commit -m "feat: add new ML model"  # Commit v·ªõi message r√µ r√†ng
git push origin main         # Push l√™n remote

# Branch management
git checkout -b feature/new-algorithm    # T·∫°o v√† chuy·ªÉn sang branch m·ªõi
git merge feature/new-algorithm          # Merge branch v√†o main
```

**Gi·∫£i th√≠ch Git concepts:**
- **Stage**: Chu·∫©n b·ªã files ƒë·ªÉ commit
- **Commit**: L∆∞u snapshot c·ªßa code t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
- **Branch**: Nh√°nh ph√°t tri·ªÉn ri√™ng bi·ªát
- **Merge**: K·∫øt h·ª£p code t·ª´ c√°c branch

## üìö T√†i li·ªáu tham kh·∫£o

### Python
- [Python Documentation](https://docs.python.org/3/) - T√†i li·ªáu ch√≠nh th·ª©c
- [Real Python Tutorials](https://realpython.com/) - H∆∞·ªõng d·∫´n th·ª±c t·∫ø
- [Effective Python - Brett Slatkin](https://effectivepython.com/) - Best practices
- [Fluent Python - Luciano Ramalho](https://www.oreilly.com/library/view/fluent-python/9781491946237/) - Python n√¢ng cao

### To√°n h·ªçc
- [Linear Algebra - Gilbert Strang](https://math.mit.edu/~gs/linearalgebra/) - ƒê·∫°i s·ªë tuy·∫øn t√≠nh
- [Probability and Statistics - DeGroot](https://www.pearson.com/en-us/subject-catalog/p/probability-and-statistics/P200000000968/9780134995472) - X√°c su·∫•t th·ªëng k√™

### SQL
- [SQL Tutorial - W3Schools](https://www.w3schools.com/sql/) - H·ªçc SQL c∆° b·∫£n
- [SQL Performance Explained - Markus Winand](https://use-the-index-luke.com/) - T·ªëi ∆∞u hi·ªáu su·∫•t SQL

### Visualization
- [Matplotlib Tutorial](https://matplotlib.org/stable/tutorials/index.html) - H∆∞·ªõng d·∫´n Matplotlib
- [Seaborn Gallery](https://seaborn.pydata.org/examples/index.html) - V√≠ d·ª• Seaborn

## üéØ B√†i t·∫≠p th·ª±c h√†nh

1. **Python**: T·∫°o package Python v·ªõi testing ƒë·∫ßy ƒë·ªß
2. **Data Structures**: Implement c√°c thu·∫≠t to√°n sort v√† search
3. **SQL**: Thi·∫øt k·∫ø database schema cho e-commerce
4. **Visualization**: T·∫°o dashboard cho dataset m·∫´u
5. **Git**: Th·ª±c h√†nh workflow v·ªõi team

## üöÄ B∆∞·ªõc ti·∫øp theo

Sau khi ho√†n th√†nh n·ªÅn t·∫£ng, b·∫°n s·∫Ω:
- Hi·ªÉu s√¢u v·ªÅ Python v√† c√°c c√¥ng c·ª• development
- C√≥ ki·∫øn th·ª©c to√°n h·ªçc c∆° b·∫£n cho ML
- Bi·∫øt c√°ch qu·∫£n l√Ω d·ªØ li·ªáu v·ªõi SQL
- C√≥ th·ªÉ t·∫°o visualization chuy√™n nghi·ªáp
- S·∫µn s√†ng h·ªçc Data Analysis v√† Machine Learning

---

*Ch√∫c b·∫°n h·ªçc t·∫≠p hi·ªáu qu·∫£! üéâ*


# üßÆ To√°n h·ªçc n√¢ng cao cho AI/ML

> **M·ª•c ti√™u**: Hi·ªÉu s√¢u c√°c kh√°i ni·ªám to√°n h·ªçc c·ªët l√µi trong AI/ML, t·ª´ l√Ω thuy·∫øt ƒë·∫øn ·ª©ng d·ª•ng th·ª±c t·∫ø

## üìã T·ªïng quan n·ªôi dung

```mermaid
graph TD
    A[üßÆ To√°n h·ªçc n√¢ng cao] --> B[üìê ƒê·∫°i s·ªë tuy·∫øn t√≠nh n√¢ng cao]
    A --> C[üìà Gi·∫£i t√≠ch & T·ªëi ∆∞u h√≥a]
    A --> D[üé≤ X√°c su·∫•t & Th·ªëng k√™ n√¢ng cao]
    A --> E[üåê L√Ω thuy·∫øt th√¥ng tin]
    A --> F[üìä L√Ω thuy·∫øt h·ªçc m√°y]
    
    B --> B1[SVD & Matrix Factorizations]
    B --> B2[Eigenvalue Theory]
    B --> B3[Tensor Operations]
    
    C --> C1[Convex Optimization]
    C --> C2[Gradient Methods]
    C --> C3[Lagrange Multipliers]
    
    D --> D1[Bayesian Inference]
    D --> D2[Statistical Learning Theory]
    D --> D3[Information Theory]
    
    E --> E1[Entropy & Mutual Information]
    E --> E2[KL Divergence]
    E --> E3[Rate-Distortion Theory]
    
    F --> F1[VC Dimension]
    F --> F2[Generalization Bounds]
    F --> F3[PAC Learning]
```

## üìö **1. B·∫£ng k√Ω hi·ªáu (Notation)**

### **Scalars & Vectors:**
- **Scalars**: $a, b, c$ (s·ªë th·ª±c)
- **Vectors**: $\mathbf{x}, \mathbf{y}, \mathbf{z}$ (vect∆°)
- **Matrices**: $\mathbf{X}, \mathbf{Y}, \mathbf{A}, \mathbf{B}$ (ma tr·∫≠n)
- **Tensors**: $\mathcal{T}$ (tenx∆°)

### **Dataset & Training:**
- **Dataset**: $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n$
- **Features**: $\mathbf{x}_i \in \mathbb{R}^d$
- **Labels**: $y_i \in \mathbb{R}$ (regression) ho·∫∑c $y_i \in \{0,1\}$ (classification)
- **Parameters**: $\boldsymbol{\theta} \in \mathbb{R}^p$

### **Loss & Optimization:**
- **Loss function**: $\mathcal{L}(\boldsymbol{\theta})$
- **Gradient**: $\nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})$
- **Hessian**: $\mathbf{H} = \nabla^2_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})$

### **Probability & Statistics:**
- **Probability**: $P(Y|X)$, $P(X)$
- **Expectation**: $\mathbb{E}[X]$, $\mathbb{E}_{X \sim P}[f(X)]$
- **Variance**: $\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]$
- **Covariance**: $\text{Cov}(X,Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]$

### **Linear Algebra:**
- **Eigenvalues**: $\lambda_1, \lambda_2, \ldots, \lambda_n$
- **Eigenvectors**: $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n$
- **Singular values**: $\sigma_1, \sigma_2, \ldots, \sigma_r$
- **Matrix norm**: $\|\mathbf{A}\|_F$ (Frobenius), $\|\mathbf{A}\|_2$ (spectral)

## üìñ **2. Glossary (ƒê·ªãnh nghƒ©a c·ªët l√µi)**

### **Core Concepts:**
- **Feature**: ƒê·∫∑c tr∆∞ng c·ªßa d·ªØ li·ªáu (v√≠ d·ª•: tu·ªïi, thu nh·∫≠p, ƒëi·ªÉm s·ªë)
- **Label**: Nh√£n c·∫ßn d·ª± ƒëo√°n (v√≠ d·ª•: mua h√†ng hay kh√¥ng, ƒëi·ªÉm s·ªë cu·ªëi k·ª≥)
- **Model**: H√†m $f(\mathbf{x}; \boldsymbol{\theta})$ √°nh x·∫° t·ª´ features sang predictions
- **Parameter**: C√°c gi√° tr·ªã ƒë∆∞·ª£c h·ªçc t·ª´ d·ªØ li·ªáu (weights, biases)
- **Hyperparameter**: C√°c gi√° tr·ªã ƒë∆∞·ª£c set tr∆∞·ªõc (learning rate, batch size)

### **Training Concepts:**
- **Loss**: H√†m ƒëo l·ªói gi·ªØa prediction v√† ground truth
- **Metric**: H√†m ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng m√¥ h√¨nh (accuracy, precision, recall)
- **Overfitting**: M√¥ h√¨nh h·ªçc qu√° k·ªπ training data, k√©m generalization
- **Underfitting**: M√¥ h√¨nh ch∆∞a h·ªçc ƒë·ªß, c·∫£ training v√† test error ƒë·ªÅu cao

### **Regularization & Normalization:**
- **Regularization**: K·ªπ thu·∫≠t ngƒÉn overfitting (L1, L2, dropout)
- **Normalization**: Chu·∫©n h√≥a d·ªØ li·ªáu (StandardScaler, MinMaxScaler)
- **Batch Normalization**: Chu·∫©n h√≥a theo batch trong training
- **Layer Normalization**: Chu·∫©n h√≥a theo layer

## üß© Ch∆∞∆°ng tr√¨nh 50/50 (L√Ω thuy·∫øt : Th·ª±c h√†nh)

| M√¥-ƒëun | L√Ω thuy·∫øt (50%) | Th·ª±c h√†nh (50%) |
|---|---|---|
| ƒê·∫°i s·ªë tuy·∫øn t√≠nh | SVD, Eigenvalue, Tensor | PCA, LDA, Matrix operations |
| Gi·∫£i t√≠ch | Convexity, Optimization | Gradient descent, Newton method |
| X√°c su·∫•t | Bayesian, Information theory | MCMC, Entropy calculation |
| L√Ω thuy·∫øt h·ªçc | VC dimension, PAC learning | Generalization bounds |

---

## üìê **3. Th·∫ª thu·∫≠t to√°n - SVD (Singular Value Decomposition)**

### **1. B√†i to√°n & d·ªØ li·ªáu:**
- **B√†i to√°n**: Ph√¢n t√≠ch ma tr·∫≠n $\mathbf{A} \in \mathbb{R}^{m \times n}$ th√†nh t√≠ch c·ªßa 3 ma tr·∫≠n
- **D·ªØ li·ªáu**: Ma tr·∫≠n b·∫•t k·ª≥ (kh√¥ng c·∫ßn ƒë·ªëi x·ª©ng, kh√¥ng c·∫ßn vu√¥ng)
- **·ª®ng d·ª•ng**: PCA, Recommendation systems, Image compression

### **2. M√¥ h√¨nh & c√¥ng th·ª©c:**
$$\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T$$

Trong ƒë√≥:
- $\mathbf{U} \in \mathbb{R}^{m \times m}$: Left singular vectors (orthogonal)
- $\mathbf{\Sigma} \in \mathbb{R}^{m \times n}$: Singular values (diagonal)
- $\mathbf{V} \in \mathbb{R}^{n \times n}$: Right singular vectors (orthogonal)

### **3. Loss & m·ª•c ti√™u:**
- **M·ª•c ti√™u**: T√¨m decomposition sao cho $\|\mathbf{A} - \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T\|_F$ nh·ªè nh·∫•t
- **Loss**: $\mathcal{L} = \|\mathbf{A} - \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T\|_F^2$

### **4. T·ªëi ∆∞u ho√° & c·∫≠p nh·∫≠t:**
- **Algorithm**: Power iteration ho·∫∑c QR decomposition
- **C·∫≠p nh·∫≠t**: Kh√¥ng c√≥ gradient descent, d√πng eigenvalue decomposition

### **5. Hyperparams:**
- **Rank**: S·ªë singular values gi·ªØ l·∫°i
- **Tolerance**: Ng∆∞·ª°ng d·ª´ng cho convergence

### **6. ƒê·ªô ph·ª©c t·∫°p:**
- **Time**: $O(mn^2)$ cho ma tr·∫≠n $m \times n$
- **Space**: $O(mn)$

### **7. Metrics ƒë√°nh gi√°:**
- **Reconstruction error**: $\|\mathbf{A} - \mathbf{A}_{reconstructed}\|_F$
- **Explained variance ratio**: $\frac{\sum_{i=1}^k \sigma_i^2}{\sum_{i=1}^r \sigma_i^2}$

### **8. ∆Øu / Nh∆∞·ª£c:**
**∆Øu ƒëi·ªÉm:**
- Ho·∫°t ƒë·ªông v·ªõi ma tr·∫≠n b·∫•t k·ª≥
- Stable numerical computation
- C√≥ √Ω nghƒ©a geometric r√µ r√†ng

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Computational cost cao
- Kh√¥ng unique khi c√≥ singular values b·∫±ng nhau

### **9. B·∫´y & m·∫πo:**
- **B·∫´y**: Qu√™n normalize vectors
- **M·∫πo**: D√πng `np.linalg.svd()` thay v√¨ implement t·ª´ ƒë·∫ßu
- **M·∫πo**: Ch·ªâ gi·ªØ top-k singular values ƒë·ªÉ compression

### **10. Pseudocode:**
```python
def svd_decomposition(A):
    # 1. Compute A^T A and AA^T
    ATA = A.T @ A
    AAT = A @ A.T
    
    # 2. Find eigenvalues and eigenvectors
    eigenvals_V, eigenvecs_V = eig(ATA)
    eigenvals_U, eigenvecs_U = eig(AAT)
    
    # 3. Sort by eigenvalues (descending)
    sorted_indices = argsort(eigenvals_V)[::-1]
    V = eigenvecs_V[:, sorted_indices]
    U = eigenvecs_U[:, sorted_indices]
    
    # 4. Compute singular values
    sigma = sqrt(eigenvals_V[sorted_indices])
    Sigma = diag(sigma)
    
    return U, Sigma, V.T
```

### **11. Code m·∫´u:**
```python
import numpy as np
from scipy import linalg

def demonstrate_svd():
    """Demonstrate SVD decomposition"""
    # T·∫°o ma tr·∫≠n m·∫´u
    A = np.random.randn(10, 8)
    
    # SVD decomposition
    U, s, Vt = linalg.svd(A, full_matrices=False)
    
    # Ki·ªÉm tra reconstruction
    A_reconstructed = U @ np.diag(s) @ Vt
    
    print(f"Original matrix shape: {A.shape}")
    print(f"U shape: {U.shape}, s length: {len(s)}, Vt shape: {Vt.shape}")
    print(f"Reconstruction error: {np.linalg.norm(A - A_reconstructed):.2e}")
    
    return U, s, Vt, A_reconstructed
```

### **12. Checklist ki·ªÉm tra nhanh:**
- [ ] Ma tr·∫≠n input c√≥ ƒë√∫ng shape?
- [ ] Singular values c√≥ s·∫Øp x·∫øp gi·∫£m d·∫ßn?
- [ ] U v√† V c√≥ orthogonal?
- [ ] Reconstruction error c√≥ nh·ªè?
- [ ] S·ªë singular values c√≥ ph√π h·ª£p v·ªõi rank?

---

## üìà **4. Th·∫ª thu·∫≠t to√°n - Gradient Descent**

### **1. B√†i to√°n & d·ªØ li·ªáu:**
- **B√†i to√°n**: T√¨m minimum c·ªßa h√†m $f(\boldsymbol{\theta})$
- **D·ªØ li·ªáu**: H√†m differentiable, starting point $\boldsymbol{\theta}_0$
- **·ª®ng d·ª•ng**: Training neural networks, optimization

### **2. M√¥ h√¨nh & c√¥ng th·ª©c:**
$$\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \alpha \nabla f(\boldsymbol{\theta}_t)$$

Trong ƒë√≥:
- $\alpha$: Learning rate
- $\nabla f(\boldsymbol{\theta}_t)$: Gradient t·∫°i ƒëi·ªÉm $\boldsymbol{\theta}_t$

### **3. Loss & m·ª•c ti√™u:**
- **M·ª•c ti√™u**: $\min_{\boldsymbol{\theta}} f(\boldsymbol{\theta})$
- **Loss**: $f(\boldsymbol{\theta})$ (function value)

### **4. T·ªëi ∆∞u ho√° & c·∫≠p nh·∫≠t:**
- **Algorithm**: Iterative gradient descent
- **C·∫≠p nh·∫≠t**: $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \alpha \nabla f(\boldsymbol{\theta})$

### **5. Hyperparams:**
- **Learning rate**: $\alpha$ (th∆∞·ªùng 0.01 - 0.1)
- **Max iterations**: S·ªë l·∫ßn l·∫∑p t·ªëi ƒëa
- **Tolerance**: Ng∆∞·ª°ng d·ª´ng cho gradient norm

### **6. ƒê·ªô ph·ª©c t·∫°p:**
- **Time per iteration**: $O(n)$ v·ªõi $n$ l√† s·ªë parameters
- **Space**: $O(n)$ cho storing parameters

### **7. Metrics ƒë√°nh gi√°:**
- **Function value**: $f(\boldsymbol{\theta})$
- **Gradient norm**: $\|\nabla f(\boldsymbol{\theta})\|$
- **Convergence rate**: T·ªëc ƒë·ªô gi·∫£m c·ªßa function value

### **8. ∆Øu / Nh∆∞·ª£c:**
**∆Øu ƒëi·ªÉm:**
- ƒê∆°n gi·∫£n, d·ªÖ implement
- Ho·∫°t ƒë·ªông v·ªõi m·ªçi h√†m differentiable
- Memory efficient

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Ch·∫≠m convergence v·ªõi ill-conditioned problems
- C·∫ßn tuning learning rate
- C√≥ th·ªÉ stuck ·ªü local minima

### **9. B·∫´y & m·∫πo:**
- **B·∫´y**: Learning rate qu√° l·ªõn ‚Üí divergence
- **B·∫´y**: Learning rate qu√° nh·ªè ‚Üí ch·∫≠m convergence
- **M·∫πo**: D√πng adaptive learning rate (Adam, RMSprop)
- **M·∫πo**: Normalize data tr∆∞·ªõc khi training

### **10. Pseudocode:**
```python
def gradient_descent(f, grad_f, theta0, alpha, max_iter, tol):
    theta = theta0
    for t in range(max_iter):
        gradient = grad_f(theta)
        if norm(gradient) < tol:
            break
        theta = theta - alpha * gradient
    return theta
```

### **11. Code m·∫´u:**
```python
def gradient_descent_example():
    """Gradient descent for quadratic function"""
    
    def f(theta):
        """f(x,y) = x^2 + y^2"""
        return theta[0]**2 + theta[1]**2
    
    def grad_f(theta):
        """Gradient: [2x, 2y]"""
        return np.array([2*theta[0], 2*theta[1]])
    
    # Parameters
    theta0 = np.array([1.0, 1.0])
    alpha = 0.1
    max_iter = 100
    
    # Gradient descent
    theta = theta0.copy()
    trajectory = [theta.copy()]
    
    for i in range(max_iter):
        gradient = grad_f(theta)
        theta = theta - alpha * gradient
        trajectory.append(theta.copy())
        
        if np.linalg.norm(gradient) < 1e-6:
            break
    
    print(f"Final theta: {theta}")
    print(f"Final function value: {f(theta)}")
    
    return np.array(trajectory)
```

### **12. Checklist ki·ªÉm tra nhanh:**
- [ ] Learning rate c√≥ ph√π h·ª£p?
- [ ] Gradient c√≥ ƒë∆∞·ª£c t√≠nh ƒë√∫ng?
- [ ] Function value c√≥ gi·∫£m?
- [ ] C√≥ convergence?
- [ ] C√≥ stuck ·ªü local minimum?

---

## üé≤ **5. Th·∫ª thu·∫≠t to√°n - Bayesian Inference**

### **1. B√†i to√°n & d·ªØ li·ªáu:**
- **B√†i to√°n**: C·∫≠p nh·∫≠t belief v·ªÅ parameters d·ª±a tr√™n data
- **D·ªØ li·ªáu**: Prior $P(\theta)$, Likelihood $P(D|\theta)$, Data $D$
- **·ª®ng d·ª•ng**: Uncertainty quantification, decision making

### **2. M√¥ h√¨nh & c√¥ng th·ª©c:**
**Bayes' Rule:**
$$P(\theta|D) = \frac{P(D|\theta) P(\theta)}{P(D)}$$

Trong ƒë√≥:
- $P(\theta)$: Prior distribution
- $P(D|\theta)$: Likelihood function
- $P(\theta|D)$: Posterior distribution
- $P(D)$: Evidence (normalizing constant)

### **3. Loss & m·ª•c ti√™u:**
- **M·ª•c ti√™u**: T√¨m posterior distribution $P(\theta|D)$
- **Loss**: Negative log-likelihood ho·∫∑c KL divergence

### **4. T·ªëi ∆∞u ho√° & c·∫≠p nh·∫≠t:**
- **Algorithm**: MCMC, Variational inference, Laplace approximation
- **C·∫≠p nh·∫≠t**: Sampling t·ª´ posterior ho·∫∑c optimizing variational parameters

### **5. Hyperparams:**
- **Prior distribution**: Choice of prior (conjugate, non-informative)
- **MCMC parameters**: Number of samples, burn-in period
- **Variational parameters**: Family of approximating distributions

### **6. ƒê·ªô ph·ª©c t·∫°p:**
- **Time**: $O(n \times \text{samples})$ v·ªõi $n$ l√† data size
- **Space**: $O(\text{samples} \times \text{parameters})$

### **7. Metrics ƒë√°nh gi√°:**
- **Posterior mean**: $\mathbb{E}[\theta|D]$
- **Posterior variance**: $\text{Var}[\theta|D]$
- **Credible intervals**: 95% confidence intervals
- **Effective sample size**: ESS for MCMC

### **8. ∆Øu / Nh∆∞·ª£c:**
**∆Øu ƒëi·ªÉm:**
- Quantifies uncertainty
- Incorporates prior knowledge
- Natural for sequential learning

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Computationally expensive
- Sensitive to prior choice
- Difficult to scale to large datasets

### **9. B·∫´y & m·∫πo:**
- **B·∫´y**: Prior qu√° strong ‚Üí bias
- **B·∫´y**: MCMC kh√¥ng converge
- **M·∫πo**: D√πng conjugate priors khi c√≥ th·ªÉ
- **M·∫πo**: Check convergence v·ªõi multiple chains

### **10. Pseudocode:**
```python
def bayesian_inference(prior, likelihood, data):
    # 1. Compute posterior (up to constant)
    def posterior(theta):
        return likelihood(data, theta) * prior(theta)
    
    # 2. Sample from posterior using MCMC
    samples = mcmc_sample(posterior, n_samples=1000)
    
    # 3. Compute posterior statistics
    mean = np.mean(samples)
    std = np.std(samples)
    
    return mean, std, samples
```

### **11. Code m·∫´u:**
```python
def bayesian_linear_regression():
    """Bayesian linear regression with conjugate priors"""
    
    # Generate synthetic data
    np.random.seed(42)
    n_samples = 100
    x = np.random.randn(n_samples, 1)
    true_w = 2.0
    true_b = 1.0
    y = true_w * x + true_b + 0.1 * np.random.randn(n_samples, 1)
    
    # Prior parameters
    prior_w_mean = 0.0
    prior_w_var = 1.0
    prior_b_mean = 0.0
    prior_b_var = 1.0
    
    # Noise variance
    noise_var = 0.1**2
    
    # Bayesian update
    def bayesian_update(x, y, prior_mean, prior_var, noise_var):
        """Bayesian update for linear regression"""
        # Design matrix
        X = np.column_stack([x, np.ones_like(x)])
        
        # Posterior precision (inverse of covariance)
        posterior_precision = X.T @ X / noise_var + 1/prior_var
        posterior_cov = np.linalg.inv(posterior_precision)
        
        # Posterior mean
        posterior_mean = posterior_cov @ (X.T @ y / noise_var + prior_mean / prior_var)
        
        return posterior_mean, posterior_cov
    
    # Update for both parameters
    prior_mean = np.array([prior_w_mean, prior_b_mean])
    prior_var = np.array([prior_w_var, prior_b_var])
    
    posterior_mean, posterior_cov = bayesian_update(x, y, prior_mean, prior_var, noise_var)
    
    print("Bayesian Linear Regression:")
    print(f"True parameters: w={true_w}, b={true_b}")
    print(f"Posterior mean: w={posterior_mean[0]:.4f}, b={posterior_mean[1]:.4f}")
    print(f"Posterior std: w={np.sqrt(posterior_cov[0,0]):.4f}, b={np.sqrt(posterior_cov[1,1]):.4f}")
    
    return posterior_mean, posterior_cov
```

### **12. Checklist ki·ªÉm tra nhanh:**
- [ ] Prior c√≥ reasonable?
- [ ] Likelihood c√≥ ƒë∆∞·ª£c t√≠nh ƒë√∫ng?
- [ ] MCMC c√≥ converge?
- [ ] Posterior c√≥ meaningful?
- [ ] Uncertainty c√≥ ƒë∆∞·ª£c quantify?

---

## üß™ **6. Th·ª±c h√†nh & ·ª®ng d·ª•ng**

### **6.1 PCA v·ªõi SVD**

```python
def pca_implementation():
    """PCA implementation using SVD"""
    
    # Generate sample data
    np.random.seed(42)
    n_samples, n_features = 1000, 10
    
    # Create data with structure
    data = np.random.randn(n_samples, n_features)
    
    # Add some correlation structure
    data[:, 2] = 0.8 * data[:, 0] + 0.2 * np.random.randn(n_samples)
    data[:, 3] = 0.6 * data[:, 1] + 0.4 * np.random.randn(n_samples)
    
    # Center the data
    data_centered = data - np.mean(data, axis=0)
    
    # PCA using SVD
    U, s, Vt = linalg.svd(data_centered, full_matrices=False)
    
    # Project data onto principal components
    data_pca = data_centered @ Vt.T
    
    # Explained variance
    explained_variance = s**2 / (n_samples - 1)
    explained_variance_ratio = explained_variance / np.sum(explained_variance)
    
    # Cumulative explained variance
    cumulative_variance = np.cumsum(explained_variance_ratio)
    
    print("PCA Results:")
    print(f"Original dimensions: {n_features}")
    print(f"Top 3 explained variance ratios: {explained_variance_ratio[:3]}")
    print(f"Cumulative variance (top 3): {cumulative_variance[:3]}")
    
    return data_pca, explained_variance_ratio, cumulative_variance
```

### **6.2 Information Theory**

```python
def information_theory():
    """Information theory concepts"""
    
    def entropy(p):
        """Calculate entropy H(X) = -Œ£ p(x) log p(x)"""
        # Remove zero probabilities to avoid log(0)
        p = p[p > 0]
        return -np.sum(p * np.log2(p))
    
    def mutual_information(p_xy, p_x, p_y):
        """Calculate mutual information I(X;Y)"""
        # I(X;Y) = Œ£ p(x,y) log(p(x,y)/(p(x)p(y)))
        mutual_info = 0
        for i in range(len(p_x)):
            for j in range(len(p_y)):
                if p_xy[i,j] > 0:
                    mutual_info += p_xy[i,j] * np.log2(p_xy[i,j] / (p_x[i] * p_y[j]))
        return mutual_info
    
    def kl_divergence(p, q):
        """Calculate KL divergence D_KL(P||Q)"""
        # D_KL(P||Q) = Œ£ p(x) log(p(x)/q(x))
        # Add small epsilon to avoid log(0)
        epsilon = 1e-10
        p = p + epsilon
        q = q + epsilon
        return np.sum(p * np.log2(p / q))
    
    # Example: Binary random variables
    p_x = np.array([0.7, 0.3])  # P(X=0) = 0.7, P(X=1) = 0.3
    p_y = np.array([0.6, 0.4])  # P(Y=0) = 0.6, P(Y=1) = 0.4
    
    # Joint distribution (example)
    p_xy = np.array([[0.5, 0.2], [0.1, 0.2]])
    
    # Calculate entropies
    h_x = entropy(p_x)
    h_y = entropy(p_y)
    h_xy = entropy(p_xy.flatten())
    
    # Calculate mutual information
    i_xy = mutual_information(p_xy, p_x, p_y)
    
    print(f"Entropy H(X): {h_x:.4f}")
    print(f"Entropy H(Y): {h_y:.4f}")
    print(f"Joint entropy H(X,Y): {h_xy:.4f}")
    print(f"Mutual information I(X;Y): {i_xy:.4f}")
    print(f"Verification: I(X;Y) = H(X) + H(Y) - H(X,Y) = {h_x + h_y - h_xy:.4f}")
    
    return h_x, h_y, h_xy, i_xy
```

---

## üìö **T√†i li·ªáu tham kh·∫£o**

### **S√°ch gi√°o khoa:**
- [Mathematics for Machine Learning](https://mml-book.github.io/) - Deisenroth, Faisal, Ong
- [Linear Algebra Done Right](https://linear.axler.net/) - Sheldon Axler
- [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/) - Boyd & Vandenberghe
- [Pattern Recognition and Machine Learning](https://www.springer.com/gp/book/9780387310732) - Bishop
- [Information Theory, Inference, and Learning Algorithms](https://www.inference.org.uk/mackay/itila/) - MacKay

### **Papers quan tr·ªçng:**
- [Statistical Learning Theory](https://www.springer.com/gp/book/9780387943274) - Vapnik
- [PAC Learning](https://en.wikipedia.org/wiki/Probably_approximately_correct_learning) - Valiant
- [Information Theory](https://ieeexplore.ieee.org/document/6773024) - Shannon

### **Online Resources:**
- [MIT OpenCourseWare - Linear Algebra](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/)
- [Stanford CS229 - Machine Learning](http://cs229.stanford.edu/)
- [CMU 10-701 - Introduction to Machine Learning](https://www.cs.cmu.edu/~tom/10701_sp11/)

---

## üéØ **B√†i t·∫≠p th·ª±c h√†nh**

### **B√†i t·∫≠p 1: SVD Analysis**
1. T·∫°o ma tr·∫≠n 10x8 v·ªõi rank th·∫•p
2. Th·ª±c hi·ªán SVD v√† ph√¢n t√≠ch singular values
3. Reconstruct v·ªõi k singular values ƒë·∫ßu ti√™n
4. T√≠nh reconstruction error

### **B√†i t·∫≠p 2: Bayesian Inference**
1. Implement Bayesian linear regression
2. So s√°nh v·ªõi frequentist approach
3. Visualize posterior distributions
4. T√≠nh credible intervals

### **B√†i t·∫≠p 3: Information Theory**
1. T√≠nh entropy cho c√°c distribution kh√°c nhau
2. Implement mutual information calculation
3. Analyze KL divergence between distributions
4. Apply to feature selection

### **B√†i t·∫≠p 4: Optimization**
1. Implement gradient descent v·ªõi momentum
2. So s√°nh v·ªõi Newton's method
3. Analyze convergence rates
4. Apply to logistic regression

---

## üéì **C√°ch h·ªçc hi·ªáu qu·∫£**

### **B∆∞·ªõc 1: ƒê·ªçc c√¥ng th·ª©c ‚Üí tra k√Ω hi·ªáu ‚Üí hi·ªÉu tr·ª±c gi√°c**
- ƒê·ªçc c√¥ng th·ª©c to√°n h·ªçc
- Tra c·ª©u b·∫£ng k√Ω hi·ªáu ƒë·ªÉ hi·ªÉu t·ª´ng th√†nh ph·∫ßn
- T√¨m hi·ªÉu √Ω nghƒ©a tr·ª±c gi√°c c·ªßa c√¥ng th·ª©c

### **B∆∞·ªõc 2: ƒêi·ªÅn "Th·∫ª thu·∫≠t to√°n" cho t·ª´ng m√¥ h√¨nh**
- Ho√†n th√†nh 12 m·ª•c trong th·∫ª thu·∫≠t to√°n
- Vi·∫øt pseudocode v√† code m·∫´u
- Ki·ªÉm tra checklist

### **B∆∞·ªõc 3: L√†m Lab nh·ªè ‚Üí Mini-project ‚Üí Case study**
- B·∫Øt ƒë·∫ßu v·ªõi lab ƒë∆°n gi·∫£n
- Ti·∫øn t·ªõi mini-project ph·ª©c t·∫°p h∆°n
- √Åp d·ª•ng v√†o case study th·ª±c t·∫ø

### **B∆∞·ªõc 4: ƒê√°nh gi√° b·∫±ng metric ph√π h·ª£p**
- Ch·ªçn metric ƒë√°nh gi√° ph√π h·ª£p v·ªõi b√†i to√°n
- So s√°nh v·ªõi baseline
- Ph√¢n t√≠ch k·∫øt qu·∫£

---

*Ch√∫c b·∫°n h·ªçc t·∫≠p hi·ªáu qu·∫£! üöÄ*

## üìê 1. ƒê·∫°i s·ªë tuy·∫øn t√≠nh n√¢ng cao

### 1.1 Singular Value Decomposition (SVD)

**ƒê·ªãnh nghƒ©a to√°n h·ªçc:**
```python
import numpy as np
from scipy import linalg

def demonstrate_svd():
    """Demonstrate SVD decomposition"""
    # T·∫°o ma tr·∫≠n m·∫´u
    A = np.random.randn(10, 8)
    
    # SVD decomposition
    U, s, Vt = linalg.svd(A, full_matrices=False)
    
    # Ki·ªÉm tra reconstruction
    A_reconstructed = U @ np.diag(s) @ Vt
    
    print(f"Original matrix shape: {A.shape}")
    print(f"U shape: {U.shape}, s length: {len(s)}, Vt shape: {Vt.shape}")
    print(f"Reconstruction error: {np.linalg.norm(A - A_reconstructed):.2e}")
    
    return U, s, Vt, A_reconstructed
```

**·ª®ng d·ª•ng trong ML:**
- **Dimensionality Reduction**: PCA s·ª≠ d·ª•ng SVD
- **Recommendation Systems**: Matrix factorization
- **Image Compression**: SVD cho ma tr·∫≠n ·∫£nh
- **Natural Language Processing**: Latent Semantic Analysis (LSA)

### 1.2 Eigenvalue Theory & Spectral Analysis

**Eigenvalue decomposition:**
```python
def spectral_analysis():
    """Spectral analysis of matrices"""
    # T·∫°o ma tr·∫≠n ƒë·ªëi x·ª©ng
    n = 100
    A = np.random.randn(n, n)
    A = (A + A.T) / 2  # ƒê·ªëi x·ª©ng
    
    # Eigenvalue decomposition
    eigenvalues, eigenvectors = linalg.eigh(A)
    
    # Spectral properties
    spectral_radius = np.max(np.abs(eigenvalues))
    condition_number = np.max(eigenvalues) / np.min(eigenvalues)
    
    print(f"Spectral radius: {spectral_radius:.4f}")
    print(f"Condition number: {condition_number:.4f}")
    
    return eigenvalues, eigenvectors
```

### 1.3 Tensor Operations

**Tensor basics:**
```python
def tensor_operations():
    """Basic tensor operations"""
    # 3D tensor (batch, height, width)
    tensor = np.random.randn(32, 64, 64)
    
    # Tensor reshaping
    flattened = tensor.reshape(32, -1)  # Flatten spatial dimensions
    transposed = np.transpose(tensor, (0, 2, 1))  # Swap height/width
    
    # Tensor contraction (Einstein summation)
    # C_ij = A_ik B_kj
    A = np.random.randn(10, 5)
    B = np.random.randn(5, 8)
    C = np.einsum('ik,kj->ij', A, B)
    
    return tensor, flattened, transposed, C
```

---

## üìà 2. Gi·∫£i t√≠ch & T·ªëi ∆∞u h√≥a n√¢ng cao

### 2.1 Convex Optimization

**ƒê·ªãnh nghƒ©a h√†m l·ªìi:**
```python
def convex_function_demo():
    """Demonstrate convex functions"""
    import matplotlib.pyplot as plt
    
    x = np.linspace(-2, 2, 100)
    
    # H√†m l·ªìi: f(x) = x¬≤
    f_convex = x**2
    
    # H√†m kh√¥ng l·ªìi: f(x) = sin(x)
    f_nonconvex = np.sin(x)
    
    # Ki·ªÉm tra t√≠nh l·ªìi b·∫±ng ƒë·ªãnh nghƒ©a
    def is_convex(f, x):
        """Check if function is convex using definition"""
        alpha = 0.5
        for i in range(1, len(x)-1):
            x1, x2 = x[i-1], x[i+1]
            f1, f2 = f[i-1], f[i+1]
            f_mid = f[i]
            
            # Jensen's inequality: f(Œ±x1 + (1-Œ±)x2) ‚â§ Œ±f(x1) + (1-Œ±)f(x2)
            if f_mid > alpha * f1 + (1-alpha) * f2:
                return False
        return True
    
    print(f"x¬≤ is convex: {is_convex(f_convex, x)}")
    print(f"sin(x) is convex: {is_convex(f_nonconvex, x)}")
    
    return x, f_convex, f_nonconvex
```

### 2.2 Gradient Methods & Optimization

**Advanced gradient methods:**
```python
def advanced_optimization():
    """Advanced optimization methods"""
    
    def rosenbrock(x):
        """Rosenbrock function"""
        return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2
    
    def rosenbrock_gradient(x):
        """Gradient of Rosenbrock function"""
        dx = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)
        dy = 200 * (x[1] - x[0]**2)
        return np.array([dx, dy])
    
    def rosenbrock_hessian(x):
        """Hessian of Rosenbrock function"""
        H = np.zeros((2, 2))
        H[0, 0] = 2 + 1200 * x[0]**2 - 400 * x[1]
        H[0, 1] = H[1, 0] = -400 * x[0]
        H[1, 1] = 200
        return H
    
    # Gradient Descent
    def gradient_descent(f, grad_f, x0, lr=0.001, max_iter=1000):
        x = x0.copy()
        trajectory = [x.copy()]
        
        for i in range(max_iter):
            grad = grad_f(x)
            x = x - lr * grad
            trajectory.append(x.copy())
            
            if np.linalg.norm(grad) < 1e-6:
                break
        
        return np.array(trajectory)
    
    # Newton's Method
    def newton_method(f, grad_f, hess_f, x0, max_iter=100):
        x = x0.copy()
        trajectory = [x.copy()]
        
        for i in range(max_iter):
            grad = grad_f(x)
            hess = hess_f(x)
            
            # Newton step: x = x - H‚Åª¬π‚àáf(x)
            try:
                step = np.linalg.solve(hess, grad)
                x = x - step
                trajectory.append(x.copy())
                
                if np.linalg.norm(step) < 1e-6:
                    break
            except np.linalg.LinAlgError:
                print("Hessian is singular")
                break
        
        return np.array(trajectory)
    
    # Test optimization methods
    x0 = np.array([-1.5, -1.5])
    
    gd_traj = gradient_descent(rosenbrock, rosenbrock_gradient, x0)
    newton_traj = newton_method(rosenbrock, rosenbrock_gradient, rosenbrock_hessian, x0)
    
    print(f"Gradient descent final: {gd_traj[-1]}")
    print(f"Newton method final: {newton_traj[-1]}")
    print(f"True minimum: [1, 1]")
    
    return gd_traj, newton_traj
```

### 2.3 Lagrange Multipliers & Constrained Optimization

**Constrained optimization:**
```python
def lagrange_multipliers():
    """Lagrange multipliers example"""
    
    def objective(x):
        """Objective function: f(x,y) = x¬≤ + y¬≤"""
        return x[0]**2 + x[1]**2
    
    def constraint(x):
        """Constraint: g(x,y) = x + y - 1 = 0"""
        return x[0] + x[1] - 1
    
    def lagrange_function(x, lambda_val):
        """Lagrangian: L(x,Œª) = f(x) + Œªg(x)"""
        return objective(x) + lambda_val * constraint(x)
    
    def solve_lagrange():
        """Solve using Lagrange multipliers"""
        # From ‚àáf + Œª‚àág = 0 and g(x) = 0
        # We get: 2x = Œª, 2y = Œª, x + y = 1
        # Therefore: x = y = 0.5, Œª = 1
        
        x_opt = np.array([0.5, 0.5])
        lambda_opt = 1.0
        
        print(f"Optimal point: {x_opt}")
        print(f"Lagrange multiplier: {lambda_opt}")
        print(f"Objective value: {objective(x_opt)}")
        print(f"Constraint value: {constraint(x_opt)}")
        
        return x_opt, lambda_opt
    
    return solve_lagrange()
```

---

## üé≤ 3. X√°c su·∫•t & Th·ªëng k√™ n√¢ng cao

### 3.1 Bayesian Inference

**Bayesian framework:**
```python
def bayesian_inference():
    """Bayesian inference demonstration"""
    
    # Prior distribution: Beta(Œ±=2, Œ≤=5)
    alpha_prior, beta_prior = 2, 5
    
    # Data: 10 successes out of 20 trials
    n_success, n_trials = 10, 20
    
    # Likelihood: Binomial(n=20, p=Œ∏)
    def likelihood(theta, n_success, n_trials):
        """Binomial likelihood"""
        from scipy.special import comb
        return comb(n_trials, n_success) * theta**n_success * (1-theta)**(n_trials-n_success)
    
    # Posterior: Beta(Œ± + n_success, Œ≤ + n_trials - n_success)
    alpha_posterior = alpha_prior + n_success
    beta_posterior = beta_prior + (n_trials - n_success)
    
    # Posterior mean and variance
    posterior_mean = alpha_posterior / (alpha_posterior + beta_posterior)
    posterior_var = (alpha_posterior * beta_posterior) / ((alpha_posterior + beta_posterior)**2 * (alpha_posterior + beta_posterior + 1))
    
    print(f"Prior: Beta({alpha_prior}, {beta_prior})")
    print(f"Data: {n_success} successes out of {n_trials} trials")
    print(f"Posterior: Beta({alpha_posterior}, {beta_posterior})")
    print(f"Posterior mean: {posterior_mean:.4f}")
    print(f"Posterior std: {np.sqrt(posterior_var):.4f}")
    
    return alpha_posterior, beta_posterior, posterior_mean, posterior_var
```

### 3.2 Statistical Learning Theory

**VC Dimension & Generalization:**
```python
def statistical_learning_theory():
    """Statistical learning theory concepts"""
    
    def vc_dimension_example():
        """VC dimension example for linear classifiers in 2D"""
        # Linear classifiers in 2D can shatter 3 points but not 4 points
        # Therefore VC dimension = 3
        
        def can_shatter(n_points):
            """Check if n points can be shattered by linear classifiers"""
            if n_points <= 3:
                return True
            elif n_points == 4:
                # XOR pattern cannot be shattered by linear classifiers
                return False
            else:
                return False
        
        print("VC dimension for linear classifiers in 2D:")
        for n in range(1, 6):
            print(f"  {n} points: {'Yes' if can_shatter(n) else 'No'}")
    
    def generalization_bound():
        """Generalization bound using VC dimension"""
        # VC dimension
        d_vc = 3
        
        # Sample size
        n = 1000
        
        # Confidence level
        delta = 0.05
        
        # Generalization bound: R(h) ‚â§ R_emp(h) + ‚àö((d_vc * log(2n/d_vc) + log(1/Œ¥))/n)
        empirical_risk = 0.1  # Assume 10% training error
        
        generalization_bound = empirical_risk + np.sqrt((d_vc * np.log(2*n/d_vc) + np.log(1/delta)) / n)
        
        print(f"VC dimension: {d_vc}")
        print(f"Sample size: {n}")
        print(f"Confidence: {1-delta:.1%}")
        print(f"Empirical risk: {empirical_risk:.3f}")
        print(f"Generalization bound: {generalization_bound:.3f}")
        
        return generalization_bound
    
    vc_dimension_example()
    return generalization_bound()
```

### 3.3 Information Theory

**Entropy & Mutual Information:**
```python
def information_theory():
    """Information theory concepts"""
    
    def entropy(p):
        """Calculate entropy H(X) = -Œ£ p(x) log p(x)"""
        # Remove zero probabilities to avoid log(0)
        p = p[p > 0]
        return -np.sum(p * np.log2(p))
    
    def mutual_information(p_xy, p_x, p_y):
        """Calculate mutual information I(X;Y)"""
        # I(X;Y) = Œ£ p(x,y) log(p(x,y)/(p(x)p(y)))
        mutual_info = 0
        for i in range(len(p_x)):
            for j in range(len(p_y)):
                if p_xy[i,j] > 0:
                    mutual_info += p_xy[i,j] * np.log2(p_xy[i,j] / (p_x[i] * p_y[j]))
        return mutual_info
    
    def kl_divergence(p, q):
        """Calculate KL divergence D_KL(P||Q)"""
        # D_KL(P||Q) = Œ£ p(x) log(p(x)/q(x))
        # Add small epsilon to avoid log(0)
        epsilon = 1e-10
        p = p + epsilon
        q = q + epsilon
        return np.sum(p * np.log2(p / q))
    
    # Example: Binary random variables
    p_x = np.array([0.7, 0.3])  # P(X=0) = 0.7, P(X=1) = 0.3
    p_y = np.array([0.6, 0.4])  # P(Y=0) = 0.6, P(Y=1) = 0.4
    
    # Joint distribution (example)
    p_xy = np.array([[0.5, 0.2], [0.1, 0.2]])
    
    # Calculate entropies
    h_x = entropy(p_x)
    h_y = entropy(p_y)
    h_xy = entropy(p_xy.flatten())
    
    # Calculate mutual information
    i_xy = mutual_information(p_xy, p_x, p_y)
    
    print(f"Entropy H(X): {h_x:.4f}")
    print(f"Entropy H(Y): {h_y:.4f}")
    print(f"Joint entropy H(X,Y): {h_xy:.4f}")
    print(f"Mutual information I(X;Y): {i_xy:.4f}")
    print(f"Verification: I(X;Y) = H(X) + H(Y) - H(X,Y) = {h_x + h_y - h_xy:.4f}")
    
    return h_x, h_y, h_xy, i_xy
```

---

## üìä 4. L√Ω thuy·∫øt h·ªçc m√°y

### 4.1 PAC Learning

**Probably Approximately Correct Learning:**
```python
def pac_learning():
    """PAC learning framework"""
    
    def pac_bound(epsilon, delta, d_vc, n):
        """PAC generalization bound"""
        # With probability at least 1-Œ¥:
        # R(h) ‚â§ R_emp(h) + ‚àö((d_vc * log(2n/d_vc) + log(1/Œ¥))/n)
        
        empirical_risk = 0.1  # Assume 10% training error
        generalization_bound = empirical_risk + np.sqrt((d_vc * np.log(2*n/d_vc) + np.log(1/delta)) / n)
        
        return generalization_bound
    
    def sample_complexity(epsilon, delta, d_vc):
        """Sample complexity for PAC learning"""
        # n ‚â• (1/Œµ¬≤) * (d_vc * log(1/Œµ) + log(1/Œ¥))
        n = (1/epsilon**2) * (d_vc * np.log(1/epsilon) + np.log(1/delta))
        return int(np.ceil(n))
    
    # Example parameters
    epsilon = 0.1  # Accuracy parameter
    delta = 0.05   # Confidence parameter
    d_vc = 3       # VC dimension
    
    # Calculate sample complexity
    n_required = sample_complexity(epsilon, delta, d_vc)
    
    # Calculate generalization bound for given sample size
    n_actual = 1000
    gen_bound = pac_bound(epsilon, delta, d_vc, n_actual)
    
    print(f"PAC Learning Parameters:")
    print(f"  Accuracy (Œµ): {epsilon}")
    print(f"  Confidence (Œ¥): {delta}")
    print(f"  VC dimension: {d_vc}")
    print(f"  Required sample size: {n_required}")
    print(f"  Actual sample size: {n_actual}")
    print(f"  Generalization bound: {gen_bound:.4f}")
    
    return n_required, gen_bound
```

### 4.2 Rademacher Complexity

**Rademacher complexity bounds:**
```python
def rademacher_complexity():
    """Rademacher complexity demonstration"""
    
    def empirical_rademacher_complexity(hypothesis_class, data, n_samples=1000):
        """Estimate empirical Rademacher complexity"""
        n = len(data)
        rademacher_sum = 0
        
        for _ in range(n_samples):
            # Generate random Rademacher variables
            sigma = np.random.choice([-1, 1], size=n)
            
            # Find best hypothesis for this Rademacher sequence
            best_value = -np.inf
            for h in hypothesis_class:
                value = np.sum(sigma * h(data))
                best_value = max(best_value, value)
            
            rademacher_sum += best_value
        
        return rademacher_sum / (n * n_samples)
    
    # Example: Linear classifiers in 1D
    def linear_classifier_1d(data, threshold):
        """Linear classifier in 1D"""
        return np.where(data > threshold, 1, -1)
    
    # Generate sample data
    data = np.random.randn(100)
    
    # Define hypothesis class (linear classifiers with different thresholds)
    hypothesis_class = [lambda x, t=t: linear_classifier_1d(x, t) for t in np.linspace(-2, 2, 20)]
    
    # Estimate Rademacher complexity
    rad_complexity = empirical_rademacher_complexity(hypothesis_class, data)
    
    print(f"Estimated Rademacher complexity: {rad_complexity:.4f}")
    print(f"Theoretical bound: O(1/‚àön) ‚âà {1/np.sqrt(len(data)):.4f}")
    
    return rad_complexity
```

---

## üß™ 5. Th·ª±c h√†nh & ·ª®ng d·ª•ng

### 5.1 PCA v·ªõi SVD

```python
def pca_implementation():
    """PCA implementation using SVD"""
    
    # Generate sample data
    np.random.seed(42)
    n_samples, n_features = 1000, 10
    
    # Create data with structure
    data = np.random.randn(n_samples, n_features)
    
    # Add some correlation structure
    data[:, 2] = 0.8 * data[:, 0] + 0.2 * np.random.randn(n_samples)
    data[:, 3] = 0.6 * data[:, 1] + 0.4 * np.random.randn(n_samples)
    
    # Center the data
    data_centered = data - np.mean(data, axis=0)
    
    # PCA using SVD
    U, s, Vt = linalg.svd(data_centered, full_matrices=False)
    
    # Project data onto principal components
    data_pca = data_centered @ Vt.T
    
    # Explained variance
    explained_variance = s**2 / (n_samples - 1)
    explained_variance_ratio = explained_variance / np.sum(explained_variance)
    
    # Cumulative explained variance
    cumulative_variance = np.cumsum(explained_variance_ratio)
    
    print("PCA Results:")
    print(f"Original dimensions: {n_features}")
    print(f"Top 3 explained variance ratios: {explained_variance_ratio[:3]}")
    print(f"Cumulative variance (top 3): {cumulative_variance[:3]}")
    
    return data_pca, explained_variance_ratio, cumulative_variance
```

### 5.2 Bayesian Linear Regression

```python
def bayesian_linear_regression():
    """Bayesian linear regression implementation"""
    
    # Generate synthetic data
    np.random.seed(42)
    n_samples = 100
    x = np.random.randn(n_samples, 1)
    true_w = 2.0
    true_b = 1.0
    y = true_w * x + true_b + 0.1 * np.random.randn(n_samples, 1)
    
    # Prior parameters
    prior_w_mean = 0.0
    prior_w_var = 1.0
    prior_b_mean = 0.0
    prior_b_var = 1.0
    
    # Noise variance
    noise_var = 0.1**2
    
    # Bayesian update
    def bayesian_update(x, y, prior_mean, prior_var, noise_var):
        """Bayesian update for linear regression"""
        # Design matrix
        X = np.column_stack([x, np.ones_like(x)])
        
        # Posterior precision (inverse of covariance)
        posterior_precision = X.T @ X / noise_var + 1/prior_var
        posterior_cov = np.linalg.inv(posterior_precision)
        
        # Posterior mean
        posterior_mean = posterior_cov @ (X.T @ y / noise_var + prior_mean / prior_var)
        
        return posterior_mean, posterior_cov
    
    # Update for both parameters
    prior_mean = np.array([prior_w_mean, prior_b_mean])
    prior_var = np.array([prior_w_var, prior_b_var])
    
    posterior_mean, posterior_cov = bayesian_update(x, y, prior_mean, prior_var, noise_var)
    
    print("Bayesian Linear Regression:")
    print(f"True parameters: w={true_w}, b={true_b}")
    print(f"Posterior mean: w={posterior_mean[0]:.4f}, b={posterior_mean[1]:.4f}")
    print(f"Posterior std: w={np.sqrt(posterior_cov[0,0]):.4f}, b={np.sqrt(posterior_cov[1,1]):.4f}")
    
    return posterior_mean, posterior_cov
```

---

## üìö T√†i li·ªáu tham kh·∫£o

### **S√°ch gi√°o khoa:**
- [Mathematics for Machine Learning](https://mml-book.github.io/) - Deisenroth, Faisal, Ong
- [Linear Algebra Done Right](https://linear.axler.net/) - Sheldon Axler
- [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/) - Boyd & Vandenberghe
- [Pattern Recognition and Machine Learning](https://www.springer.com/gp/book/9780387310732) - Bishop
- [Information Theory, Inference, and Learning Algorithms](https://www.inference.org.uk/mackay/itila/) - MacKay

### **Papers quan tr·ªçng:**
- [Statistical Learning Theory](https://www.springer.com/gp/book/9780387943274) - Vapnik
- [PAC Learning](https://en.wikipedia.org/wiki/Probably_approximately_correct_learning) - Valiant
- [Information Theory](https://ieeexplore.ieee.org/document/6773024) - Shannon

### **Online Resources:**
- [MIT OpenCourseWare - Linear Algebra](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/)
- [Stanford CS229 - Machine Learning](http://cs229.stanford.edu/)
- [CMU 10-701 - Introduction to Machine Learning](https://www.cs.cmu.edu/~tom/10701_sp11/)

---

## üéØ B√†i t·∫≠p th·ª±c h√†nh

### **B√†i t·∫≠p 1: SVD Analysis**
1. T·∫°o ma tr·∫≠n 10x8 v·ªõi rank th·∫•p
2. Th·ª±c hi·ªán SVD v√† ph√¢n t√≠ch singular values
3. Reconstruct v·ªõi k singular values ƒë·∫ßu ti√™n
4. T√≠nh reconstruction error

### **B√†i t·∫≠p 2: Bayesian Inference**
1. Implement Bayesian linear regression
2. So s√°nh v·ªõi frequentist approach
3. Visualize posterior distributions
4. T√≠nh credible intervals

### **B√†i t·∫≠p 3: Information Theory**
1. T√≠nh entropy cho c√°c distribution kh√°c nhau
2. Implement mutual information calculation
3. Analyze KL divergence between distributions
4. Apply to feature selection

### **B√†i t·∫≠p 4: Optimization**
1. Implement gradient descent v·ªõi momentum
2. So s√°nh v·ªõi Newton's method
3. Analyze convergence rates
4. Apply to logistic regression

---

*Ch√∫c b·∫°n h·ªçc t·∫≠p hi·ªáu qu·∫£! üöÄ*
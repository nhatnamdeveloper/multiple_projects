# ğŸ¨ Generative Models - MÃ´ hÃ¬nh sinh

> **Má»¥c tiÃªu**: Hiá»ƒu cÃ¡c kiáº¿n trÃºc vÃ  nguyÃªn lÃ½ Ä‘áº±ng sau cÃ¡c mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng *táº¡o ra* dá»¯ liá»‡u má»›i (áº£nh, vÄƒn báº£n, Ã¢m thanh), táº­p trung vÃ o GANs, VAEs vÃ  Diffusion Models.

## ğŸ“‹ Tá»•ng quan ná»™i dung

```mermaid
graph TD
    A[ğŸ¨ Generative Models] --> B[ğŸ§  Ná»n táº£ng lÃ½ thuyáº¿t]
    A --> C[âš”ï¸ Generative Adversarial Networks (GANs)]
    A --> D[ğŸ§¬ Variational Autoencoders (VAEs)]
    A --> E[ğŸŒ«ï¸ Diffusion Models]
    A --> F[ğŸ“Š ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh sinh]
    
    B --> B1[Latent Space (KhÃ´ng gian áº©n)]
    B --> B2[Likelihood-based vs. Implicit Models]
    B --> B3[Generative Learning Trilemma]
    
    C --> C1[Kiáº¿n trÃºc Generator & Discriminator]
    C --> C2[HÃ m máº¥t mÃ¡t Min-Max]
    C --> C3[CÃ¡c váº¥n Ä‘á» (Mode Collapse, Instability)]
    C --> C4[CÃ¡c biáº¿n thá»ƒ (DCGAN, WGAN, StyleGAN)]
    
    D --> D1[Kiáº¿n trÃºc Encoder & Decoder]
    D --> D2[Reparameterization Trick]
    D --> D3[HÃ m máº¥t mÃ¡t (Reconstruction + KL Divergence)]
    
    E --> E1[Forward & Reverse Process]
    E --> E2[Noise Schedule]
    E --> E3[U-Net Denoising Model]
    
    F --> F1[Inception Score (IS)]
    F --> F2[FrÃ©chet Inception Distance (FID)]
    F --> F3[Precision & Recall]
```

## ğŸ“š 1. Báº£ng kÃ½ hiá»‡u (Notation)

- **Real Data ($x$)**: Dá»¯ liá»‡u tháº­t tá»« táº­p huáº¥n luyá»‡n.
- **Latent Vector ($z$)**: Vector ngáº«u nhiÃªn trong khÃ´ng gian áº©n, dÃ¹ng lÃ m "háº¡t giá»‘ng" Ä‘á»ƒ sinh dá»¯ liá»‡u.
- **Generator ($G$)**: Máº¡ng nÆ¡-ron sinh ra dá»¯ liá»‡u giáº£, $G(z) = \hat{x}$.
- **Discriminator ($D$)**: Máº¡ng nÆ¡-ron phÃ¢n biá»‡t dá»¯ liá»‡u tháº­t/giáº£, $D(x)$ tráº£ vá» xÃ¡c suáº¥t $x$ lÃ  tháº­t.
- **Encoder ($E$)**: Máº¡ng nÆ¡-ron mÃ£ hÃ³a dá»¯ liá»‡u tháº­t vÃ o khÃ´ng gian áº©n, $E(x) = z$.
- **Decoder ($Dec$)**: Máº¡ng nÆ¡-ron giáº£i mÃ£ tá»« khÃ´ng gian áº©n ra dá»¯ liá»‡u, $Dec(z) = \hat{x}$.

## ğŸ“– 2. Glossary (Äá»‹nh nghÄ©a cá»‘t lÃµi)

-   **Generative Model**: MÃ´ hÃ¬nh há»c phÃ¢n phá»‘i xÃ¡c suáº¥t $P(x)$ cá»§a dá»¯ liá»‡u vÃ  cÃ³ thá»ƒ sinh ra cÃ¡c máº«u má»›i tá»« phÃ¢n phá»‘i Ä‘Ã³.
-   **Discriminative Model**: MÃ´ hÃ¬nh há»c ranh giá»›i quyáº¿t Ä‘á»‹nh giá»¯a cÃ¡c lá»›p, hay há»c xÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n $P(y|x)$.
-   **Latent Space**: KhÃ´ng gian biá»ƒu diá»…n cÃ³ sá»‘ chiá»u tháº¥p hÆ¡n, nÆ¡i cÃ¡c Ä‘áº·c trÆ°ng cá»‘t lÃµi, trá»«u tÆ°á»£ng cá»§a dá»¯ liá»‡u Ä‘Æ°á»£c mÃ£ hÃ³a.
-   **Mode Collapse**: Má»™t lá»—i phá»• biáº¿n cá»§a GAN khi Generator chá»‰ há»c cÃ¡ch táº¡o ra má»™t vÃ i máº«u dá»¯ liá»‡u giáº£ ráº¥t thuyáº¿t phá»¥c thay vÃ¬ toÃ n bá»™ sá»± Ä‘a dáº¡ng cá»§a dá»¯ liá»‡u tháº­t.
-   **Reparameterization Trick**: Má»™t ká»¹ thuáº­t toÃ¡n há»c cho phÃ©p backpropagation cÃ³ thá»ƒ "cháº£y" qua má»™t node láº¥y máº«u ngáº«u nhiÃªn (stochastic node), lÃ  chÃ¬a khÃ³a Ä‘á»ƒ huáº¥n luyá»‡n VAE.
-   **Diffusion Process**: QuÃ¡ trÃ¬nh thÃªm nhiá»…u (noise) dáº§n dáº§n vÃ o dá»¯ liá»‡u (forward) vÃ  sau Ä‘Ã³ há»c cÃ¡ch khá»­ nhiá»…u Ä‘á»ƒ tÃ¡i táº¡o láº¡i dá»¯ liá»‡u gá»‘c (reverse).

---

## ğŸ§  3. Ná»n táº£ng lÃ½ thuyáº¿t: KhÃ´ng gian áº©n (Latent Space)

> **TÆ° tÆ°á»Ÿng cá»‘t lÃµi**: Háº§u háº¿t dá»¯ liá»‡u trong tháº¿ giá»›i thá»±c (nhÆ° áº£nh chÃ¢n dung) Ä‘á»u cÃ³ má»™t cáº¥u trÃºc tiá»m áº©n. Thay vÃ¬ náº±m ngáº«u nhiÃªn trong khÃ´ng gian pixel (vÃ­ dá»¥: áº£nh nhiá»…u), chÃºng náº±m trÃªn má»™t **manifold** cÃ³ sá»‘ chiá»u tháº¥p hÆ¡n nhiá»u. VÃ­ dá»¥, táº¥t cáº£ cÃ¡c áº£nh chÃ¢n dung ngÆ°á»i Ä‘á»u cÃ³ chung cÃ¡c Ä‘áº·c Ä‘iá»ƒm nhÆ° "cÃ³ máº¯t", "cÃ³ mÅ©i", "miá»‡ng á»Ÿ dÆ°á»›i mÅ©i".

-   **Latent Space (KhÃ´ng gian áº©n)** lÃ  má»™t khÃ´ng gian cÃ³ sá»‘ chiá»u tháº¥p hÆ¡n dÃ¹ng Ä‘á»ƒ biá»ƒu diá»…n cÃ¡c Ä‘áº·c trÆ°ng trá»«u tÆ°á»£ng nÃ y.
-   **VÃ­ dá»¥**: Má»™t khÃ´ng gian áº©n 2 chiá»u cho áº£nh chÃ¢n dung cÃ³ thá»ƒ cÃ³ má»™t trá»¥c lÃ  "Ä‘á»™ tuá»•i" vÃ  trá»¥c cÃ²n láº¡i lÃ  "gÃ³c nhÃ¬n khuÃ´n máº·t".
-   **Má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh sinh**:
    1.  Há»c má»™t **Encoder** Ä‘á»ƒ Ã¡nh xáº¡ má»™t bá»©c áº£nh tháº­t $x$ vÃ o má»™t Ä‘iá»ƒm $z$ trong khÃ´ng gian áº©n.
    2.  Há»c má»™t **Decoder (hay Generator)** Ä‘á»ƒ Ã¡nh xáº¡ má»™t Ä‘iá»ƒm $z$ báº¥t ká»³ trong khÃ´ng gian áº©n trá»Ÿ láº¡i má»™t bá»©c áº£nh thá»±c táº¿ $\hat{x}$.

Náº¿u há»c thÃ nh cÃ´ng, ta cÃ³ thá»ƒ láº¥y má»™t Ä‘iá»ƒm $z$ ngáº«u nhiÃªn trong khÃ´ng gian nÃ y vÃ  dÃ¹ng Decoder Ä‘á»ƒ sinh ra má»™t bá»©c áº£nh chÃ¢n dung hoÃ n toÃ n má»›i chÆ°a tá»«ng tá»“n táº¡i.

---

## âš”ï¸ 4. Generative Adversarial Networks (GANs)

GAN Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi Ian Goodfellow vÃ  cá»™ng sá»± vÃ o nÄƒm 2014, dá»±a trÃªn má»™t Ã½ tÆ°á»Ÿng Ä‘á»™c Ä‘Ã¡o vá» má»™t "trÃ² chÆ¡i" giá»¯a hai máº¡ng nÆ¡-ron.

### 4.1 TÆ° duy trá»±c quan: TrÃ² chÆ¡i MÃ¨o vá»n Chuá»™t
HÃ£y tÆ°á»Ÿng tÆ°á»£ng má»™t cuá»™c Ä‘á»‘i Ä‘áº§u giá»¯a hai nhÃ¢n váº­t:
1.  **Generator (G)**: Má»™t **há»a sÄ© chuyÃªn lÃ m tranh giáº£**. Má»¥c tiÃªu cá»§a G lÃ  váº½ ra nhá»¯ng bá»©c tranh giáº£ trÃ´ng y nhÆ° tháº­t Ä‘á»ƒ lá»«a ngÆ°á»i khÃ¡c.
2.  **Discriminator (D)**: Má»™t **nhÃ  phÃª bÃ¬nh nghá»‡ thuáº­t**. Má»¥c tiÃªu cá»§a D lÃ  phÃ¢n biá»‡t Ä‘Ã¢u lÃ  tranh tháº­t (tá»« bá»™ sÆ°u táº­p gá»‘c) vÃ  Ä‘Ã¢u lÃ  tranh giáº£ do G váº½ ra.

**QuÃ¡ trÃ¬nh huáº¥n luyá»‡n:**
-   **VÃ²ng 1 (Huáº¥n luyá»‡n Discriminator)**:
    -   ÄÆ°a cho D má»™t ná»­a lÃ  tranh tháº­t vÃ  má»™t ná»­a lÃ  tranh giáº£ do G váº½.
    -   Dá»± Ä‘oÃ¡n cá»§a D Ä‘Æ°á»£c so sÃ¡nh vá»›i nhÃ£n tháº­t/giáº£.
    -   Cáº­p nháº­t trá»ng sá»‘ cá»§a D Ä‘á»ƒ nÃ³ ngÃ y cÃ ng giá»i hÆ¡n trong viá»‡c phÃ¢n biá»‡t.
-   **VÃ²ng 2 (Huáº¥n luyá»‡n Generator)**:
    -   G váº½ ra má»™t bá»©c tranh giáº£ vÃ  Ä‘Æ°a cho D.
    -   G muá»‘n D pháº£i tin ráº±ng Ä‘Ã¢y lÃ  tranh tháº­t (tá»©c $D(G(z))$ pháº£i tiáº¿n vá» 1).
    -   Lá»—i cá»§a D Ä‘Æ°á»£c lan truyá»n ngÆ°á»£c láº¡i Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘ cá»§a **chá»‰ G**, giÃºp G há»c cÃ¡ch váº½ ra nhá»¯ng bá»©c tranh ngÃ y cÃ ng thuyáº¿t phá»¥c hÆ¡n.

QuÃ¡ trÃ¬nh nÃ y láº·p Ä‘i láº·p láº¡i. D ngÃ y cÃ ng tinh vi, buá»™c G cÅ©ng pháº£i ngÃ y cÃ ng tiáº¿n bá»™. Cuá»‘i cÃ¹ng, G sáº½ táº¡o ra nhá»¯ng sáº£n pháº©m giáº£ mÃ  D khÃ´ng thá»ƒ phÃ¢n biá»‡t Ä‘Æ°á»£c ná»¯a.

### 4.2 HÃ m máº¥t mÃ¡t Min-Max
TrÃ² chÆ¡i nÃ y Ä‘Æ°á»£c mÃ´ táº£ báº±ng hÃ m máº¥t mÃ¡t min-max:
$$ \min_{G} \max_{D} V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)}[\log(1 - D(G(z)))] $$

-   **Pháº§n $\max_{D}$**: Discriminator `D` muá»‘n tá»‘i Ä‘a hÃ³a hÃ m nÃ y. NÃ³ muá»‘n $D(x)$ (xÃ¡c suáº¥t áº£nh tháº­t lÃ  tháº­t) tiáº¿n vá» 1 vÃ  $D(G(z))$ (xÃ¡c suáº¥t áº£nh giáº£ lÃ  tháº­t) tiáº¿n vá» 0.
-   **Pháº§n $\min_{G}$**: Generator `G` muá»‘n tá»‘i thiá»ƒu hÃ³a hÃ m nÃ y. NÃ³ muá»‘n $D(G(z))$ tiáº¿n vá» 1, lÃ m cho váº¿ thá»© hai cá»§a cÃ´ng thá»©c trá»Ÿ nÃªn Ã¢m vÃ´ cÃ¹ng.

### 4.3 CÃ¡c váº¥n Ä‘á» thÆ°á»ng gáº·p khi huáº¥n luyá»‡n GAN
-   **Training Instability**: QuÃ¡ trÃ¬nh huáº¥n luyá»‡n GAN ráº¥t "mong manh". Náº¿u D quÃ¡ giá»i, G sáº½ khÃ´ng há»c Ä‘Æ°á»£c gÃ¬ (gradient báº±ng 0). Náº¿u G quÃ¡ giá»i, nÃ³ sáº½ dá»… dÃ ng lá»«a D vÃ  khÃ´ng cáº£i thiá»‡n thÃªm.
-   **Mode Collapse**: G chá»‰ tÃ¬m ra má»™t vÃ i "chiÃªu" Ä‘á»ƒ lá»«a D (vÃ­ dá»¥: chá»‰ váº½ Ä‘Ãºng má»™t loáº¡i chÃ³ trÃ´ng ráº¥t tháº­t). NÃ³ khÃ´ng há»c Ä‘Æ°á»£c toÃ n bá»™ phÃ¢n phá»‘i cá»§a dá»¯ liá»‡u mÃ  chá»‰ "sá»¥p Ä‘á»•" vÃ o má»™t vÃ i mode.
-   **Vanishing Gradients**: Náº¿u D quÃ¡ tá»± tin, gradient cho G sáº½ trá»Ÿ nÃªn ráº¥t nhá», khiáº¿n G há»c ráº¥t cháº­m hoáº·c khÃ´ng há»c Ä‘Æ°á»£c.

### 4.4 CÃ¡c biáº¿n thá»ƒ quan trá»ng
-   **DCGAN (Deep Convolutional GAN)**:
    -   **Má»¥c tiÃªu**: á»”n Ä‘á»‹nh huáº¥n luyá»‡n GAN báº±ng cÃ¡ch káº¿t há»£p cÃ¡c kiáº¿n trÃºc CNN.
    -   **Äá» xuáº¥t kiáº¿n trÃºc**:
        -   Thay tháº¿ má»i lá»›p pooling báº±ng **strided convolutions** (trong Discriminator) vÃ  **fractional-strided convolutions** (Deconvolution/ConvTranspose2d trong Generator).
        -   Sá»­ dá»¥ng **Batch Normalization** trong cáº£ Generator vÃ  Discriminator (trá»« layer output cá»§a G vÃ  layer input cá»§a D).
        -   Sá»­ dá»¥ng **ReLU** cho cÃ¡c lá»›p Generator (trá»« layer output dÃ¹ng Tanh).
        -   Sá»­ dá»¥ng **LeakyReLU** cho cÃ¡c lá»›p Discriminator.
-   **WGAN (Wasserstein GAN)**:
    -   **Váº¥n Ä‘á» GAN truyá»n thá»‘ng**: HÃ m máº¥t mÃ¡t cross-entropy cá»§a GAN dá»±a trÃªn khoáº£ng cÃ¡ch Jensen-Shannon (JS divergence), cÃ³ thá»ƒ gÃ¢y ra **vanishing gradients** khi hai phÃ¢n phá»‘i (tháº­t vÃ  giáº£) khÃ´ng chá»“ng cháº­p nhiá»u (common support), Ä‘iá»u thÆ°á»ng xuyÃªn xáº£y ra khi huáº¥n luyá»‡n.
    -   **Giáº£i phÃ¡p**: Thay tháº¿ JS divergence báº±ng **Wasserstein distance** (Earth Mover's Distance).
    -   **CÃ¡ch hoáº¡t Ä‘á»™ng**: Äá»ƒ Ä‘áº£m báº£o Lipschitz continuity (má»™t Ä‘iá»u kiá»‡n cáº§n Ä‘á»ƒ Wasserstein distance cÃ³ Ä‘áº¡o hÃ m tá»‘t), WGAN sá»­ dá»¥ng **weight clipping** (cáº¯t giá»›i háº¡n trá»ng sá»‘) hoáº·c **gradient penalty** (thÃªm penalty vÃ o gradient cá»§a Discriminator).
    -   **Lá»£i Ã­ch**: GiÃºp huáº¥n luyá»‡n á»•n Ä‘á»‹nh hÆ¡n, Ã­t bá»‹ mode collapse hÆ¡n, vÃ  Discriminator (trong WGAN gá»i lÃ  Critic) tráº£ vá» má»™t giÃ¡ trá»‹ cÃ³ Ã½ nghÄ©a hÆ¡n (Æ°á»›c tÃ­nh khoáº£ng cÃ¡ch Wasserstein).
-   **StyleGAN**: Má»™t kiáº¿n trÃºc phá»©c táº¡p cho phÃ©p kiá»ƒm soÃ¡t cÃ¡c khÃ­a cáº¡nh khÃ¡c nhau cá»§a áº£nh Ä‘Æ°á»£c sinh ra (vÃ­ dá»¥: tuá»•i, giá»›i tÃ­nh, kiá»ƒu tÃ³c trong áº£nh chÃ¢n dung) thÃ´ng qua khÃ´ng gian áº©n.

### 1. BÃ i toÃ¡n & dá»¯ liá»‡u
- **BÃ i toÃ¡n**: Táº¡o ra hÃ¬nh áº£nh cháº¥t lÆ°á»£ng cao tá»« nhiá»…u ngáº«u nhiÃªn, sá»­ dá»¥ng kiáº¿n trÃºc máº¡ng tÃ­ch cháº­p (CNNs).
- **Dá»¯ liá»‡u**: Táº­p há»£p áº£nh tháº­t (vÃ­ dá»¥: MNIST, CelebA).
- **á»¨ng dá»¥ng**: Sinh áº£nh chÃ¢n dung, váº­t thá»ƒ, tÄƒng cÆ°á»ng dá»¯ liá»‡u.

### 2. MÃ´ hÃ¬nh & cÃ´ng thá»©c
- **Generator ($G$)**: Máº¡ng CNN chuyá»ƒn Ä‘á»•i latent vector `z` thÃ nh áº£nh $\hat{x}$. Sá»­ dá»¥ng `ConvTranspose2d` (Deconvolution) Ä‘á»ƒ upsample.
- **Discriminator ($D$)**: Máº¡ng CNN phÃ¢n loáº¡i áº£nh Ä‘áº§u vÃ o $x$ lÃ  tháº­t hay giáº£.
- **Kiáº¿n trÃºc Ä‘á» xuáº¥t**:
  -   Thay tháº¿ Pooling layer báº±ng cÃ¡c bÆ°á»›c tiáº¿n tÃ­ch cháº­p (strided convolutions) trong D vÃ  tÃ­ch cháº­p phÃ¢n sá»‘ (fractional-strided convolutions / deconvolution) trong G.
  -   Sá»­ dá»¥ng Batch Normalization trong cáº£ G vÃ  D (trá»« layer output cá»§a G vÃ  layer input cá»§a D).
  -   Sá»­ dá»¥ng ReLU trong G (trá»« layer output dÃ¹ng Tanh).
  -   Sá»­ dá»¥ng LeakyReLU trong D.

### 3. Loss & má»¥c tiÃªu
- **Má»¥c tiÃªu**: Huáº¥n luyá»‡n $G$ Ä‘á»ƒ sinh áº£nh giáº£ $G(z)$ mÃ  $D$ khÃ´ng thá»ƒ phÃ¢n biá»‡t Ä‘Æ°á»£c vá»›i áº£nh tháº­t, Ä‘á»“ng thá»i huáº¥n luyá»‡n $D$ Ä‘á»ƒ phÃ¢n biá»‡t áº£nh tháº­t vÃ  giáº£.
- **HÃ m máº¥t mÃ¡t**: HÃ m máº¥t mÃ¡t nhá»‹ phÃ¢n cross-entropy (`nn.BCELoss`) cho cáº£ G vÃ  D.

### 4. Tá»‘i Æ°u hoÃ¡ & cáº­p nháº­t
- **Algorithm**: Huáº¥n luyá»‡n D vÃ  G xen káº½.
  1.  **Huáº¥n luyá»‡n D**: TÃ­nh loss cá»§a D trÃªn áº£nh tháº­t vÃ  áº£nh giáº£, thá»±c hiá»‡n má»™t bÆ°á»›c tá»‘i Æ°u hÃ³a Ä‘á»ƒ tá»‘i Ä‘a hÃ³a $D(x)$ vÃ  tá»‘i thiá»ƒu hÃ³a $D(G(z))$.
  2.  **Huáº¥n luyá»‡n G**: TÃ­nh loss cá»§a G (báº±ng cÃ¡ch cá»‘ gáº¯ng lÃ m cho $D(G(z))$ gáº§n 1), thá»±c hiá»‡n má»™t bÆ°á»›c tá»‘i Æ°u hÃ³a Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a $1 - D(G(z))$.
- **Optimizer**: ThÆ°á»ng dÃ¹ng Adam.

### 5. Hyperparams
- **Batch Size**: 64-128.
- **Learning Rate**: 0.0002.
- **Adam Betas**: (0.5, 0.999).
- **Latent Vector Size**: 100 chiá»u.

### 6. Äá»™ phá»©c táº¡p
- **Time**: Tá»‘n kÃ©m, Ä‘áº·c biá»‡t cho G (do upsampling) vÃ  D (do xá»­ lÃ½ CNN).
- **Space**: Tá»‘n bá»™ nhá»› VRAM, cáº§n GPU máº¡nh.

### 7. Metrics Ä‘Ã¡nh giÃ¡
- **Inception Score (IS)**, **FrÃ©chet Inception Distance (FID)**: Äá»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng vÃ  sá»± Ä‘a dáº¡ng cá»§a áº£nh sinh ra.
- **Quan sÃ¡t báº±ng máº¯t**: Ráº¥t quan trá»ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ trá»±c quan.

### 8. Æ¯u / NhÆ°á»£c Ä‘iá»ƒm
**Æ¯u Ä‘iá»ƒm**:
- Táº¡o áº£nh cÃ³ Ä‘á»™ phÃ¢n giáº£i cao vÃ  chÃ¢n thá»±c.
- Kiáº¿n trÃºc CNN giÃºp há»c cÃ¡c Ä‘áº·c trÆ°ng khÃ´ng gian tá»‘t.

**NhÆ°á»£c Ä‘iá»ƒm**:
- KhÃ³ huáº¥n luyá»‡n (training instability).
- Dá»… bá»‹ Mode Collapse.
- YÃªu cáº§u cÃ¢n báº±ng cáº©n tháº­n giá»¯a G vÃ  D.

### 9. Báº«y & máº¹o
- **Báº«y**: Training Instability, Mode Collapse.
- **Máº¹o**: Sá»­ dá»¥ng kiáº¿n trÃºc DCGAN vá»›i cÃ¡c hÆ°á»›ng dáº«n Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh.
- **Máº¹o**: Giáº£m Learning Rate cho Discriminator.

### 10. Pseudocode:
```python
# Khá»Ÿi táº¡o G vÃ  D
for epoch in range(num_epochs):
    for batch in dataloader:
        # 1. Huáº¥n luyá»‡n Discriminator
        D.zero_grad()
        real_images = batch
        batch_size = real_images.size(0)
        
        # Loss trÃªn áº£nh tháº­t
        output_real = D(real_images)
        loss_D_real = criterion(output_real, labels_real) # labels_real = 1
        loss_D_real.backward()
        
        # Loss trÃªn áº£nh giáº£
        noise = sample_latent_vector()
        fake_images = G(noise)
        output_fake = D(fake_images.detach()) # .detach() Ä‘á»ƒ khÃ´ng cáº­p nháº­t G
        loss_D_fake = criterion(output_fake, labels_fake) # labels_fake = 0
        loss_D_fake.backward()
        
        loss_D = loss_D_real + loss_D_fake
        optimizer_D.step()
        
        # 2. Huáº¥n luyá»‡n Generator
        G.zero_grad()
        output_fake_from_G = D(fake_images) # KhÃ´ng .detach()
        loss_G = criterion(output_fake_from_G, labels_real) # G muá»‘n D nghÄ© áº£nh giáº£ lÃ  tháº­t
        loss_G.backward()
        optimizer_G.step()
```

### 11. Code máº«u (kiáº¿n trÃºc DCGAN cÆ¡ báº£n)
```python
import torch
import torch.nn as nn

class Generator(nn.Module):
    def __init__(self, latent_dim, img_channels, img_size):
        super(Generator, self).__init__()
        self.img_size = img_size
        self.main = nn.Sequential(
            # input is Z, going into a convolution
            nn.ConvTranspose2d(latent_dim, 256, 4, 1, 0, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            # state size. (256)x4x4
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            # state size. (128)x8x8
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            # state size. (64)x16x16
            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),
            nn.Tanh()
            # state size. (img_channels)x32x32
        )

    def forward(self, input):
        return self.main(input)

class Discriminator(nn.Module):
    def __init__(self, img_channels, img_size):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            # input is (img_channels)x32x32
            nn.Conv2d(img_channels, 64, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (64)x16x16
            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (128)x8x8
            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (256)x4x4
            nn.Conv2d(256, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, input):
        return self.main(input).view(-1, 1)

# VÃ­ dá»¥ khá»Ÿi táº¡o
# latent_dim = 100
# img_channels = 1 # For MNIST
# img_size = 32
# netG = Generator(latent_dim, img_channels, img_size)
# netD = Discriminator(img_channels, img_size)
# print(netG)
# print(netD)
```

### 12. Checklist kiá»ƒm tra nhanh:
- [ ] HÃ m máº¥t mÃ¡t cÃ³ Ä‘Æ°á»£c tÃ­nh Ä‘Ãºng cho G vÃ  D?
- [ ] Kiáº¿n trÃºc cÃ³ tuÃ¢n theo cÃ¡c hÆ°á»›ng dáº«n DCGAN khÃ´ng?
- [ ] Optimizer cÃ³ Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘Ãºng khÃ´ng (learning rate, betas)?
- [ ] CÃ³ cÃ¢n báº±ng giá»¯a khÃ¡m phÃ¡ vÃ  khai thÃ¡c khÃ´ng (epsilon-greedy)?
- [ ] áº¢nh sinh ra cÃ³ cháº¥t lÆ°á»£ng vÃ  Ä‘a dáº¡ng khÃ´ng?

---
5. Variational Autoencoders (VAEs)

VAE lÃ  má»™t loáº¡i mÃ´ hÃ¬nh sinh khÃ¡c, cÃ³ ná»n táº£ng lÃ½ thuyáº¿t vá»¯ng cháº¯c hÆ¡n GAN vÃ  thÆ°á»ng dá»… huáº¥n luyá»‡n hÆ¡n.

### 5.1 Kiáº¿n trÃºc Encoder-Decoder
VAE bao gá»“m hai pháº§n:
1.  **Encoder**: Nháº­n má»™t áº£nh Ä‘áº§u vÃ o $x$, vÃ  mÃ£ hÃ³a nÃ³ thÃ nh má»™t **phÃ¢n phá»‘i xÃ¡c suáº¥t** trong khÃ´ng gian áº©n. Thay vÃ¬ mÃ£ hÃ³a thÃ nh má»™t Ä‘iá»ƒm duy nháº¥t $z$, nÃ³ mÃ£ hÃ³a thÃ nh má»™t phÃ¢n phá»‘i hÃ¬nh chuÃ´ng (Gaussian) vá»›i trung bÃ¬nh $\mu$ vÃ  Ä‘á»™ lá»‡ch chuáº©n $\sigma$.
2.  **Decoder**: Láº¥y má»™t Ä‘iá»ƒm $z$ Ä‘Æ°á»£c **láº¥y máº«u (sampled)** tá»« phÃ¢n phá»‘i Ä‘Ã³ vÃ  cá»‘ gáº¯ng tÃ¡i táº¡o láº¡i áº£nh gá»‘c $\hat{x}$.

### 5.2 Reparameterization Trick
-   **Váº¥n Ä‘á»**: QuÃ¡ trÃ¬nh "láº¥y máº«u ngáº«u nhiÃªn" tá»« phÃ¢n phá»‘i $(\mu, \sigma)$ lÃ  má»™t phÃ©p toÃ¡n ngáº«u nhiÃªn, khÃ´ng cÃ³ Ä‘áº¡o hÃ m, do Ä‘Ã³ khÃ´ng thá»ƒ lan truyá»n ngÆ°á»£c gradient qua nÃ³.
-   **Giáº£i phÃ¡p (Reparameterization Trick)**: Thay vÃ¬ láº¥y máº«u trá»±c tiáº¿p, ta biáº¿n Ä‘á»•i nÃ³:
    $$ z = \mu + \sigma \odot \epsilon $$
    Trong Ä‘Ã³ $\epsilon$ lÃ  má»™t biáº¿n nhiá»…u ngáº«u nhiÃªn láº¥y tá»« phÃ¢n phá»‘i chuáº©n $N(0, 1)$.
-   **Táº¡i sao hiá»‡u quáº£?**: Báº±ng cÃ¡ch nÃ y, sá»± ngáº«u nhiÃªn Ä‘Æ°á»£c "tÃ¡ch" ra khá»i máº¡ng. Máº¡ng chá»‰ há»c cÃ¡ch táº¡o ra $\mu$ vÃ  $\sigma$, cÃ²n gradient cÃ³ thá»ƒ cháº£y ngÆ°á»£c qua cÃ¡c phÃ©p nhÃ¢n vÃ  cá»™ng má»™t cÃ¡ch bÃ¬nh thÆ°á»ng.

### 5.3 HÃ m máº¥t mÃ¡t kÃ©p (Dual Loss Function)
Loss cá»§a VAE bao gá»“m hai thÃ nh pháº§n:
$$ L(\theta, \phi) = \underbrace{\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{Reconstruction Loss}} - \underbrace{D_{KL}(q_\phi(z|x) || p(z))}_{\text{KL Divergence}} $$
1.  **Reconstruction Loss**: Äo lÆ°á»ng má»©c Ä‘á»™ giá»‘ng nhau giá»¯a áº£nh tÃ¡i táº¡o $\hat{x}$ vÃ  áº£nh gá»‘c $x$. NÃ³ buá»™c mÃ´ hÃ¬nh pháº£i há»c cÃ¡ch mÃ£ hÃ³a táº¥t cáº£ cÃ¡c thÃ´ng tin cáº§n thiáº¿t vÃ o khÃ´ng gian áº©n.
2.  **KL Divergence**: ÄÃ¢y lÃ  má»™t thÃ nh pháº§n **regularizer**. NÃ³ Ä‘o lÆ°á»ng sá»± khÃ¡c biá»‡t giá»¯a phÃ¢n phá»‘i $(\mu, \sigma)$ mÃ  Encoder táº¡o ra vÃ  má»™t phÃ¢n phá»‘i chuáº©n $N(0, 1)$. NÃ³ buá»™c Encoder pháº£i táº¡o ra cÃ¡c khÃ´ng gian áº©n "gá»n gÃ ng", cÃ³ cáº¥u trÃºc tá»‘t, trÃ¡nh viá»‡c cÃ¡c "cá»¥m" dá»¯ liá»‡u náº±m quÃ¡ xa nhau.

### 5.4 So sÃ¡nh GANs vÃ  VAEs
| Äáº·c Ä‘iá»ƒm | GANs | VAEs |
| :--- | :--- | :--- |
| **Cháº¥t lÆ°á»£ng áº£nh** | Sáº¯c nÃ©t, chÃ¢n thá»±c hÆ¡n. | Má» hÆ¡n, "trung bÃ¬nh" hÆ¡n. |
| **Äá»™ á»•n Ä‘á»‹nh** | KhÃ³ huáº¥n luyá»‡n, dá»… sá»¥p Ä‘á»•. | Dá»… huáº¥n luyá»‡n hÆ¡n, á»•n Ä‘á»‹nh hÆ¡n. |
| **KhÃ´ng gian áº©n** | KhÃ´ng cÃ³ cáº¥u trÃºc rÃµ rÃ ng. | CÃ³ cáº¥u trÃºc, liÃªn tá»¥c. |
| **Ná»n táº£ng** | LÃ½ thuyáº¿t trÃ² chÆ¡i. | LÃ½ thuyáº¿t xÃ¡c suáº¥t (Bayesian). |
| **Má»¥c tiÃªu** | ÄÃ¡nh lá»«a Discriminator. | Tá»‘i Ä‘a hÃ³a lower bound cá»§a log-likelihood. |

## ğŸŒ«ï¸ 6. Diffusion Models

ÄÃ¢y lÃ  kiáº¿n trÃºc mÃ´ hÃ¬nh sinh hiá»‡n Ä‘áº¡i vÃ  máº¡nh máº½ nháº¥t hiá»‡n nay, Ä‘á»©ng sau cÃ¡c mÃ´ hÃ¬nh ná»•i tiáº¿ng nhÆ° DALL-E 2, Midjourney, vÃ  Stable Diffusion.

### 6.1 TÆ° duy trá»±c quan: ThÃªm nhiá»…u vÃ  Khá»­ nhiá»…u
QuÃ¡ trÃ¬nh hoáº¡t Ä‘á»™ng cá»§a Diffusion Model bao gá»“m hai bÆ°á»›c:

1.  **Forward Process (QuÃ¡ trÃ¬nh xuÃ´i - Cá»‘ Ä‘á»‹nh)**:
    -   Báº¯t Ä‘áº§u vá»›i má»™t bá»©c áº£nh tháº­t $x_0$.
    -   ThÃªm má»™t chÃºt nhiá»…u (noise) vÃ o áº£nh Ä‘á»ƒ táº¡o ra $x_1$.
    -   ThÃªm má»™t chÃºt nhiá»…u vÃ o $x_1$ Ä‘á»ƒ táº¡o ra $x_2$.
    -   Láº·p láº¡i quÃ¡ trÃ¬nh nÃ y `T` láº§n (vÃ­ dá»¥: `T=1000`) cho Ä‘áº¿n khi áº£nh $x_T$ trá»Ÿ thÃ nh nhiá»…u hoÃ n toÃ n (pure noise).
    -   QuÃ¡ trÃ¬nh nÃ y lÃ  cá»‘ Ä‘á»‹nh vÃ  khÃ´ng cáº§n há»c.

2.  **Reverse Process (QuÃ¡ trÃ¬nh ngÆ°á»£c - Pháº£i há»c)**:
    -   ÄÃ¢y lÃ  pháº§n cá»‘t lÃµi cá»§a mÃ´ hÃ¬nh.
    -   MÃ´ hÃ¬nh (thÆ°á»ng lÃ  má»™t kiáº¿n trÃºc U-Net) Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ lÃ m má»™t viá»‡c duy nháº¥t: **dá»± Ä‘oÃ¡n nhiá»…u Ä‘Ã£ Ä‘Æ°á»£c thÃªm vÃ o á»Ÿ má»™t bÆ°á»›c báº¥t ká»³**.
    -   Nhiá»‡m vá»¥ cá»§a nÃ³ lÃ : nháº­n vÃ o má»™t áº£nh nhiá»…u $x_t$ vÃ  time step $t$, vÃ  dá»± Ä‘oÃ¡n ra nhiá»…u $\epsilon$ Ä‘Ã£ Ä‘Æ°á»£c thÃªm vÃ o $x_{t-1}$ Ä‘á»ƒ táº¡o ra $x_t$.
    -   Sau khi Ä‘Æ°á»£c huáº¥n luyá»‡n, Ä‘á»ƒ sinh áº£nh má»›i:
        -   Báº¯t Ä‘áº§u vá»›i má»™t áº£nh nhiá»…u hoÃ n toÃ n ngáº«u nhiÃªn $x_T$.
        -   DÃ¹ng mÃ´ hÃ¬nh Ä‘á»ƒ dá»± Ä‘oÃ¡n nhiá»…u trong $x_T$, sau Ä‘Ã³ trá»« nhiá»…u Ä‘Ã³ Ä‘i Ä‘á»ƒ táº¡o ra $x_{T-1}$.
        -   Láº·p láº¡i quÃ¡ trÃ¬nh: tá»« $x_{T-1}$ táº¡o ra $x_{T-2}$,... cho Ä‘áº¿n khi ta cÃ³ Ä‘Æ°á»£c $x_0$, má»™t bá»©c áº£nh sáº¡ch vÃ  hoÃ n toÃ n má»›i.

### 6.2 Táº¡i sao Diffusion Models hiá»‡u quáº£?
-   **BÃ i toÃ¡n Ä‘Æ¡n giáº£n**: Thay vÃ¬ há»c cÃ¡ch sinh ra má»™t bá»©c áº£nh phá»©c táº¡p tá»« Ä‘áº§u, mÃ´ hÃ¬nh chá»‰ cáº§n há»c má»™t nhiá»‡m vá»¥ Ä‘Æ¡n giáº£n hÆ¡n nhiá»u lÃ  "khá»­ nhiá»…u".
-   **Huáº¥n luyá»‡n á»•n Ä‘á»‹nh**: QuÃ¡ trÃ¬nh training á»•n Ä‘á»‹nh hÆ¡n nhiá»u so vá»›i GANs.
-   **Cháº¥t lÆ°á»£ng vÃ  Ä‘a dáº¡ng**: Cho káº¿t quáº£ sinh áº£nh vá»«a sáº¯c nÃ©t, vá»«a Ä‘a dáº¡ng, káº¿t há»£p Ä‘Æ°á»£c Æ°u Ä‘iá»ƒm cá»§a cáº£ GANs vÃ  VAEs.

### 6.3 Äiá»u khiá»ƒn táº¡o áº£nh: Conditional Diffusion (Diffusion cÃ³ Ä‘iá»u kiá»‡n)
-   **Má»¥c tiÃªu**: Thay vÃ¬ sinh áº£nh ngáº«u nhiÃªn, ta muá»‘n sinh áº£nh theo má»™t Ä‘iá»u kiá»‡n nÃ o Ä‘Ã³ (vÃ­ dá»¥: sinh áº£nh chÃ³, hoáº·c sinh áº£nh tá»« text "con mÃ¨o Ä‘ang bay").
-   **CÃ¡ch hoáº¡t Ä‘á»™ng**: Trong quÃ¡ trÃ¬nh khá»­ nhiá»…u (reverse process), ta cung cáº¥p thÃªm thÃ´ng tin Ä‘iá»u kiá»‡n (vÃ­ dá»¥: one-hot vector cá»§a nhÃ£n lá»›p, hoáº·c embedding cá»§a text) cho mÃ´ hÃ¬nh khá»­ nhiá»…u (thÆ°á»ng lÃ  U-Net). MÃ´ hÃ¬nh sáº½ há»c cÃ¡ch káº¿t há»£p thÃ´ng tin nÃ y Ä‘á»ƒ táº¡o ra áº£nh phÃ¹ há»£p vá»›i Ä‘iá»u kiá»‡n.

### 6.4 TÄƒng tá»‘c Ä‘á»™ vÃ  hiá»‡u quáº£: Latent Diffusion Models (LDMs)
-   **Váº¥n Ä‘á»**: CÃ¡c Diffusion Models truyá»n thá»‘ng hoáº¡t Ä‘á»™ng trá»±c tiáº¿p trÃªn khÃ´ng gian pixel. Äiá»u nÃ y ráº¥t tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n vÃ  bá»™ nhá»›, Ä‘áº·c biá»‡t vá»›i áº£nh Ä‘á»™ phÃ¢n giáº£i cao.
-   **Giáº£i phÃ¡p**: Thay vÃ¬ cháº¡y quÃ¡ trÃ¬nh diffusion trÃªn áº£nh gá»‘c, Latent Diffusion Models (LDMs) nÃ©n áº£nh vÃ o má»™t **khÃ´ng gian tiá»m áº©n (latent space)** cÃ³ sá»‘ chiá»u tháº¥p hÆ¡n báº±ng má»™t Autoencoder Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c. QuÃ¡ trÃ¬nh diffusion sau Ä‘Ã³ diá»…n ra hoÃ n toÃ n trong khÃ´ng gian tiá»m áº©n nÃ y.
-   **CÃ¡ch hoáº¡t Ä‘á»™ng**:
    1.  **Encoder**: NÃ©n áº£nh pixel $x$ thÃ nh biá»ƒu diá»…n tiá»m áº©n $z$.
    2.  **Diffusion trong Latent Space**: Ãp dá»¥ng quÃ¡ trÃ¬nh thÃªm nhiá»…u vÃ  khá»­ nhiá»…u trong khÃ´ng gian tiá»m áº©n $z$.
    3.  **Decoder**: Giáº£i mÃ£ biá»ƒu diá»…n tiá»m áº©n Ä‘Ã£ Ä‘Æ°á»£c khá»­ nhiá»…u trá»Ÿ láº¡i khÃ´ng gian pixel Ä‘á»ƒ táº¡o ra áº£nh cuá»‘i cÃ¹ng.
-   **Lá»£i Ã­ch**: Giáº£m Ä‘Ã¡ng ká»ƒ chi phÃ­ tÃ­nh toÃ¡n, cho phÃ©p huáº¥n luyá»‡n vÃ  sinh áº£nh nhanh hÆ¡n mÃ  váº«n giá»¯ Ä‘Æ°á»£c cháº¥t lÆ°á»£ng cao. **Stable Diffusion** lÃ  má»™t vÃ­ dá»¥ ná»•i báº­t cá»§a LDM.
## ğŸ“Š 7. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh sinh (Evaluation of Generative Models)

ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh sinh lÃ  má»™t bÃ i toÃ¡n khÃ³, vÃ¬ khÃ´ng cÃ³ má»™t "Ä‘Ã¡p Ã¡n Ä‘Ãºng" duy nháº¥t. Ta cáº§n Ä‘o lÆ°á»ng hai yáº¿u tá»‘: **cháº¥t lÆ°á»£ng (quality)** vÃ  **tÃ­nh Ä‘a dáº¡ng (diversity)** cá»§a cÃ¡c máº«u Ä‘Æ°á»£c sinh ra.

### 7.1 Inception Score (IS)
- **TÆ° tÆ°á»Ÿng**: Má»™t mÃ´ hÃ¬nh tá»‘t sáº½ sinh ra nhá»¯ng hÃ¬nh áº£nh **rÃµ rÃ ng** (dá»… phÃ¢n loáº¡i) vÃ  **Ä‘a dáº¡ng** (bao trÃ¹m nhiá»u lá»›p khÃ¡c nhau).
- **CÃ¡ch hoáº¡t Ä‘á»™ng**:
    1.  DÃ¹ng má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i áº£nh (Inception Net) Ä‘Ã£ Ä‘Æ°á»£c pre-trained trÃªn ImageNet.
    2.  Cho mÃ´ hÃ¬nh sinh táº¡o ra nhiá»u áº£nh.
    3.  Vá»›i má»—i áº£nh, láº¥y phÃ¢n phá»‘i xÃ¡c suáº¥t trÃªn cÃ¡c lá»›p tá»« Inception Net ($P(y|x)$).
    4.  **Cháº¥t lÆ°á»£ng**: Náº¿u áº£nh rÃµ rÃ ng, $P(y|x)$ sáº½ cÃ³ entropy tháº¥p (vÃ­ dá»¥: ráº¥t cháº¯c cháº¯n Ä‘Ã¢y lÃ  áº£nh "chÃ³").
    5.  **Äa dáº¡ng**: PhÃ¢n phá»‘i xÃ¡c suáº¥t trung bÃ¬nh cá»§a táº¥t cáº£ cÃ¡c áº£nh ($P(y)$) pháº£i cÃ³ entropy cao (mÃ´ hÃ¬nh sinh ra nhiá»u loáº¡i áº£nh khÃ¡c nhau).
- **CÃ´ng thá»©c**: $IS = \exp(\mathbb{E}_x [D_{KL}(P(y|x) || P(y))])$
- **NhÆ°á»£c Ä‘iá»ƒm**: KhÃ´ng so sÃ¡nh vá»›i dá»¯ liá»‡u tháº­t, cÃ³ thá»ƒ bá»‹ "lá»«a" bá»Ÿi cÃ¡c mÃ´ hÃ¬nh chá»‰ sinh ra má»™t áº£nh Ä‘áº¹p cho má»—i lá»›p.

### 7.2 FrÃ©chet Inception Distance (FID)
- **TÆ° tÆ°á»Ÿng**: So sÃ¡nh phÃ¢n phá»‘i cá»§a cÃ¡c áº£nh tháº­t vÃ  áº£nh giáº£ trong khÃ´ng gian Ä‘áº·c trÆ°ng cá»§a má»™t máº¡ng nÆ¡-ron. FID cÃ ng tháº¥p, hai phÃ¢n phá»‘i cÃ ng gáº§n nhau, mÃ´ hÃ¬nh cÃ ng tá»‘t.
- **CÃ¡ch hoáº¡t Ä‘á»™ng**:
    1.  Láº¥y má»™t táº­p áº£nh tháº­t vÃ  má»™t táº­p áº£nh do mÃ´ hÃ¬nh sinh ra.
    2.  ÄÆ°a cáº£ hai táº­p áº£nh qua máº¡ng Inception Net (Ä‘Ã£ bá» lá»›p cuá»‘i) Ä‘á»ƒ láº¥y cÃ¡c vector Ä‘áº·c trÆ°ng (feature vectors).
    3.  Modelling hai táº­p vector Ä‘áº·c trÆ°ng nÃ y nhÆ° hai phÃ¢n phá»‘i Gaussian Ä‘a biáº¿n. TÃ­nh toÃ¡n trung bÃ¬nh ($\mu$) vÃ  ma tráº­n hiá»‡p phÆ°Æ¡ng sai ($\Sigma$) cho má»—i táº­p.
    4.  TÃ­nh khoáº£ng cÃ¡ch FrÃ©chet giá»¯a hai phÃ¢n phá»‘i Gaussian nÃ y.
- **CÃ´ng thá»©c**: $FID(x, g) = ||\mu_x - \mu_g||^2 + \text{Tr}(\Sigma_x + \Sigma_g - 2(\Sigma_x \Sigma_g)^{1/2})$
- **Æ¯u Ä‘iá»ƒm**: Robust hÆ¡n IS, cÃ³ tÆ°Æ¡ng quan tá»‘t hÆ¡n vá»›i cháº¥t lÆ°á»£ng áº£nh mÃ  con ngÆ°á»i cáº£m nháº­n. LÃ  metric tiÃªu chuáº©n hiá»‡n nay Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ GANs vÃ  cÃ¡c mÃ´ hÃ¬nh sinh áº£nh khÃ¡c.

## ğŸ¯ 8. BÃ i táº­p thá»±c hÃ nh
1.  **DCGAN trÃªn MNIST**: Implement vÃ  huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh Deep Convolutional GAN Ä‘Æ¡n giáº£n trÃªn bá»™ dá»¯ liá»‡u chá»¯ sá»‘ viáº¿t tay MNIST. Cá»‘ gáº¯ng táº¡o ra nhá»¯ng chá»¯ sá»‘ trÃ´ng nhÆ° tháº­t.
2.  **VAE trÃªn Fashion-MNIST**: Huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh VAE trÃªn bá»™ dá»¯ liá»‡u Fashion-MNIST. Trá»±c quan hÃ³a khÃ´ng gian áº©n 2D vÃ  thá»­ sinh ra cÃ¡c sáº£n pháº©m thá»i trang má»›i báº±ng cÃ¡ch di chuyá»ƒn trong khÃ´ng gian Ä‘Ã³.
3.  **Text-to-Image vá»›i Diffusion**: Sá»­ dá»¥ng thÆ° viá»‡n `diffusers` cá»§a Hugging Face Ä‘á»ƒ cháº¡y má»™t mÃ´ hÃ¬nh Stable Diffusion Ä‘Ã£ Ä‘Æ°á»£c pre-trained. Thá»­ nghiá»‡m vá»›i cÃ¡c prompt khÃ¡c nhau Ä‘á»ƒ táº¡o ra cÃ¡c bá»©c áº£nh Ä‘á»™c Ä‘Ã¡o.

## ğŸ“š 9. TÃ i liá»‡u tham kháº£o
-   **GANs**: "Generative Adversarial Nets" - Goodfellow et al. (2014)
-   **VAEs**: "Auto-Encoding Variational Bayes" - Kingma & Welling (2013)
-   **Diffusion Models**: "Denoising Diffusion Probabilistic Models" - Ho et al. (2020)
-   **Tutorials**:
    -   [PyTorch DCGAN Tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)
    -   [Hugging Face Diffusers Library](https://huggingface.co/docs/diffusers/index)
    -   [Blog: "Intuitively Understanding Variational Autoencoders"](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)

---
*ChÃºc báº¡n há»c táº­p hiá»‡u quáº£! ğŸš€*
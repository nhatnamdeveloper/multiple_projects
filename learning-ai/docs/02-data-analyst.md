# üìä Data Analyst (DA) - Ph√¢n t√≠ch d·ªØ li·ªáu chuy√™n nghi·ªáp

> **M·ª•c ti√™u**: Tr·ªü th√†nh chuy√™n gia ph√¢n t√≠ch d·ªØ li·ªáu, c√≥ kh·∫£ nƒÉng kh√°m ph√° insights, t·∫°o b√°o c√°o v√† h·ªó tr·ª£ ra quy·∫øt ƒë·ªãnh d·ª±a tr√™n d·ªØ li·ªáu

## üìö **1. B·∫£ng k√Ω hi·ªáu (Notation)**

### **Data Analysis:**
- **Dataset**: $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n$ (t·∫≠p d·ªØ li·ªáu)
- **Feature**: $\mathbf{x}_i = [x_{i1}, x_{i2}, \ldots, x_{id}]^T$ (vector ƒë·∫∑c tr∆∞ng)
- **Target**: $y_i$ (bi·∫øn m·ª•c ti√™u)
- **Sample**: $(\mathbf{x}_i, y_i)$ (m·∫´u d·ªØ li·ªáu)

### **Statistics:**
- **Mean**: $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$
- **Median**: $\text{median}(X)$ (gi√° tr·ªã trung v·ªã)
- **Standard Deviation**: $s = \sqrt{\frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2}$
- **Correlation**: $\rho = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}$

### **Hypothesis Testing:**
- **Null Hypothesis**: $H_0$ (gi·∫£ thuy·∫øt kh√¥ng)
- **Alternative Hypothesis**: $H_1$ (gi·∫£ thuy·∫øt ƒë·ªëi)
- **P-value**: $P(\text{data}|H_0)$ (x√°c su·∫•t quan s√°t ƒë∆∞·ª£c d·ªØ li·ªáu n·∫øu $H_0$ ƒë√∫ng)
- **Significance Level**: $\alpha$ (m·ª©c √Ω nghƒ©a, th∆∞·ªùng 0.05)

### **A/B Testing:**
- **Control Group**: $C$ (nh√≥m ƒë·ªëi ch·ª©ng)
- **Treatment Group**: $T$ (nh√≥m th·ª≠ nghi·ªám)
- **Effect Size**: $\delta = \mu_T - \mu_C$ (hi·ªáu ·ª©ng ƒëi·ªÅu tr·ªã)
- **Confidence Interval**: $[\text{CI}_{\text{lower}}, \text{CI}_{\text{upper}}]$

## üìñ **2. Glossary (ƒê·ªãnh nghƒ©a c·ªët l√µi)**

### **Data Analysis Process:**
- **CRISP-DM**: Cross-Industry Standard Process for Data Mining - quy tr√¨nh chu·∫©n ph√¢n t√≠ch d·ªØ li·ªáu
- **EDA**: Exploratory Data Analysis - ph√¢n t√≠ch kh√°m ph√° d·ªØ li·ªáu
- **Data Cleaning**: L√†m s·∫°ch d·ªØ li·ªáu - lo·∫°i b·ªè l·ªói v√† inconsistencies
- **Data Quality**: Ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu - ƒë·ªô ch√≠nh x√°c, completeness, consistency

### **Statistical Concepts:**
- **Descriptive Statistics**: Th·ªëng k√™ m√¥ t·∫£ - t√≥m t·∫Øt ƒë·∫∑c ƒëi·ªÉm c·ªßa d·ªØ li·ªáu
- **Inferential Statistics**: Th·ªëng k√™ suy lu·∫≠n - ƒë∆∞a ra k·∫øt lu·∫≠n v·ªÅ population t·ª´ sample
- **Hypothesis Testing**: Ki·ªÉm ƒë·ªãnh gi·∫£ thuy·∫øt - ƒë√°nh gi√° t√≠nh h·ª£p l√Ω c·ªßa gi·∫£ thuy·∫øt
- **Confidence Interval**: Kho·∫£ng tin c·∫≠y - kho·∫£ng ∆∞·ªõc l∆∞·ª£ng cho parameter

### **Visualization:**
- **Chart**: Bi·ªÉu ƒë·ªì - c√°ch bi·ªÉu di·ªÖn d·ªØ li·ªáu tr·ª±c quan
- **Dashboard**: B·∫£ng ƒëi·ªÅu khi·ªÉn - t·∫≠p h·ª£p c√°c bi·ªÉu ƒë·ªì v√† metrics
- **Storytelling**: K·ªÉ chuy·ªán b·∫±ng d·ªØ li·ªáu - c√°ch tr√¨nh b√†y insights
- **BI Tools**: Business Intelligence Tools - c√¥ng c·ª• ph√¢n t√≠ch kinh doanh

### **A/B Testing:**
- **Randomization**: Ng·∫´u nhi√™n h√≥a - ph√¢n b·ªï ng·∫´u nhi√™n v√†o c√°c nh√≥m
- **Control Group**: Nh√≥m ƒë·ªëi ch·ª©ng - nh√≥m kh√¥ng nh·∫≠n ƒëi·ªÅu tr·ªã
- **Treatment Group**: Nh√≥m th·ª≠ nghi·ªám - nh√≥m nh·∫≠n ƒëi·ªÅu tr·ªã
- **Statistical Power**: NƒÉng l·ª±c th·ªëng k√™ - kh·∫£ nƒÉng ph√°t hi·ªán effect th·ª±c s·ª±

## üìê **3. Th·∫ª thu·∫≠t to√°n - CRISP-DM Framework**

### **1. B√†i to√°n & d·ªØ li·ªáu:**
- **B√†i to√°n**: Quy tr√¨nh chu·∫©n ƒë·ªÉ th·ª±c hi·ªán d·ª± √°n ph√¢n t√≠ch d·ªØ li·ªáu
- **D·ªØ li·ªáu**: Business objectives, raw data, domain knowledge
- **·ª®ng d·ª•ng**: Data mining, business intelligence, analytics projects

### **2. M√¥ h√¨nh & c√¥ng th·ª©c:**
**CRISP-DM Phases:**
$$\text{CRISP-DM} = \{\text{BU}, \text{DU}, \text{DP}, \text{M}, \text{E}, \text{D}\}$$

Trong ƒë√≥:
- $\text{BU}$: Business Understanding
- $\text{DU}$: Data Understanding  
- $\text{DP}$: Data Preparation
- $\text{M}$: Modeling
- $\text{E}$: Evaluation
- $\text{D}$: Deployment

### **3. Loss & m·ª•c ti√™u:**
- **M·ª•c ti√™u**: T·∫°o quy tr√¨nh c√≥ c·∫•u tr√∫c ƒë·ªÉ gi·∫£i quy·∫øt business problems
- **Loss**: Kh√¥ng c√≥ loss ri√™ng, l√† process framework

### **4. T·ªëi ∆∞u ho√° & c·∫≠p nh·∫≠t:**
- **Algorithm**: Iterative process v·ªõi feedback loops
- **C·∫≠p nh·∫≠t**: Refine process d·ª±a tr√™n results

### **5. Hyperparams:**
- **Project scope**: Ph·∫°m vi d·ª± √°n
- **Timeline**: Th·ªùi gian th·ª±c hi·ªán
- **Resources**: Ngu·ªìn l·ª±c c·∫ßn thi·∫øt
- **Success criteria**: Ti√™u ch√≠ th√†nh c√¥ng

### **6. ƒê·ªô ph·ª©c t·∫°p:**
- **Time**: $O(\text{project\_duration})$ (th·ªùi gian d·ª± √°n)
- **Space**: $O(\text{data\_size})$ (dung l∆∞·ª£ng d·ªØ li·ªáu)

### **7. Metrics ƒë√°nh gi√°:**
- **Project success**: ƒê·∫°t ƒë∆∞·ª£c business objectives
- **Process efficiency**: Th·ªùi gian v√† resources s·ª≠ d·ª•ng
- **Quality of insights**: Ch·∫•t l∆∞·ª£ng insights thu ƒë∆∞·ª£c

### **8. ∆Øu / Nh∆∞·ª£c:**
**∆Øu ƒëi·ªÉm:**
- Structured approach
- Industry standard
- Comprehensive coverage
- Iterative improvement

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Can be rigid
- Time-consuming
- May not fit all projects
- Requires expertise

### **9. B·∫´y & m·∫πo:**
- **B·∫´y**: Skip phases ‚Üí incomplete analysis
- **B·∫´y**: No iteration ‚Üí missed opportunities
- **M·∫πo**: Adapt to project needs
- **M·∫πo**: Document decisions v√† rationale

### **10. Pseudocode:**
```python
def crisp_dm_process(business_objectives, data):
    # Phase 1: Business Understanding
    business_goals = understand_business(business_objectives)
    success_criteria = define_success_criteria(business_goals)
    
    # Phase 2: Data Understanding
    data_overview = explore_data(data)
    data_quality = assess_data_quality(data)
    
    # Phase 3: Data Preparation
    cleaned_data = clean_data(data)
    prepared_data = prepare_features(cleaned_data)
    
    # Phase 4: Modeling
    models = build_models(prepared_data)
    best_model = select_best_model(models)
    
    # Phase 5: Evaluation
    results = evaluate_model(best_model, success_criteria)
    
    # Phase 6: Deployment
    deploy_solution(best_model, results)
    
    return results
```

### **11. Code m·∫´u:**
```python
class CRISPDMProcess:
    """CRISP-DM Framework Implementation"""
    
    def __init__(self):
        self.phases = ['Business Understanding', 'Data Understanding', 
                      'Data Preparation', 'Modeling', 'Evaluation', 'Deployment']
        self.current_phase = 0
        self.results = {}
    
    def business_understanding(self, business_objectives):
        """Phase 1: Business Understanding"""
        print("=== Phase 1: Business Understanding ===")
        
        # Define business objectives
        objectives = {
            'primary_goal': business_objectives.get('goal'),
            'success_metrics': business_objectives.get('metrics'),
            'constraints': business_objectives.get('constraints'),
            'timeline': business_objectives.get('timeline')
        }
        
        # Stakeholder analysis
        stakeholders = self.identify_stakeholders(business_objectives)
        
        # ROI calculation
        roi = self.calculate_roi(business_objectives)
        
        self.results['business_understanding'] = {
            'objectives': objectives,
            'stakeholders': stakeholders,
            'roi': roi
        }
        
        return objectives
    
    def data_understanding(self, data_sources):
        """Phase 2: Data Understanding"""
        print("=== Phase 2: Data Understanding ===")
        
        # Data collection
        raw_data = self.collect_data(data_sources)
        
        # Data description
        data_description = self.describe_data(raw_data)
        
        # Data exploration
        exploration_results = self.explore_data(raw_data)
        
        # Data quality assessment
        quality_report = self.assess_data_quality(raw_data)
        
        self.results['data_understanding'] = {
            'data_description': data_description,
            'exploration': exploration_results,
            'quality_report': quality_report
        }
        
        return raw_data
    
    def data_preparation(self, raw_data):
        """Phase 3: Data Preparation"""
        print("=== Phase 3: Data Preparation ===")
        
        # Data selection
        selected_data = self.select_data(raw_data)
        
        # Data cleaning
        cleaned_data = self.clean_data(selected_data)
        
        # Feature engineering
        engineered_data = self.engineer_features(cleaned_data)
        
        # Data integration
        integrated_data = self.integrate_data(engineered_data)
        
        # Data formatting
        formatted_data = self.format_data(integrated_data)
        
        self.results['data_preparation'] = {
            'cleaned_data': cleaned_data,
            'engineered_features': engineered_data,
            'final_dataset': formatted_data
        }
        
        return formatted_data
    
    def modeling(self, prepared_data):
        """Phase 4: Modeling"""
        print("=== Phase 4: Modeling ===")
        
        # Technique selection
        techniques = self.select_modeling_techniques(prepared_data)
        
        # Test design
        test_plan = self.design_tests(prepared_data)
        
        # Model building
        models = self.build_models(prepared_data, techniques)
        
        # Model assessment
        model_assessment = self.assess_models(models, test_plan)
        
        self.results['modeling'] = {
            'techniques_used': techniques,
            'models_built': models,
            'assessment': model_assessment
        }
        
        return models
    
    def evaluation(self, models, business_criteria):
        """Phase 5: Evaluation"""
        print("=== Phase 5: Evaluation ===")
        
        # Evaluate results
        evaluation_results = self.evaluate_results(models, business_criteria)
        
        # Review process
        process_review = self.review_process()
        
        # Determine next steps
        next_steps = self.determine_next_steps(evaluation_results)
        
        self.results['evaluation'] = {
            'results': evaluation_results,
            'process_review': process_review,
            'next_steps': next_steps
        }
        
        return evaluation_results
    
    def deployment(self, best_model, evaluation_results):
        """Phase 6: Deployment"""
        print("=== Phase 6: Deployment ===")
        
        # Deployment planning
        deployment_plan = self.plan_deployment(best_model)
        
        # Monitoring setup
        monitoring_setup = self.setup_monitoring(best_model)
        
        # Final report
        final_report = self.create_final_report(evaluation_results)
        
        self.results['deployment'] = {
            'plan': deployment_plan,
            'monitoring': monitoring_setup,
            'report': final_report
        }
        
        return final_report
```

### **12. Checklist ki·ªÉm tra nhanh:**
- [ ] Business objectives c√≥ r√µ r√†ng?
- [ ] Data quality c√≥ acceptable?
- [ ] Modeling approach c√≥ ph√π h·ª£p?
- [ ] Results c√≥ meet success criteria?
- [ ] Deployment plan c√≥ feasible?

---

# üìã T·ªïng quan n·ªôi dung

```mermaid
graph TD
    A[üéØ Data Analyst] --> B[üîÑ Quy tr√¨nh ph√¢n t√≠ch]
    A --> C[üîç EDA & Kh√°m ph√° d·ªØ li·ªáu]
    A --> D[üìà Tr·ª±c quan h√≥a & Dashboard]
    A --> E[üß™ A/B Testing & Causal Inference]
    A --> F[üìä B√°o c√°o & Storytelling]
    
    B --> B1[CRISP-DM Framework]
    B --> B2[Data Collection & Cleaning]
    B --> B3[Data Quality Assessment]
    
    C --> C1[Statistical Analysis]
    C --> C2[Pattern Recognition]
    C --> C3[Outlier Detection]
    
    D --> D1[Static Charts]
    D --> D2[Interactive Dashboards]
    D --> D3[Business Intelligence Tools]
    
    E --> E1[Experimental Design]
    E --> E2[Hypothesis Testing]
    E --> E3[Causal Relationships]
    
    F --> F1[Executive Summary]
    F --> F2[Technical Details]
    F --> F3[Actionable Insights]
```

![Data Analyst Overview](assets/data-analyst-overview.svg)

![Data Analyst Overview PNG](assets/data-analyst-overview.png)

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/data-analyst-overview.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/data-analyst-overview.png)**

## üîÑ 1. Quy tr√¨nh ph√¢n t√≠ch d·ªØ li·ªáu

### 1.1 CRISP-DM Framework

> **CRISP-DM** (Cross-Industry Standard Process for Data Mining) l√† quy tr√¨nh chu·∫©n, ƒë∆∞·ª£c c√¥ng nh·∫≠n r·ªông r√£i trong ng√†nh ƒë·ªÉ th·ª±c hi·ªán c√°c d·ª± √°n khoa h·ªçc d·ªØ li·ªáu. H√£y coi n√≥ nh∆∞ m·ªôt b·∫£n ƒë·ªì chi ti·∫øt h∆∞·ªõng d·∫´n b·∫°n ƒëi t·ª´ m·ªôt c√¢u h·ªèi kinh doanh m∆° h·ªì ƒë·∫øn m·ªôt gi·∫£i ph√°p d·ª±a tr√™n d·ªØ li·ªáu c√≥ th·ªÉ tri·ªÉn khai.

**T∆∞ duy c·ªët l√µi**: CRISP-DM kh√¥ng ph·∫£i l√† m·ªôt con ƒë∆∞·ªùng th·∫≥ng. ƒê√¢y l√† m·ªôt **quy tr√¨nh l·∫∑p (iterative)**, n∆°i b·∫°n th∆∞·ªùng xuy√™n quay l·∫°i c√°c b∆∞·ªõc tr∆∞·ªõc ƒë√≥ ƒë·ªÉ tinh ch·ªânh v√† c·∫£i thi·ªán.

**V√≠ d·ª• t∆∞∆°ng t·ª±**: H√£y t∆∞·ªüng t∆∞·ª£ng b·∫°n l√† m·ªôt th√°m t·ª≠ ƒëi·ªÅu tra m·ªôt v·ª• √°n ph·ª©c t·∫°p.
1.  **Hi·ªÉu v·ª• √°n (Business Understanding)**: N·∫°n nh√¢n l√† ai? ƒê·ªông c∆° c√≥ th·ªÉ l√† g√¨?
2.  **Thu th·∫≠p b·∫±ng ch·ª©ng (Data Understanding)**: Kh√°m nghi·ªám hi·ªán tr∆∞·ªùng, l·∫•y l·ªùi khai nh√¢n ch·ª©ng, thu th·∫≠p v·∫≠t ch·ª©ng.
3.  **X·ª≠ l√Ω b·∫±ng ch·ª©ng (Data Preparation)**: L√†m s·∫°ch d·∫•u v√¢n tay, ph√¢n t√≠ch DNA, s·∫Øp x·∫øp l·ªùi khai.
4.  **X√¢y d·ª±ng gi·∫£ thuy·∫øt (Modeling)**: ƒê∆∞a ra c√°c gi·∫£ thuy·∫øt v·ªÅ nghi ph·∫°m d·ª±a tr√™n b·∫±ng ch·ª©ng.
5.  **Ki·ªÉm tra gi·∫£ thuy·∫øt (Evaluation)**: ƒê·ªëi chi·∫øu b·∫±ng ch·ª©ng v·ªõi gi·∫£ thuy·∫øt, lo·∫°i b·ªè c√°c gi·∫£ thuy·∫øt y·∫øu.
6.  **K·∫øt lu·∫≠n v√† B·∫Øt gi·ªØ (Deployment)**: ƒê∆∞a ra k·∫øt lu·∫≠n cu·ªëi c√πng v√† th·ª±c hi·ªán h√†nh ƒë·ªông.
Trong qu√° tr√¨nh n√†y, b·∫°n c√≥ th·ªÉ ph·∫£i quay l·∫°i hi·ªán tr∆∞·ªùng (Data Understanding) n·∫øu c√≥ t√¨nh ti·∫øt m·ªõi.

```mermaid
flowchart TD
    A[üíº 1. Business Understanding] <--> B[üìä 2. Data Understanding]
    B --> C[üßπ 3. Data Preparation]
    C <--> B
    C --> D[ü§ñ 4. Modeling]
    D <--> C
    D --> E[‚úÖ 5. Evaluation]
    E --> A
    E --> D
    E --> F[üöÄ 6. Deployment]
    
    A --> A1[X√°c ƒë·ªãnh m·ª•c ti√™u kinh doanh]
    A --> A2[ƒê√°nh gi√° t√¨nh h√¨nh hi·ªán t·∫°i]
    A --> A3[X√°c ƒë·ªãnh y·∫øu t·ªë th√†nh c√¥ng]
    
    B --> B1[Thu th·∫≠p d·ªØ li·ªáu ban ƒë·∫ßu]
    B --> B2[M√¥ t·∫£ d·ªØ li·ªáu]
    B --> B3[Kh√°m ph√° d·ªØ li·ªáu (EDA)]
    B --> B4[Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu]
    
    C --> C1[L·ª±a ch·ªçn d·ªØ li·ªáu]
    C --> C2[L√†m s·∫°ch d·ªØ li·ªáu]
    C --> C3[T·∫°o features m·ªõi]
    C --> C4[T√≠ch h·ª£p d·ªØ li·ªáu]
    C --> C5[ƒê·ªãnh d·∫°ng d·ªØ li·ªáu]
    
    D --> D1[L·ª±a ch·ªçn k·ªπ thu·∫≠t modeling]
    D --> D2[Thi·∫øt k·∫ø b·ªô test]
    D --> D3[X√¢y d·ª±ng m√¥ h√¨nh]
    D --> D4[ƒê√°nh gi√° m√¥ h√¨nh (k·ªπ thu·∫≠t)]
    
    E --> E1[ƒê√°nh gi√° k·∫øt qu·∫£ theo ti√™u ch√≠ kinh doanh]
    E --> E2[Xem x√©t l·∫°i quy tr√¨nh]
    E --> E3[X√°c ƒë·ªãnh b∆∞·ªõc ti·∫øp theo]
    
    F --> F1[L√™n k·∫ø ho·∫°ch tri·ªÉn khai]
    F --> F2[Gi√°m s√°t v√† b·∫£o tr√¨]
    F --> F3[B√°o c√°o v√† t·ªïng k·∫øt d·ª± √°n]
```

![CRISP-DM Framework](assets/crisp-dm-framework.svg)

![CRISP-DM Framework PNG](assets/crisp-dm-framework.png)

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/crisp-dm-framework.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/crisp-dm-framework.png)**

#### Gi·∫£i th√≠ch chi ti·∫øt c√°c pha (v·ªõi v√≠ d·ª• "D·ª± ƒëo√°n kh√°ch h√†ng r·ªùi b·ªè m·∫°ng vi·ªÖn th√¥ng")

##### 1. üíº Business Understanding (Hi·ªÉu b√†i to√°n kinh doanh)
ƒê√¢y l√† pha quan tr·ªçng nh·∫•t. N·∫øu hi·ªÉu sai b√†i to√°n, to√†n b·ªô d·ª± √°n s·∫Ω ƒëi sai h∆∞·ªõng.

-   **M·ª•c ti√™u**: Chuy·ªÉn m·ªôt v·∫•n ƒë·ªÅ kinh doanh th√†nh m·ªôt b√†i to√°n khoa h·ªçc d·ªØ li·ªáu c√≥ th·ªÉ gi·∫£i quy·∫øt.
-   **C√¢u h·ªèi c·∫ßn tr·∫£ l·ªùi**:
    -   B√†i to√°n kinh doanh th·ª±c s·ª± l√† g√¨? (V√≠ d·ª•: "T·ª∑ l·ªá kh√°ch h√†ng r·ªùi b·ªè (churn rate) ƒëang tƒÉng, ·∫£nh h∆∞·ªüng ƒë·∫øn doanh thu.")
    -   M·ª•c ti√™u c·ªßa d·ª± √°n l√† g√¨? (V√≠ d·ª•: "Gi·∫£m churn rate xu·ªëng 5% trong qu√Ω t·ªõi.")
    -   L√†m th·∫ø n√†o ƒë·ªÉ ƒëo l∆∞·ªùng th√†nh c√¥ng? (V√≠ d·ª•: "X√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n kh√°ch h√†ng s·∫Øp churn v·ªõi ƒë·ªô ch√≠nh x√°c > 80% ƒë·ªÉ ƒë·ªôi ChƒÉm s√≥c kh√°ch h√†ng c√≥ th·ªÉ can thi·ªáp k·ªãp th·ªùi.")
    -   C√°c r√†ng bu·ªôc l√† g√¨? (V√≠ d·ª•: D·ªØ li·ªáu c√° nh√¢n nh·∫°y c·∫£m, th·ªùi gian tri·ªÉn khai d∆∞·ªõi 3 th√°ng.)

##### 2. üìä Data Understanding (Hi·ªÉu d·ªØ li·ªáu)
Pha n√†y gi·ªëng nh∆∞ l·∫ßn ƒë·∫ßu "l√†m quen" v·ªõi d·ªØ li·ªáu c·ªßa b·∫°n.

-   **M·ª•c ti√™u**: Thu th·∫≠p v√† kh√°m ph√° d·ªØ li·ªáu ban ƒë·∫ßu ƒë·ªÉ h√¨nh th√†nh c√°c gi·∫£ thuy·∫øt.
-   **H√†nh ƒë·ªông**:
    -   **Thu th·∫≠p d·ªØ li·ªáu**: L·∫•y d·ªØ li·ªáu t·ª´ c√°c ngu·ªìn (database, file log, API).
    -   **M√¥ t·∫£ d·ªØ li·ªáu**: Bao nhi√™u h√†ng, bao nhi√™u c·ªôt? Ki·ªÉu d·ªØ li·ªáu c·ªßa m·ªói c·ªôt l√† g√¨?
    -   **Ph√¢n t√≠ch kh√°m ph√° (EDA)**: V·∫Ω bi·ªÉu ƒë·ªì ƒë·ªÉ xem ph√¢n ph·ªëi c·ªßa c√°c bi·∫øn, m·ªëi quan h·ªá gi·ªØa ch√∫ng.
    -   **Ki·ªÉm tra ch·∫•t l∆∞·ª£ng**: D·ªØ li·ªáu c√≥ b·ªã thi·∫øu (missing values) kh√¥ng? C√≥ gi√° tr·ªã ngo·∫°i lai (outliers) kh√¥ng?
-   **V√≠ d·ª• (D·ª± ƒëo√°n churn)**:
    -   Thu th·∫≠p d·ªØ li·ªáu v·ªÅ l·ªãch s·ª≠ s·ª≠ d·ª•ng (s·ªë ph√∫t g·ªçi, data ƒë√£ d√πng), th√¥ng tin h·ª£p ƒë·ªìng (lo·∫°i g√≥i c∆∞·ªõc, th·ªùi h·∫°n), l·ªãch s·ª≠ thanh to√°n, c√°c cu·ªôc g·ªçi h·ªó tr·ª£.
    -   Ph√°t hi·ªán c·ªôt `total_charges` c√≥ nhi·ªÅu gi√° tr·ªã b·ªã thi·∫øu.
    -   V·∫Ω bi·ªÉu ƒë·ªì th·∫•y r·∫±ng nh·ªØng kh√°ch h√†ng d√πng g√≥i c∆∞·ªõc theo th√°ng (month-to-month) c√≥ t·ª∑ l·ªá churn cao h∆°n h·∫≥n. ƒê√¢y l√† m·ªôt gi·∫£ thuy·∫øt ban ƒë·∫ßu!

##### 3. üßπ Data Preparation (Chu·∫©n b·ªã d·ªØ li·ªáu)
ƒê√¢y l√† pha t·ªën nhi·ªÅu th·ªùi gian nh·∫•t (th∆∞·ªùng chi·∫øm 60-80% th·ªùi gian d·ª± √°n).

-   **M·ª•c ti√™u**: Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√¥ th√†nh m·ªôt b·ªô d·ªØ li·ªáu s·∫°ch, s·∫µn s√†ng cho vi·ªác x√¢y d·ª±ng m√¥ h√¨nh.
-   **H√†nh ƒë·ªông**:
    -   **L√†m s·∫°ch**: X·ª≠ l√Ω gi√° tr·ªã thi·∫øu (v√≠ d·ª•: ƒëi·ªÅn gi√° tr·ªã trung b√¨nh), s·ª≠a l·ªói d·ªØ li·ªáu.
    -   **T·∫°o Feature m·ªõi (Feature Engineering)**: T·ª´ c√°c c·ªôt c√≥ s·∫µn, t·∫°o ra c√°c c·ªôt m·ªõi c√≥ √Ω nghƒ©a h∆°n. V√≠ d·ª•: t·ª´ `total_charges` v√† `tenure` (th·ªùi gian s·ª≠ d·ª•ng), t·∫°o ra `average_monthly_charges`.
    -   **Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu**: M√£ h√≥a c√°c bi·∫øn ph√¢n lo·∫°i (v√≠ d·ª•: `gender` -> 0/1), chu·∫©n h√≥a c√°c bi·∫øn s·ªë.
-   **V√≠ d·ª• (D·ª± ƒëo√°n churn)**:
    -   ƒêi·ªÅn gi√° tr·ªã `total_charges` b·ªã thi·∫øu b·∫±ng gi√° tr·ªã trung b√¨nh c·ªßa nh·ªØng kh√°ch h√†ng c√≥ `tenure` t∆∞∆°ng t·ª±.
    -   T·∫°o feature `has_called_support` (ƒë√£ t·ª´ng g·ªçi h·ªó tr·ª£ hay ch∆∞a).
    -   M√£ h√≥a c·ªôt `contract_type` (Month-to-month, One year, Two year) th√†nh c√°c s·ªë.

##### 4. ü§ñ Modeling (X√¢y d·ª±ng m√¥ h√¨nh)
ƒê√¢y l√† pha m√† c√°c thu·∫≠t to√°n machine learning ƒë∆∞·ª£c √°p d·ª•ng.

-   **M·ª•c ti√™u**: L·ª±a ch·ªçn v√† x√¢y d·ª±ng c√°c m√¥ h√¨nh c√≥ kh·∫£ nƒÉng d·ª± ƒëo√°n t·ªët nh·∫•t.
-   **H√†nh ƒë·ªông**:
    -   **Ch·ªçn thu·∫≠t to√°n**: D·ª±a v√†o b√†i to√°n (ph√¢n lo·∫°i, h·ªìi quy), ch·ªçn c√°c thu·∫≠t to√°n ph√π h·ª£p (Logistic Regression, Random Forest, Gradient Boosting...).
    -   **X√¢y d·ª±ng m√¥ h√¨nh**: Hu·∫•n luy·ªán (train) c√°c m√¥ h√¨nh tr√™n b·ªô d·ªØ li·ªáu ƒë√£ chu·∫©n b·ªã.
    -   **Tinh ch·ªânh tham s·ªë (Hyperparameter Tuning)**: T·ªëi ∆∞u h√≥a c√°c tham s·ªë c·ªßa m√¥ h√¨nh ƒë·ªÉ ƒë·∫°t hi·ªáu su·∫•t cao nh·∫•t.
-   **V√≤ng l·∫∑p**: N·∫øu m√¥ h√¨nh ho·∫°t ƒë·ªông kh√¥ng t·ªët, c√≥ th·ªÉ b·∫°n c·∫ßn quay l·∫°i pha **Data Preparation** ƒë·ªÉ t·∫°o th√™m feature m·ªõi ho·∫∑c x·ª≠ l√Ω d·ªØ li·ªáu kh√°c ƒëi.

##### 5. ‚úÖ Evaluation (ƒê√°nh gi√°)
Pha n√†y ƒë√°nh gi√° xem m√¥ h√¨nh ho·∫°t ƒë·ªông t·ªët ƒë·∫øn ƒë√¢u *theo g√≥c ƒë·ªô kinh doanh*.

-   **M·ª•c ti√™u**: ƒê·∫£m b·∫£o m√¥ h√¨nh ƒë√°p ·ª©ng ƒë∆∞·ª£c m·ª•c ti√™u kinh doanh ƒë√£ ƒë·ªÅ ra.
-   **H√†nh ƒë·ªông**:
    -   **ƒê√°nh gi√° k·ªπ thu·∫≠t**: D√πng c√°c metric nh∆∞ Accuracy, Precision, Recall, F1-score...
    -   **ƒê√°nh gi√° kinh doanh**: M√¥ h√¨nh c√≥ gi√∫p gi·∫£m churn rate kh√¥ng? Chi ph√≠ ƒë·ªÉ gi·ªØ ch√¢n m·ªôt kh√°ch h√†ng c√≥ nh·ªè h∆°n l·ª£i nhu·∫≠n h·ªç mang l·∫°i kh√¥ng? M√¥ h√¨nh c√≥ d·ªÖ gi·∫£i th√≠ch cho ban l√£nh ƒë·∫°o kh√¥ng?
-   **V√≤ng l·∫∑p**: N·∫øu k·∫øt qu·∫£ ƒë√°nh gi√° kh√¥ng ƒë·∫°t y√™u c·∫ßu, b·∫°n ph·∫£i quay l·∫°i pha **Business Understanding** ƒë·ªÉ xem l·∫°i m·ª•c ti√™u, ho·∫∑c quay l·∫°i pha **Modeling** ƒë·ªÉ th·ª≠ thu·∫≠t to√°n kh√°c.

##### 6. üöÄ Deployment (Tri·ªÉn khai)
ƒê√¢y l√† l√∫c ƒë∆∞a m√¥ h√¨nh v√†o ho·∫°t ƒë·ªông th·ª±c t·∫ø.

-   **M·ª•c ti√™u**: T√≠ch h·ª£p m√¥ h√¨nh v√†o h·ªá th·ªëng hi·ªán t·∫°i ƒë·ªÉ t·∫°o ra gi√° tr·ªã.
-   **H√†nh ƒë·ªông**:
    -   **L√™n k·∫ø ho·∫°ch**: Tri·ªÉn khai d∆∞·ªõi d·∫°ng API, batch job, hay ·ª©ng d·ª•ng web?
    -   **Gi√°m s√°t (Monitoring)**: Theo d√µi hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh theo th·ªùi gian. D·ªØ li·ªáu th·ª±c t·∫ø c√≥ thay ƒë·ªïi kh√¥ng (data drift)? Hi·ªáu su·∫•t m√¥ h√¨nh c√≥ gi·∫£m kh√¥ng (model degradation)?
    -   **B·∫£o tr√¨**: Hu·∫•n luy·ªán l·∫°i (re-train) m√¥ h√¨nh ƒë·ªãnh k·ª≥ v·ªõi d·ªØ li·ªáu m·ªõi.
    -   **B√°o c√°o**: T·ªïng k·∫øt d·ª± √°n v√† b√°o c√°o k·∫øt qu·∫£ cho c√°c b√™n li√™n quan.

#### C√°c c·∫°m b·∫´y th∆∞·ªùng g·∫∑p (Common Pitfalls)
-   **Business Understanding**: Kh√¥ng x√°c ƒë·ªãnh r√µ KPI. "C·∫£i thi·ªán tr·∫£i nghi·ªám kh√°ch h√†ng" l√† m·ªôt m·ª•c ti√™u m∆° h·ªì. "Gi·∫£m th·ªùi gian ph·∫£n h·ªìi chat bot xu·ªëng d∆∞·ªõi 30 gi√¢y" l√† m·ªôt KPI r√µ r√†ng.
-   **Data Understanding**: V·ªôi v√†ng x√¢y d·ª±ng m√¥ h√¨nh m√† kh√¥ng kh√°m ph√° k·ªπ d·ªØ li·ªáu, d·∫´n ƒë·∫øn vi·ªác b·ªè qua c√°c insight quan tr·ªçng ho·∫∑c c√°c v·∫•n ƒë·ªÅ v·ªÅ ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu.
-   **Data Preparation**: √Åp d·ª•ng c√°c k·ªπ thu·∫≠t x·ª≠ l√Ω m·ªôt c√°ch m√°y m√≥c m√† kh√¥ng hi·ªÉu t·∫°i sao. V√≠ d·ª•, lu√¥n ƒëi·ªÅn gi√° tr·ªã thi·∫øu b·∫±ng trung b√¨nh m√† kh√¥ng xem x√©t t·∫°i sao n√≥ l·∫°i thi·∫øu.
-   **Modeling**: "Overfitting" - x√¢y d·ª±ng m·ªôt m√¥ h√¨nh qu√° ph·ª©c t·∫°p, ho·∫°t ƒë·ªông ho√†n h·∫£o tr√™n d·ªØ li·ªáu training nh∆∞ng th·∫•t b·∫°i tr√™n d·ªØ li·ªáu th·ª±c t·∫ø.
-   **Evaluation**: Ch·ªâ t·∫≠p trung v√†o c√°c ch·ªâ s·ªë k·ªπ thu·∫≠t (nh∆∞ accuracy) m√† qu√™n m·∫•t m·ª•c ti√™u kinh doanh. M·ªôt m√¥ h√¨nh c√≥ accuracy 99% c√≥ th·ªÉ v√¥ d·ª•ng n·∫øu n√≥ kh√¥ng gi√∫p c√¥ng ty ki·∫øm ti·ªÅn.
-   **Deployment**: Coi vi·ªác tri·ªÉn khai l√† b∆∞·ªõc cu·ªëi c√πng. Th·ª±c t·∫ø, ƒë√¢y l√† kh·ªüi ƒë·∫ßu c·ªßa m·ªôt v√≤ng ƒë·ªùi m·ªõi, ƒë√≤i h·ªèi gi√°m s√°t v√† b·∫£o tr√¨ li√™n t·ª•c.

#### Business Understanding - Hi·ªÉu v·∫•n ƒë·ªÅ kinh doanh

**L√Ω thuy·∫øt c∆° b·∫£n:**
- **CRISP-DM Phase 1**: Chuy·ªÉn ƒë·ªïi business objectives th√†nh data mining goals
- **Stakeholder Analysis**: X√°c ƒë·ªãnh t·∫•t c·∫£ parties c√≥ li√™n quan
- **ROI Calculation**: Return on Investment cho data mining project

**Framework chi ti·∫øt:**

**1. Business Objectives Mapping:**
```python
from dataclasses import dataclass
from typing import List, Dict, Any
from enum import Enum

class BusinessObjectiveType(Enum):
    REVENUE_INCREASE = "revenue_increase"
    COST_REDUCTION = "cost_reduction"
    CUSTOMER_SATISFACTION = "customer_satisfaction"
    OPERATIONAL_EFFICIENCY = "operational_efficiency"
    RISK_MITIGATION = "risk_mitigation"

@dataclass
class BusinessObjective:
    """Structured business objective"""
    id: str
    type: BusinessObjectiveType
    description: str
    target_value: float
    current_value: float
    unit: str
    timeline_months: int
    stakeholders: List[str]
    success_criteria: List[str]
    
    @property
    def improvement_needed(self) -> float:
        """Calculate improvement needed to reach target"""
        return self.target_value - self.current_value
    
    @property
    def improvement_percentage(self) -> float:
        """Calculate percentage improvement needed"""
        if self.current_value == 0:
            return float('inf')
        return (self.improvement_needed / self.current_value) * 100

# Example usage
revenue_obj = BusinessObjective(
    id="REV_001",
    type=BusinessObjectiveType.REVENUE_INCREASE,
    description="Increase quarterly revenue by 20%",
    target_value=1200000,  # $1.2M
    current_value=1000000,  # $1M
    unit="USD",
    timeline_months=3,
    stakeholders=["Sales Team", "Marketing", "Product"],
    success_criteria=["Q4 revenue >= $1.2M", "Monthly growth rate > 5%"]
)

print(f"Improvement needed: ${revenue_obj.improvement_needed:,.0f}")
print(f"Percentage improvement: {revenue_obj.improvement_percentage:.1f}%")
```

**2. Stakeholder Analysis Matrix:**
```python
import pandas as pd
import numpy as np

class StakeholderAnalysis:
    """Analyze stakeholder influence and interest"""
    
    def __init__(self):
        self.stakeholders = []
    
    def add_stakeholder(self, name: str, influence: int, interest: int, 
                       role: str, requirements: List[str]):
        """Add stakeholder with influence (1-5) and interest (1-5)"""
        self.stakeholders.append({
            'name': name,
            'influence': influence,  # 1=Low, 5=High
            'interest': interest,    # 1=Low, 5=High
            'role': role,
            'requirements': requirements
        })
    
    def get_stakeholder_matrix(self) -> pd.DataFrame:
        """Create stakeholder influence-interest matrix"""
        df = pd.DataFrame(self.stakeholders)
        
        # Categorize stakeholders
        def categorize_stakeholder(row):
            if row['influence'] >= 4 and row['interest'] >= 4:
                return "Key Player"
            elif row['influence'] >= 4 and row['interest'] < 4:
                return "Keep Satisfied"
            elif row['influence'] < 4 and row['interest'] >= 4:
                return "Keep Informed"
            else:
                return "Monitor"
        
        df['category'] = df.apply(categorize_stakeholder, axis=1)
        return df
    
    def get_communication_plan(self) -> Dict[str, List[str]]:
        """Generate communication plan based on stakeholder categories"""
        df = self.get_stakeholder_matrix()
        
        plan = {
            "Key Players": df[df['category'] == "Key Player"]['name'].tolist(),
            "Keep Satisfied": df[df['category'] == "Keep Satisfied"]['name'].tolist(),
            "Keep Informed": df[df['category'] == "Keep Informed"]['name'].tolist(),
            "Monitor": df[df['category'] == "Monitor"]['name'].tolist()
        }
        
        return plan

# Example usage
stakeholder_analysis = StakeholderAnalysis()
stakeholder_analysis.add_stakeholder("CEO", 5, 4, "Executive", ["Revenue growth", "ROI"])
stakeholder_analysis.add_stakeholder("Sales Manager", 4, 5, "Manager", ["Lead quality", "Conversion rates"])
stakeholder_analysis.add_stakeholder("Data Engineer", 3, 5, "Technical", ["Data quality", "Infrastructure"])

matrix = stakeholder_analysis.get_stakeholder_matrix()
communication_plan = stakeholder_analysis.get_communication_plan()
```

**3. ROI v√† Business Case Development:**
```python
from dataclasses import dataclass
from typing import List, Dict
import numpy as np

@dataclass
class BusinessCase:
    """Business case cho data mining project"""
    project_name: str
    duration_months: int
    team_size: int
    costs: Dict[str, float]
    benefits: Dict[str, float]
    risk_factors: List[str]
    
    def calculate_total_cost(self) -> float:
        """Calculate total project cost"""
        return sum(self.costs.values())
    
    def calculate_total_benefits(self) -> float:
        """Calculate total project benefits"""
        return sum(self.benefits.values())
    
    def calculate_roi(self) -> float:
        """Calculate Return on Investment"""
        total_cost = self.calculate_total_cost()
        if total_cost == 0:
            return float('inf')
        return ((self.calculate_total_benefits() - total_cost) / total_cost) * 100
    
    def calculate_payback_period(self) -> float:
        """Calculate payback period in months"""
        monthly_benefit = self.calculate_total_benefits() / self.duration_months
        if monthly_benefit <= 0:
            return float('inf')
        return self.calculate_total_cost() / monthly_benefit
    
    def get_risk_assessment(self) -> Dict[str, str]:
        """Assess project risks"""
        risk_levels = {
            "Low": "Project likely to succeed with minimal issues",
            "Medium": "Some challenges expected, manageable with proper planning",
            "High": "Significant risks, requires careful mitigation strategies"
        }
        
        # Simple risk scoring based on factors
        risk_score = len(self.risk_factors) * 0.2 + (self.duration_months / 12) * 0.3
        
        if risk_score < 0.5:
            risk_level = "Low"
        elif risk_score < 1.0:
            risk_level = "Medium"
        else:
            risk_level = "High"
        
        return {
            "risk_level": risk_level,
            "description": risk_levels[risk_level],
            "risk_score": risk_score
        }

# Example business case
business_case = BusinessCase(
    project_name="Customer Churn Prediction",
    duration_months=6,
    team_size=5,
    costs={
        "team_salary": 150000,      # 6 months * 5 people * $50K/year
        "infrastructure": 25000,    # Cloud costs, tools
        "external_consulting": 30000,  # Expert consultation
        "training": 15000           # Team training
    },
    benefits={
        "revenue_retention": 200000,  # Prevented churn
        "cost_reduction": 50000,      # Reduced acquisition costs
        "efficiency_gains": 75000     # Better targeting
    },
    risk_factors=[
        "Data quality issues",
        "Model performance below expectations",
        "Integration challenges with existing systems"
    ]
)

print(f"Total Cost: ${business_case.calculate_total_cost():,.0f}")
print(f"Total Benefits: ${business_case.calculate_total_benefits():,.0f}")
print(f"ROI: {business_case.calculate_roi():.1f}%")
print(f"Payback Period: {business_case.calculate_payback_period():.1f} months")
print(f"Risk Level: {business_case.get_risk_assessment()['risk_level']}")
```

**4. Success Metrics v√† KPIs:**
```python
class KPIFramework:
    """Framework cho defining v√† tracking KPIs"""
    
    def __init__(self):
        self.kpis = {}
    
    def add_kpi(self, name: str, current_value: float, target_value: float, 
                unit: str, frequency: str, owner: str):
        """Add KPI v·ªõi baseline v√† target"""
        self.kpis[name] = {
            'current': current_value,
            'target': target_value,
            'unit': unit,
            'frequency': frequency,
            'owner': owner,
            'history': [current_value]
        }
    
    def update_kpi(self, name: str, new_value: float):
        """Update KPI value v√† track history"""
        if name in self.kpis:
            self.kpis[name]['current'] = new_value
            self.kpis[name]['history'].append(new_value)
    
    def get_kpi_status(self, name: str) -> Dict[str, Any]:
        """Get KPI status v√† progress"""
        if name not in self.kpis:
            return {}
        
        kpi = self.kpis[name]
        progress = ((kpi['current'] - kpi['history'][0]) / 
                   (kpi['target'] - kpi['history'][0])) * 100
        
        return {
            'name': name,
            'current': kpi['current'],
            'target': kpi['target'],
            'progress_percentage': progress,
            'status': 'On Track' if progress >= 0 else 'Behind',
            'unit': kpi['unit'],
            'owner': kpi['owner']
        }
    
    def get_all_kpis_status(self) -> List[Dict[str, Any]]:
        """Get status c·ªßa t·∫•t c·∫£ KPIs"""
        return [self.get_kpi_status(name) for name in self.kpis.keys()]

# Example KPI framework
kpi_framework = KPIFramework()
kpi_framework.add_kpi("Conversion Rate", 2.5, 3.5, "%", "Weekly", "Marketing")
kpi_framework.add_kpi("Customer Acquisition Cost", 150, 120, "USD", "Monthly", "Sales")
kpi_framework.add_kpi("Customer Lifetime Value", 800, 1000, "USD", "Quarterly", "Product")

# Update KPIs
kpi_framework.update_kpi("Conversion Rate", 2.8)
kpi_framework.update_kpi("Customer Acquisition Cost", 140)

# Get status
all_status = kpi_framework.get_all_kpis_status()
for status in all_status:
    print(f"{status['name']}: {status['status']} ({status['progress_percentage']:.1f}%)")
```

**T√†i li·ªáu tham kh·∫£o chuy√™n s√¢u:**
- **CRISP-DM Official Guide**: [IBM CRISP-DM](https://www.ibm.com/docs/en/spss-modeler/SaaS?topic=dm-crisp-help-overview)
- **Business Analysis Body of Knowledge (BABOK)**: [IIBA BABOK](https://www.iiba.org/business-analysis-body-of-knowledge/)
- **Stakeholder Management**: [PMI Guide](https://www.pmi.org/pmbok-guide-standards)
- **ROI Analysis**: [Harvard Business Review - ROI](https://hbr.org/topic/roi)

#### Data Understanding - Hi·ªÉu d·ªØ li·ªáu

**L√Ω thuy·∫øt c∆° b·∫£n:**
- **CRISP-DM Phase 2**: Systematic data collection v√† exploration
- **Data Lineage**: Tracking data origin v√† transformations
- **Data Schema Evolution**: Understanding structural changes over time
- **Data Volume, Velocity, Variety**: 3V framework cho big data

**Framework chi ti·∫øt:**

**1. Data Collection Strategy:**
```python
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from enum import Enum
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

class DataSourceType(Enum):
    DATABASE = "database"
    API = "api"
    FILE = "file"
    STREAMING = "streaming"
    EXTERNAL = "external"

@dataclass
class DataSource:
    """Structured data source information"""
    name: str
    type: DataSourceType
    location: str
    format: str
    update_frequency: str
    last_updated: datetime
    reliability_score: float  # 0-1
    data_volume_gb: float
    schema_version: str
    
    def is_fresh(self, max_age_hours: int = 24) -> bool:
        """Check if data source is fresh"""
        return datetime.now() - self.last_updated < timedelta(hours=max_age_hours)
    
    def get_health_status(self) -> str:
        """Get data source health status"""
        if self.reliability_score >= 0.9:
            return "Excellent"
        elif self.reliability_score >= 0.7:
            return "Good"
        elif self.reliability_score >= 0.5:
            return "Fair"
        else:
            return "Poor"

class DataCollectionFramework:
    """Framework cho systematic data collection"""
    
    def __init__(self):
        self.sources: List[DataSource] = []
        self.collection_log: List[Dict[str, Any]] = []
    
    def add_source(self, source: DataSource) -> None:
        """Add data source to collection framework"""
        self.sources.append(source)
    
    def collect_data(self, source_name: str) -> Optional[pd.DataFrame]:
        """Collect data from specified source"""
        source = next((s for s in self.sources if s.name == source_name), None)
        if not source:
            raise ValueError(f"Source {source_name} not found")
        
        try:
            # Simulate data collection
            start_time = datetime.now()
            
            # Log collection attempt
            self.collection_log.append({
                'source': source_name,
                'timestamp': start_time,
                'status': 'started'
            })
            
            # Simulate data loading (replace with actual implementation)
            if source.type == DataSourceType.DATABASE:
                # Simulate database query
                data = pd.DataFrame({
                    'id': range(1000),
                    'value': np.random.randn(1000),
                    'timestamp': pd.date_range(start='2024-01-01', periods=1000, freq='H')
                })
            elif source.type == DataSourceType.API:
                # Simulate API call
                data = pd.DataFrame({
                    'id': range(500),
                    'metric': np.random.exponential(2, 500),
                    'category': np.random.choice(['A', 'B', 'C'], 500)
                })
            else:
                data = pd.DataFrame()
            
            # Log successful collection
            self.collection_log.append({
                'source': source_name,
                'timestamp': datetime.now(),
                'status': 'completed',
                'rows_collected': len(data)
            })
            
            return data
            
        except Exception as e:
            # Log error
            self.collection_log.append({
                'source': source_name,
                'timestamp': datetime.now(),
                'status': 'failed',
                'error': str(e)
            })
            return None
    
    def get_collection_summary(self) -> pd.DataFrame:
        """Get summary of all collection attempts"""
        return pd.DataFrame(self.collection_log)
    
    def get_source_health_report(self) -> pd.DataFrame:
        """Get health report for all data sources"""
        health_data = []
        for source in self.sources:
            health_data.append({
                'source_name': source.name,
                'type': source.type.value,
                'reliability_score': source.reliability_score,
                'health_status': source.get_health_status(),
                'is_fresh': source.is_fresh(),
                'last_updated': source.last_updated,
                'data_volume_gb': source.data_volume_gb
            })
        return pd.DataFrame(health_data)

# Example usage
collection_framework = DataCollectionFramework()

# Add data sources
db_source = DataSource(
    name="customer_database",
    type=DataSourceType.DATABASE,
    location="postgresql://localhost:5432/customers",
    format="relational",
    update_frequency="hourly",
    last_updated=datetime.now() - timedelta(hours=2),
    reliability_score=0.95,
    data_volume_gb=50.5,
    schema_version="2.1"
)

api_source = DataSource(
    name="analytics_api",
    type=DataSourceType.API,
    location="https://api.analytics.com/v1/metrics",
    format="json",
    update_frequency="real-time",
    last_updated=datetime.now() - timedelta(minutes=30),
    reliability_score=0.88,
    data_volume_gb=2.1,
    schema_version="1.0"
)

collection_framework.add_source(db_source)
collection_framework.add_source(api_source)

# Collect data
customer_data = collection_framework.collect_data("customer_database")
analytics_data = collection_framework.collect_data("analytics_api")

# Get reports
health_report = collection_framework.get_source_health_report()
collection_summary = collection_framework.get_collection_summary()
```

**2. Data Schema Analysis:**
```python
class SchemaAnalyzer:
    """Analyze data schema v√† detect changes"""
    
    def __init__(self):
        self.schema_history: List[Dict[str, Any]] = []
    
    def analyze_schema(self, df: pd.DataFrame, source_name: str) -> Dict[str, Any]:
        """Analyze current schema c·ªßa dataframe"""
        schema_info = {
            'source_name': source_name,
            'timestamp': datetime.now(),
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'columns': {}
        }
        
        for col in df.columns:
            col_info = {
                'dtype': str(df[col].dtype),
                'null_count': df[col].isnull().sum(),
                'null_percentage': (df[col].isnull().sum() / len(df)) * 100,
                'unique_count': df[col].nunique(),
                'unique_percentage': (df[col].nunique() / len(df)) * 100
            }
            
            # Add statistical info for numeric columns
            if np.issubdtype(df[col].dtype, np.number):
                col_info.update({
                    'min': df[col].min(),
                    'max': df[col].max(),
                    'mean': df[col].mean(),
                    'std': df[col].std(),
                    'median': df[col].median()
                })
            
            # Add sample values for categorical columns
            if col_info['unique_count'] < 20:
                col_info['sample_values'] = df[col].value_counts().head(5).to_dict()
            
            schema_info['columns'][col] = col_info
        
        # Store in history
        self.schema_history.append(schema_info)
        return schema_info
    
    def detect_schema_changes(self, current_schema: Dict[str, Any], 
                            previous_schema: Dict[str, Any]) -> Dict[str, Any]:
        """Detect changes between two schemas"""
        changes = {
            'new_columns': [],
            'removed_columns': [],
            'type_changes': [],
            'cardinality_changes': []
        }
        
        current_cols = set(current_schema['columns'].keys())
        previous_cols = set(previous_schema['columns'].keys())
        
        # Detect new and removed columns
        changes['new_columns'] = list(current_cols - previous_cols)
        changes['removed_columns'] = list(previous_cols - current_cols)
        
        # Detect type and cardinality changes
        common_cols = current_cols & previous_cols
        for col in common_cols:
            current_info = current_schema['columns'][col]
            previous_info = previous_schema['columns'][col]
            
            # Type changes
            if current_info['dtype'] != previous_info['dtype']:
                changes['type_changes'].append({
                    'column': col,
                    'old_type': previous_info['dtype'],
                    'new_type': current_info['dtype']
                })
            
            # Cardinality changes (significant changes in unique values)
            current_unique = current_info['unique_percentage']
            previous_unique = previous_info['unique_percentage']
            if abs(current_unique - previous_unique) > 10:  # 10% threshold
                changes['cardinality_changes'].append({
                    'column': col,
                    'old_unique_percentage': previous_unique,
                    'new_unique_percentage': current_unique,
                    'change': current_unique - previous_unique
                })
        
        return changes
    
    def get_schema_evolution_report(self) -> pd.DataFrame:
        """Get report of schema evolution over time"""
        if len(self.schema_history) < 2:
            return pd.DataFrame()
        
        evolution_data = []
        for i in range(1, len(self.schema_history)):
            current = self.schema_history[i]
            previous = self.schema_history[i-1]
            
            changes = self.detect_schema_changes(current, previous)
            
            evolution_data.append({
                'timestamp': current['timestamp'],
                'source_name': current['source_name'],
                'new_columns': len(changes['new_columns']),
                'removed_columns': len(changes['removed_columns']),
                'type_changes': len(changes['type_changes']),
                'cardinality_changes': len(changes['cardinality_changes']),
                'total_changes': sum(len(v) for v in changes.values())
            })
        
        return pd.DataFrame(evolution_data)

# Example usage
schema_analyzer = SchemaAnalyzer()

# Analyze schemas
if customer_data is not None:
    current_schema = schema_analyzer.analyze_schema(customer_data, "customer_database")
    print(f"Schema analyzed: {current_schema['total_columns']} columns, {current_schema['total_rows']} rows")

# Get evolution report
evolution_report = schema_analyzer.get_schema_evolution_report()
```

**3. Data Quality Assessment Framework:**
```python
class DataQualityFramework:
    """Comprehensive data quality assessment"""
    
    def __init__(self):
        self.quality_metrics = {}
        self.quality_history = []
    
    def assess_data_quality(self, df: pd.DataFrame, source_name: str) -> Dict[str, Any]:
        """Assess overall data quality"""
        quality_score = 0
        total_checks = 0
        
        # Completeness check
        completeness_score = self._check_completeness(df)
        quality_score += completeness_score
        total_checks += 1
        
        # Accuracy check
        accuracy_score = self._check_accuracy(df)
        quality_score += accuracy_score
        total_checks += 1
        
        # Consistency check
        consistency_score = self._check_consistency(df)
        quality_score += consistency_score
        total_checks += 1
        
        # Validity check
        validity_score = self._check_validity(df)
        quality_score += validity_score
        total_checks += 1
        
        # Timeliness check
        timeliness_score = self._check_timeliness(df)
        quality_score += timeliness_score
        total_checks += 1
        
        # Overall quality score
        overall_score = quality_score / total_checks
        
        quality_assessment = {
            'source_name': source_name,
            'timestamp': datetime.now(),
            'overall_score': overall_score,
            'completeness_score': completeness_score,
            'accuracy_score': accuracy_score,
            'consistency_score': consistency_score,
            'validity_score': validity_score,
            'timeliness_score': timeliness_score,
            'total_rows': len(df),
            'total_columns': len(df.columns)
        }
        
        # Store in history
        self.quality_history.append(quality_assessment)
        self.quality_metrics[source_name] = quality_assessment
        
        return quality_assessment
    
    def _check_completeness(self, df: pd.DataFrame) -> float:
        """Check data completeness (non-null values)"""
        total_cells = df.size
        null_cells = df.isnull().sum().sum()
        completeness = (total_cells - null_cells) / total_cells
        return completeness
    
    def _check_accuracy(self, df: pd.DataFrame) -> float:
        """Check data accuracy (basic sanity checks)"""
        accuracy_checks = 0
        total_checks = 0
        
        for col in df.columns:
            if np.issubdtype(df[col].dtype, np.number):
                # Check for extreme outliers (beyond 3 standard deviations)
                mean_val = df[col].mean()
                std_val = df[col].std()
                if std_val > 0:
                    outliers = df[col][(df[col] < mean_val - 3*std_val) | 
                                     (df[col] > mean_val + 3*std_val)]
                    outlier_ratio = len(outliers) / len(df[col])
                    if outlier_ratio < 0.01:  # Less than 1% outliers
                        accuracy_checks += 1
                    total_checks += 1
        
        return accuracy_checks / max(total_checks, 1)
    
    def _check_consistency(self, df: pd.DataFrame) -> float:
        """Check data consistency (format, patterns)"""
        consistency_checks = 0
        total_checks = 0
        
        for col in df.columns:
            # Check for consistent data types
            if df[col].dtype == 'object':
                # Check if all values follow similar patterns
                sample_values = df[col].dropna().head(100)
                if len(sample_values) > 0:
                    # Simple pattern consistency check
                    pattern_lengths = [len(str(v)) for v in sample_values]
                    if len(set(pattern_lengths)) <= 3:  # Similar lengths
                        consistency_checks += 1
                    total_checks += 1
        
        return consistency_checks / max(total_checks, 1)
    
    def _check_validity(self, df: pd.DataFrame) -> float:
        """Check data validity (within expected ranges)"""
        validity_checks = 0
        total_checks = 0
        
        for col in df.columns:
            if np.issubdtype(df[col].dtype, np.number):
                # Check if values are within reasonable bounds
                if df[col].min() >= 0 and df[col].max() < 1e12:  # Reasonable bounds
                    validity_checks += 1
                total_checks += 1
        
        return validity_checks / max(total_checks, 1)
    
    def _check_timeliness(self, df: pd.DataFrame) -> float:
        """Check data timeliness (if timestamp columns exist)"""
        timestamp_cols = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]
        
        if not timestamp_cols:
            return 0.5  # Neutral score if no timestamp columns
        
        timeliness_checks = 0
        total_checks = 0
        
        for col in timestamp_cols:
            try:
                # Convert to datetime if possible
                pd.to_datetime(df[col], errors='coerce')
                timeliness_checks += 1
                total_checks += 1
            except:
                pass
        
        return timeliness_checks / max(total_checks, 1)
    
    def get_quality_trends(self, source_name: str) -> pd.DataFrame:
        """Get quality trends over time for a specific source"""
        source_history = [q for q in self.quality_history if q['source_name'] == source_name]
        
        if len(source_history) < 2:
            return pd.DataFrame()
        
        trends_data = []
        for i in range(1, len(source_history)):
            current = source_history[i]
            previous = source_history[i-1]
            
            trends_data.append({
                'timestamp': current['timestamp'],
                'overall_change': current['overall_score'] - previous['overall_score'],
                'completeness_change': current['completeness_score'] - previous['completeness_score'],
                'accuracy_change': current['accuracy_score'] - previous['accuracy_score'],
                'consistency_change': current['consistency_score'] - previous['consistency_score'],
                'validity_change': current['validity_score'] - previous['validity_score'],
                'timeliness_change': current['timeliness_score'] - previous['timeliness_score']
            })
        
        return pd.DataFrame(trends_data)

# Example usage
quality_framework = DataQualityFramework()

# Assess data quality
if customer_data is not None:
    quality_assessment = quality_framework.assess_data_quality(customer_data, "customer_database")
    print(f"Data Quality Score: {quality_assessment['overall_score']:.3f}")

# Get quality trends
quality_trends = quality_framework.get_quality_trends("customer_database")
```

**T√†i li·ªáu tham kh·∫£o chuy√™n s√¢u:**
- **Data Quality Management**: [DAMA-DMBOK](https://www.dama.org/page/bodyofknowledge)
- **Data Lineage**: [Data Lineage Best Practices](https://www.databricks.com/blog/2020/08/26/data-lineage.html)
- **Schema Evolution**: [Schema Evolution in Data Lakes](https://delta.io/blog/2020-01-27-schema-evolution-in-data-lakes/)
- **Data Profiling**: [Great Expectations Documentation](https://docs.greatexpectations.io/)

#### Data Preparation - Chu·∫©n b·ªã d·ªØ li·ªáu

**M·ª•c ti√™u**: Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√¥ th√†nh d·ªØ li·ªáu s·∫°ch, c√≥ c·∫•u tr√∫c.

**C√°c b∆∞·ªõc th·ª±c hi·ªán:**
1. **L·ª±a ch·ªçn d·ªØ li·ªáu**: Ch·ªçn features v√† records ph√π h·ª£p
2. **L√†m s·∫°ch d·ªØ li·ªáu**: X·ª≠ l√Ω missing values, outliers, duplicates
3. **X√¢y d·ª±ng d·ªØ li·ªáu**: Feature engineering, transformations
4. **T√≠ch h·ª£p d·ªØ li·ªáu**: Merge, join multiple sources
5. **ƒê·ªãnh d·∫°ng d·ªØ li·ªáu**: Standardize formats, encoding

**T·ª∑ l·ªá th·ªùi gian trong d·ª± √°n:**
```
Data Preparation: 60-80% th·ªùi gian
Modeling: 10-20% th·ªùi gian
Evaluation: 5-10% th·ªùi gian
```

## üîç 2. Exploratory Data Analysis (EDA)

> **EDA** l√† qu√° tr√¨nh "tr√≤ chuy·ªán" v·ªõi d·ªØ li·ªáu. Gi·ªëng nh∆∞ m·ªôt ƒë·∫ßu b·∫øp n·∫øm th·ª≠ t·ª´ng nguy√™n li·ªáu tr∆∞·ªõc khi n·∫•u, m·ªôt data analyst ph·∫£i "n·∫øm" d·ªØ li·ªáu c·ªßa m√¨nh ƒë·ªÉ hi·ªÉu r√µ h∆∞∆°ng v·ªã, ƒë·∫∑c t√≠nh, v√† nh·ªØng ƒëi·ªÉm b·∫•t th∆∞·ªùng c·ªßa n√≥. M·ª•c ti√™u c·ªßa EDA kh√¥ng ph·∫£i l√† ƒë∆∞a ra k·∫øt lu·∫≠n cu·ªëi c√πng, m√† l√† ƒë·ªÉ **ƒë·∫∑t ra nh·ªØng c√¢u h·ªèi ƒë√∫ng**, **h√¨nh th√†nh c√°c gi·∫£ thuy·∫øt**, v√† **ƒë·ªãnh h∆∞·ªõng cho c√°c b∆∞·ªõc x·ª≠ l√Ω v√† m√¥ h√¨nh h√≥a ti·∫øp theo**.

EDA l√† m·ªôt ngh·ªá thu·∫≠t h∆°n l√† m·ªôt khoa h·ªçc c·ª©ng nh·∫Øc, bao g·ªìm 3 tr·ª• c·ªôt ch√≠nh:
1.  **Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu (Data Quality Assessment)**: D·ªØ li·ªáu c√≥ s·∫°ch kh√¥ng?
2.  **Ph√¢n t√≠ch th·ªëng k√™ (Statistical Analysis)**: D·ªØ li·ªáu n√≥i l√™n ƒëi·ªÅu g√¨?
3.  **Tr·ª±c quan h√≥a (Visualization)**: D·ªØ li·ªáu tr√¥ng nh∆∞ th·∫ø n√†o?

### 2.1 C√°c lo·∫°i ph√¢n t√≠ch trong EDA

T√πy thu·ªôc v√†o s·ªë l∆∞·ª£ng bi·∫øn b·∫°n ƒëang xem x√©t c√πng m·ªôt l√∫c, EDA c√≥ th·ªÉ ƒë∆∞·ª£c chia th√†nh:

1.  **Ph√¢n t√≠ch ƒë∆°n bi·∫øn (Univariate Analysis)**:
    *   **M·ª•c ti√™u**: Hi·ªÉu ƒë·∫∑c ƒëi·ªÉm c·ªßa **m·ªôt bi·∫øn duy nh·∫•t**.
    *   **C√¢u h·ªèi**: Ph√¢n ph·ªëi c·ªßa bi·∫øn n√†y nh∆∞ th·∫ø n√†o? Gi√° tr·ªã trung t√¢m l√† g√¨? D·ªØ li·ªáu ph√¢n t√°n ra sao? C√≥ gi√° tr·ªã ngo·∫°i lai kh√¥ng?
    *   **C√¥ng c·ª•**:
        *   **Bi·∫øn s·ªë (Numeric)**:
            *   Th·ªëng k√™ m√¥ t·∫£ (`describe()`).
            *   Bi·ªÉu ƒë·ªì Histogram, Box plot, KDE plot.
        *   **Bi·∫øn ph√¢n lo·∫°i (Categorical)**:
            *   ƒê·∫øm t·∫ßn su·∫•t (`value_counts()`).
            *   Bi·ªÉu ƒë·ªì Bar chart, Pie chart.

2.  **Ph√¢n t√≠ch hai bi·∫øn (Bivariate Analysis)**:
    *   **M·ª•c ti√™u**: Kh√°m ph√° m·ªëi quan h·ªá gi·ªØa **hai bi·∫øn**.
    *   **C√¢u h·ªèi**: Hai bi·∫øn n√†y c√≥ t∆∞∆°ng quan v·ªõi nhau kh√¥ng? M·ªëi quan h·ªá ƒë√≥ l√† tuy·∫øn t√≠nh hay phi tuy·∫øn? Bi·∫øn ph√¢n lo·∫°i n√†y ·∫£nh h∆∞·ªüng ƒë·∫øn bi·∫øn s·ªë kia nh∆∞ th·∫ø n√†o?
    *   **C√¥ng c·ª•**:
        *   **S·ªë vs. S·ªë**: Bi·ªÉu ƒë·ªì Scatter plot, Heatmap c·ªßa ma tr·∫≠n t∆∞∆°ng quan.
        *   **Ph√¢n lo·∫°i vs. S·ªë**: Bi·ªÉu ƒë·ªì Box plot (cho t·ª´ng nh√≥m), Bar chart (c·ªßa gi√° tr·ªã trung b√¨nh).
        *   **Ph√¢n lo·∫°i vs. Ph√¢n lo·∫°i**: B·∫£ng ch√©o (Contingency Table), bi·ªÉu ƒë·ªì thanh ch·ªìng (Stacked Bar Chart).

3.  **Ph√¢n t√≠ch ƒëa bi·∫øn (Multivariate Analysis)**:
    *   **M·ª•c ti√™u**: Hi·ªÉu m·ªëi quan h·ªá ph·ª©c t·∫°p gi·ªØa **nhi·ªÅu h∆°n hai bi·∫øn**.
    *   **C√¥ng c·ª•**: Scatter plot v·ªõi c√°c chi·ªÅu ƒë∆∞·ª£c m√£ h√≥a b·∫±ng m√†u s·∫Øc/k√≠ch th∆∞·ªõc (`hue`, `size` trong Seaborn), Pair plot, 3D plots.

### 2.2 ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu

#### Ph√¢n t√≠ch gi√° tr·ªã thi·∫øu (Missing Values Analysis)

**T·∫°i sao c·∫ßn ph√¢n t√≠ch?**
-   Gi√° tr·ªã thi·∫øu c√≥ th·ªÉ l√†m sai l·ªách k·∫øt qu·∫£ ph√¢n t√≠ch v√† l√†m h·ªèng m√¥ h√¨nh.
-   **L√Ω do thi·∫øu** c≈©ng l√† m·ªôt th√¥ng tin quan tr·ªçng. D·ªØ li·ªáu c√≥ th·ªÉ thi·∫øu m·ªôt c√°ch ng·∫´u nhi√™n (MCAR), c√≥ li√™n quan ƒë·∫øn c√°c bi·∫øn kh√°c (MAR), ho·∫∑c thi·∫øu m·ªôt c√°ch c√≥ h·ªá th·ªëng (MNAR). Hi·ªÉu ƒë∆∞·ª£c ƒëi·ªÅu n√†y gi√∫p ch·ªçn ph∆∞∆°ng ph√°p x·ª≠ l√Ω ph√π h·ª£p.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_missing_values(df):
    """
    Ph√¢n t√≠ch to√†n di·ªán v·ªÅ gi√° tr·ªã thi·∫øu trong DataFrame.
    """
    missing_data = df.isnull().sum()
    missing_percent = (missing_data / len(df)) * 100
    
    missing_summary = pd.DataFrame({
        'Missing Values': missing_data,
        'Missing Percentage': missing_percent
    }).sort_values('Missing Percentage', ascending=False)
    
    # Ch·ªâ hi·ªÉn th·ªã c√°c c·ªôt c√≥ gi√° tr·ªã thi·∫øu
    missing_summary = missing_summary[missing_summary['Missing Values'] > 0]
    
    if missing_summary.empty:
        print("üéâ Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu trong d·ªØ li·ªáu.")
        return None

    # Tr·ª±c quan h√≥a
    fig, axes = plt.subplots(1, 2, figsize=(18, 7))
    fig.suptitle("Ph√¢n T√≠ch Gi√° Tr·ªã Thi·∫øu", fontsize=16)
    
    # Bar chart
    sns.barplot(x=missing_summary.index, y=missing_summary['Missing Percentage'], ax=axes[0])
    axes[0].set_title('T·ª∑ l·ªá ph·∫ßn trƒÉm gi√° tr·ªã thi·∫øu theo c·ªôt')
    axes[0].set_xlabel('C√°c c·ªôt')
    axes[0].set_ylabel('T·ª∑ l·ªá (%)')
    axes[0].tick_params(axis='x', rotation=45)
    
    # Heatmap
    sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='viridis', ax=axes[1])
    axes[1].set_title('B·∫£n ƒë·ªì nhi·ªát c·ªßa c√°c v·ªã tr√≠ thi·∫øu')
    
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()
    
    return missing_summary

# V√≠ d·ª• s·ª≠ d·ª•ng:
# gi·∫£ s·ª≠ df l√† DataFrame c·ªßa b·∫°n
# missing_info = analyze_missing_values(df)
# if missing_info is not None:
#     print(missing_info)
```

#### Ph√¢n t√≠ch ki·ªÉu d·ªØ li·ªáu (Data Type Analysis)

**T·∫°i sao c·∫ßn ph√¢n t√≠ch?**
-   ƒê·∫£m b·∫£o m·ªói c·ªôt c√≥ ki·ªÉu d·ªØ li·ªáu ƒë√∫ng (s·ªë, ch·ªØ, ng√†y th√°ng).
-   Ph√°t hi·ªán c√°c l·ªói (v√≠ d·ª•: c·ªôt `price` b·ªã l∆∞u d∆∞·ªõi d·∫°ng `string` do c√≥ k√Ω t·ª± '$').
-   T·ªëi ∆∞u h√≥a b·ªô nh·ªõ b·∫±ng c√°ch ch·ªçn ki·ªÉu d·ªØ li·ªáu ph√π h·ª£p (v√≠ d·ª•: `int8` thay v√¨ `int64` n·∫øu gi√° tr·ªã nh·ªè).

### 2.3 Ph√¢n t√≠ch th·ªëng k√™ (Statistical Analysis)

#### Th·ªëng k√™ m√¥ t·∫£ (Descriptive Statistics)

**M·ª•c ƒë√≠ch**: T√≥m t·∫Øt v√† m√¥ t·∫£ c√°c ƒë·∫∑c ƒëi·ªÉm ch√≠nh c·ªßa d·ªØ li·ªáu b·∫±ng c√°c con s·ªë.
-   **Th∆∞·ªõc ƒëo xu h∆∞·ªõng trung t√¢m (Measures of Central Tendency)**:
    -   **Mean (Trung b√¨nh)**: T·ªïng c√°c gi√° tr·ªã chia cho s·ªë l∆∞·ª£ng. Nh·∫°y c·∫£m v·ªõi gi√° tr·ªã ngo·∫°i lai.
    -   **Median (Trung v·ªã)**: Gi√° tr·ªã ·ªü gi·ªØa sau khi s·∫Øp x·∫øp. Kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi gi√° tr·ªã ngo·∫°i lai.
    -   **Mode**: Gi√° tr·ªã xu·∫•t hi·ªán nhi·ªÅu nh·∫•t.
-   **Th∆∞·ªõc ƒëo ƒë·ªô ph√¢n t√°n (Measures of Dispersion)**:
    -   **Standard Deviation (ƒê·ªô l·ªách chu·∫©n)**: ƒêo l∆∞·ªùng m·ª©c ƒë·ªô ph√¢n t√°n c·ªßa d·ªØ li·ªáu so v·ªõi gi√° tr·ªã trung b√¨nh.
    -   **Range (Kho·∫£ng gi√° tr·ªã)**: `max - min`.
    -   **IQR (Kho·∫£ng t·ª© ph√¢n v·ªã)**: `Q3 - Q1`. Kho·∫£ng ch·ª©a 50% d·ªØ li·ªáu ·ªü gi·ªØa, √≠t b·ªã ·∫£nh h∆∞·ªüng b·ªüi outliers.

```python
def comprehensive_descriptive_analysis(df, numeric_cols=None):
    """
    Ph√¢n t√≠ch th·ªëng k√™ m√¥ t·∫£ to√†n di·ªán cho c√°c c·ªôt s·ªë.
    """
    if numeric_cols is None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if not numeric_cols:
        print("Kh√¥ng t√¨m th·∫•y c·ªôt s·ªë n√†o ƒë·ªÉ ph√¢n t√≠ch.")
        return None

    print("üìà PH√ÇN T√çCH TH·ªêNG K√ä M√î T·∫¢")
    print("=" * 60)
    
    # 1. C√°c ch·ªâ s·ªë th·ªëng k√™ c∆° b·∫£n
    basic_stats = df[numeric_cols].describe()
    print(basic_stats)
    
    # ... (ph·∫ßn code c√≤n l·∫°i ƒë·ªÉ ph√¢n t√≠ch Skewness, Kurtosis v√† Outliers)
    
    return basic_stats

# V√≠ d·ª•:
# stats_result = comprehensive_descriptive_analysis(df)
```

#### Ph√¢n t√≠ch t∆∞∆°ng quan (Correlation Analysis)

**M·ª•c ƒë√≠ch**: Hi·ªÉu m·ªëi quan h·ªá **tuy·∫øn t√≠nh** gi·ªØa c√°c bi·∫øn s·ªë.
-   **H·ªá s·ªë t∆∞∆°ng quan (Correlation Coefficient)**: M·ªôt gi√° tr·ªã t·ª´ -1 ƒë·∫øn 1.
    -   **+1**: T∆∞∆°ng quan d∆∞∆°ng ho√†n h·∫£o (bi·∫øn n√†y tƒÉng th√¨ bi·∫øn kia tƒÉng).
    -   **-1**: T∆∞∆°ng quan √¢m ho√†n h·∫£o (bi·∫øn n√†y tƒÉng th√¨ bi·∫øn kia gi·∫£m).
    -   **0**: Kh√¥ng c√≥ t∆∞∆°ng quan tuy·∫øn t√≠nh.

**C·∫°m b·∫´y quan tr·ªçng: T∆∞∆°ng quan kh√¥ng c√≥ nghƒ©a l√† Nh√¢n qu·∫£ (Correlation does not imply causation)!**
-   V√≠ d·ª•: Doanh s·ªë b√°n kem v√† s·ªë v·ª• ch·∫øt ƒëu·ªëi c√≥ t∆∞∆°ng quan d∆∞∆°ng m·∫°nh.
-   **T∆∞∆°ng quan**: C·∫£ hai ƒë·ªÅu tƒÉng v√†o m√πa h√®.
-   **Nh√¢n qu·∫£**: Kh√¥ng ph·∫£i ƒÉn kem g√¢y ra ch·∫øt ƒëu·ªëi. Bi·∫øn th·ª© ba (nhi·ªát ƒë·ªô m√πa h√®) l√† nguy√™n nh√¢n chung g√¢y ra c·∫£ hai.
-   Lu√¥n ƒë·∫∑t c√¢u h·ªèi "T·∫°i sao?" khi th·∫•y m·ªôt m·ªëi t∆∞∆°ng quan.

**C√°c ph∆∞∆°ng ph√°p t√≠nh t∆∞∆°ng quan**:
-   **Pearson**: ƒêo l∆∞·ªùng m·ªëi quan h·ªá **tuy·∫øn t√≠nh**. Y√™u c·∫ßu d·ªØ li·ªáu c√≥ ph√¢n ph·ªëi g·∫ßn chu·∫©n.
-   **Spearman**: ƒêo l∆∞·ªùng m·ªëi quan h·ªá **ƒë∆°n ƒëi·ªáu** (c·ª© tƒÉng/gi·∫£m l√† ƒë∆∞·ª£c, kh√¥ng c·∫ßn th·∫≥ng). Ho·∫°t ƒë·ªông tr√™n th·ª© h·∫°ng c·ªßa d·ªØ li·ªáu, do ƒë√≥ √≠t b·ªã ·∫£nh h∆∞·ªüng b·ªüi outliers.

```python
def correlation_analysis(df, numeric_cols=None, method='pearson'):
    """
    Ph√¢n t√≠ch t∆∞∆°ng quan v√† tr·ª±c quan h√≥a b·∫±ng heatmap.
    """
    if numeric_cols is None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
    if len(numeric_cols) < 2:
        print("C·∫ßn √≠t nh·∫•t 2 c·ªôt s·ªë ƒë·ªÉ ph√¢n t√≠ch t∆∞∆°ng quan.")
        return None

    corr_matrix = df[numeric_cols].corr(method=method)
    
    plt.figure(figsize=(12, 10))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", center=0)
    plt.title(f'Ma tr·∫≠n t∆∞∆°ng quan ({method.capitalize()})')
    plt.show()
    
    # T√¨m c√°c c·∫∑p c√≥ t∆∞∆°ng quan m·∫°nh
    strong_corr = corr_matrix.abs().unstack().sort_values(ascending=False)
    strong_corr = strong_corr[strong_corr != 1.0] # B·ªè c√°c c·∫∑p t·ª± t∆∞∆°ng quan
    print("\nC√°c c·∫∑p t∆∞∆°ng quan m·∫°nh nh·∫•t:")
    print(strong_corr.head(10))
    
    return corr_matrix

# V√≠ d·ª•:
# corr_result = correlation_analysis(df)
```
## üìà 3. Tr·ª±c quan h√≥a d·ªØ li·ªáu

### 3.1 Dashboard Creation - T·∫°o Dashboard

> **Dashboard** l√† c√¥ng c·ª• tr·ª±c quan h√≥a d·ªØ li·ªáu ƒë·ªÉ theo d√µi KPIs v√† ph√°t hi·ªán trends.

#### Interactive Dashboard v·ªõi Plotly

```python
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import dash
from dash import dcc, html
from dash.dependencies import Input, Output

def create_interactive_dashboard(df):
    """
    T·∫°o dashboard t∆∞∆°ng t√°c v·ªõi Plotly Dash
    
    Parameters:
    df (pd.DataFrame): DataFrame ch·ª©a d·ªØ li·ªáu
    """
    app = dash.Dash(__name__)
    
    app.layout = html.Div([
        html.H1("üìä Data Analysis Dashboard", 
                style={'textAlign': 'center', 'color': '#2c3e50'}),
        
        # Filters
        html.Div([
            html.Label("Select Date Range:"),
            dcc.DatePickerRange(
                id='date-picker',
                start_date=df['date'].min(),
                end_date=df['date'].max(),
                display_format='YYYY-MM-DD'
            ),
            
            html.Label("Select Category:"),
            dcc.Dropdown(
                id='category-dropdown',
                options=[{'label': cat, 'value': cat} for cat in df['category'].unique()],
                value=df['category'].unique()[0],
                multi=True
            )
        ], style={'margin': '20px'}),
        
        # Charts
        html.Div([
            dcc.Graph(id='time-series-chart'),
            dcc.Graph(id='distribution-chart'),
            dcc.Graph(id='correlation-chart')
        ])
    ])
    
    # Callbacks
    @app.callback(
        Output('time-series-chart', 'figure'),
        [Input('date-picker', 'start_date'),
         Input('date-picker', 'end_date'),
         Input('category-dropdown', 'value')]
    )
    def update_time_series(start_date, end_date, categories):
        # Filter data
        filtered_df = df[
            (df['date'] >= start_date) & 
            (df['date'] <= end_date) & 
            (df['category'].isin(categories))
        ]
        
        # Create time series chart
        fig = px.line(filtered_df, x='date', y='value', color='category',
                     title='Time Series Analysis')
        fig.update_layout(height=400)
        return fig
    
    @app.callback(
        Output('distribution-chart', 'figure'),
        [Input('category-dropdown', 'value')]
    )
    def update_distribution(categories):
        filtered_df = df[df['category'].isin(categories)]
        
        fig = px.histogram(filtered_df, x='value', color='category',
                          title='Value Distribution by Category',
                          barmode='overlay')
        fig.update_layout(height=400)
        return fig
    
    @app.callback(
        Output('correlation-chart', 'figure'),
        [Input('category-dropdown', 'value')]
    )
    def update_correlation(categories):
        filtered_df = df[df['category'].isin(categories)]
        
        # Calculate correlation matrix
        numeric_cols = filtered_df.select_dtypes(include=[np.number]).columns
        corr_matrix = filtered_df[numeric_cols].corr()
        
        fig = px.imshow(corr_matrix, 
                       title='Correlation Matrix',
                       color_continuous_scale='RdBu',
                       aspect='auto')
        fig.update_layout(height=400)
        return fig
    
    return app

# V√≠ d·ª• s·ª≠ d·ª•ng
# app = create_interactive_dashboard(df)
# app.run_server(debug=True, port=8050)
```

**Gi·∫£i th√≠ch Dashboard components:**
- **Filters**: Cho ph√©p ng∆∞·ªùi d√πng l·ªçc d·ªØ li·ªáu theo ti√™u ch√≠
- **Callbacks**: C·∫≠p nh·∫≠t charts t·ª± ƒë·ªông khi filters thay ƒë·ªïi
- **Responsive Design**: T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc theo m√†n h√¨nh

## üß™ 4. A/B Testing v√† Causal Inference

### 4.1 Experimental Design - Thi·∫øt k·∫ø th√≠ nghi·ªám

> **A/B Testing** l√† ph∆∞∆°ng ph√°p so s√°nh hai phi√™n b·∫£n ƒë·ªÉ x√°c ƒë·ªãnh phi√™n b·∫£n n√†o hi·ªáu qu·∫£ h∆°n.

#### A/B Test Framework

```python
import scipy.stats as stats
from scipy.stats import chi2_contingency
import numpy as np

def design_ab_test(baseline_conversion, mde, alpha=0.05, power=0.8):
    """
    T√≠nh to√°n sample size c·∫ßn thi·∫øt cho A/B test
    
    Parameters:
    baseline_conversion (float): T·ª∑ l·ªá chuy·ªÉn ƒë·ªïi c∆° b·∫£n (0-1)
    mde (float): Minimum Detectable Effect (0-1)
    alpha (float): Significance level (Type I error)
    power (float): Statistical power (1 - Type II error)
    
    Returns:
    dict: Th√¥ng tin v·ªÅ sample size v√† test parameters
    """
    from statsmodels.stats.power import proportions_effect_size
    from statsmodels.stats.power import zt_ind_solve_power
    
    # Effect size
    effect_size = proportions_effect_size(baseline_conversion, 
                                        baseline_conversion + mde)
    
    # Sample size per group
    sample_size = zt_ind_solve_power(effect_size=effect_size,
                                    alpha=alpha,
                                    power=power,
                                    ratio=1.0)
    
    # Round up to nearest integer
    sample_size = int(np.ceil(sample_size))
    
    print("üß™ A/B TEST SAMPLE SIZE CALCULATION")
    print("=" * 50)
    print(f"Baseline Conversion Rate: {baseline_conversion:.3f} ({baseline_conversion*100:.1f}%)")
    print(f"Minimum Detectable Effect: {mde:.3f} ({mde*100:.1f}%)")
    print(f"Significance Level (Œ±): {alpha:.3f}")
    print(f"Statistical Power: {power:.3f}")
    print(f"Effect Size: {effect_size:.3f}")
    print(f"Sample Size per Group: {sample_size:,}")
    print(f"Total Sample Size: {sample_size * 2:,}")
    
    return {
        'baseline_conversion': baseline_conversion,
        'mde': mde,
        'alpha': alpha,
        'power': power,
        'effect_size': effect_size,
        'sample_size_per_group': sample_size,
        'total_sample_size': sample_size * 2
    }

def analyze_ab_test_results(control_data, treatment_data, alpha=0.05):
    """
    Ph√¢n t√≠ch k·∫øt qu·∫£ A/B test
    
    Parameters:
    control_data (array): D·ªØ li·ªáu nh√≥m control
    treatment_data (array): D·ªØ li·ªáu nh√≥m treatment
    alpha (float): Significance level
    
    Returns:
    dict: K·∫øt qu·∫£ ph√¢n t√≠ch th·ªëng k√™
    """
    print("üìä A/B TEST RESULTS ANALYSIS")
    print("=" * 40)
    
    # Basic statistics
    control_mean = np.mean(control_data)
    treatment_mean = np.mean(treatment_data)
    control_std = np.std(control_data, ddof=1)
    treatment_std = np.std(treatment_data, ddof=1)
    
    print(f"Control Group:")
    print(f"  Mean: {control_mean:.4f}")
    print(f"  Std: {control_std:.4f}")
    print(f"  Sample Size: {len(control_data)}")
    
    print(f"\nTreatment Group:")
    print(f"  Mean: {treatment_mean:.4f}")
    print(f"  Std: {treatment_std:.4f}")
    print(f"  Sample Size: {len(treatment_data)}")
    
    # Effect size
    effect_size = (treatment_mean - control_mean) / control_mean
    print(f"\nEffect Size: {effect_size:.4f} ({effect_size*100:.2f}%)")
    
    # Statistical tests
    # 1. T-test (parametric)
    t_stat, p_value = stats.ttest_ind(control_data, treatment_data)
    
    # 2. Mann-Whitney U test (non-parametric)
    u_stat, u_p_value = stats.mannwhitneyu(control_data, treatment_data, 
                                          alternative='two-sided')
    
    print(f"\nüìà Statistical Tests:")
    print(f"T-test:")
    print(f"  t-statistic: {t_stat:.4f}")
    print(f"  p-value: {p_value:.6f}")
    print(f"  Significant: {'Yes' if p_value < alpha else 'No'}")
    
    print(f"\nMann-Whitney U test:")
    print(f"  U-statistic: {u_stat:.4f}")
    print(f"  p-value: {u_p_value:.6f}")
    print(f"  Significant: {'Yes' if u_p_value < alpha else 'No'}")
    
    # Confidence interval
    from scipy.stats import t
    n1, n2 = len(control_data), len(treatment_data)
    pooled_std = np.sqrt(((n1-1)*control_std**2 + (n2-1)*treatment_std**2) / (n1+n2-2))
    se = pooled_std * np.sqrt(1/n1 + 1/n2)
    t_critical = t.ppf(1 - alpha/2, n1 + n2 - 2)
    
    ci_lower = (treatment_mean - control_mean) - t_critical * se
    ci_upper = (treatment_mean - control_mean) + t_critical * se
    
    print(f"\nüéØ 95% Confidence Interval for Difference:")
    print(f"  Lower bound: {ci_lower:.6f}")
    print(f"  Upper bound: {ci_upper:.6f}")
    
    # Decision
    print(f"\n‚úÖ CONCLUSION:")
    if p_value < alpha:
        if treatment_mean > control_mean:
            print(f"  Treatment is significantly BETTER than control")
        else:
            print(f"  Treatment is significantly WORSE than control")
    else:
        print(f"  No significant difference between groups")
    
    return {
        'control_mean': control_mean,
        'treatment_mean': treatment_mean,
        'effect_size': effect_size,
        't_stat': t_stat,
        'p_value': p_value,
        'significant': p_value < alpha,
        'confidence_interval': (ci_lower, ci_upper)
    }

# V√≠ d·ª• s·ª≠ d·ª•ng
# Sample size calculation
# sample_info = design_ab_test(baseline_conversion=0.05, mde=0.01)

# Analyze results
# results = analyze_ab_test_results(control_data, treatment_data)
```

**Gi·∫£i th√≠ch c√°c kh√°i ni·ªám A/B Testing:**
- **Baseline Conversion**: T·ª∑ l·ªá chuy·ªÉn ƒë·ªïi hi·ªán t·∫°i c·ªßa nh√≥m control
- **MDE (Minimum Detectable Effect)**: Hi·ªáu ·ª©ng t·ªëi thi·ªÉu c√≥ th·ªÉ ph√°t hi·ªán
- **Alpha (Œ±)**: X√°c su·∫•t m·∫Øc l·ªói Type I (reject null hypothesis khi n√≥ ƒë√∫ng)
- **Power**: X√°c su·∫•t ph√°t hi·ªán effect khi n√≥ th·ª±c s·ª± t·ªìn t·∫°i
- **Effect Size**: ƒê·ªô l·ªõn c·ªßa s·ª± kh√°c bi·ªát gi·ªØa hai nh√≥m

## üìä 5. B√°o c√°o v√† Storytelling

### 5.1 Executive Summary Template

> **Executive Summary** l√† b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn d√†nh cho c·∫•p qu·∫£n l√Ω, t·∫≠p trung v√†o insights v√† recommendations.

#### Template b√°o c√°o chuy√™n nghi·ªáp

```python
def create_executive_summary(data_insights, recommendations, kpis):
    """
    T·∫°o executive summary template
    
    Parameters:
    data_insights (list): Danh s√°ch c√°c insights ch√≠nh
    recommendations (list): Danh s√°ch recommendations
    kpis (dict): Dictionary ch·ª©a KPIs v√† values
    """
    
    summary = f"""
# üìã EXECUTIVE SUMMARY
*Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}*

## üéØ Key Findings

"""
    
    for i, insight in enumerate(data_insights, 1):
        summary += f"{i}. {insight}\n"
    
    summary += f"""
## üìä Performance Metrics

"""
    
    for metric, value in kpis.items():
        if isinstance(value, float):
            summary += f"- **{metric}**: {value:.2f}\n"
        else:
            summary += f"- **{metric}**: {value}\n"
    
    summary += f"""
## üöÄ Strategic Recommendations

"""
    
    for i, rec in enumerate(recommendations, 1):
        summary += f"{i}. {rec}\n"
    
    summary += f"""
## ‚è∞ Next Steps

1. **Immediate (1-2 weeks)**: {recommendations[0] if recommendations else "Review findings"}
2. **Short-term (1-3 months)**: Implement key recommendations
3. **Long-term (3-6 months)**: Monitor and optimize based on results

---
*This report was generated automatically using data analysis tools.*
"""
    
    return summary

# V√≠ d·ª• s·ª≠ d·ª•ng
# insights = [
#     "Customer retention rate increased by 15% after implementing new onboarding",
#     "Mobile users show 25% higher engagement than desktop users",
#     "Peak usage occurs between 7-9 PM on weekdays"
# ]
# 
# recommendations = [
#     "Optimize mobile experience for better user engagement",
#     "Implement targeted marketing campaigns during peak hours",
#     "Enhance onboarding process based on successful patterns"
# ]
# 
# kpis = {
#     "Customer Retention Rate": 0.85,
#     "Average Session Duration": 12.5,
#     "Conversion Rate": 0.034,
#     "Customer Satisfaction Score": 4.2
# }
# 
# executive_summary = create_executive_summary(insights, recommendations, kpis)
# print(executive_summary)
```

## üìö T√†i li·ªáu tham kh·∫£o

### CRISP-DM v√† Quy tr√¨nh
- [CRISP-DM Guide](https://www.datascience-pm.com/crisp-dm-2/) - H∆∞·ªõng d·∫´n chi ti·∫øt
- [Data Science Process](https://www.datascience-pm.com/) - Quy tr√¨nh khoa h·ªçc d·ªØ li·ªáu

### EDA v√† Visualization
- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) - Jake VanderPlas
- [Storytelling with Data](https://www.storytellingwithdata.com/) - Cole Nussbaumer Knaflic

### A/B Testing
- [A/B Testing Guide](https://www.optimizely.com/optimization-guidance/) - Optimizely
- [Statistical Methods in A/B Testing](https://www.evanmiller.org/ab-testing/) - Evan Miller

### Dashboard v√† BI
- [Plotly Dash Documentation](https://dash.plotly.com/) - H∆∞·ªõng d·∫´n Plotly Dash
- [Tableau Best Practices](https://help.tableau.com/) - Tableau guidelines

## üéØ B√†i t·∫≠p th·ª±c h√†nh

1. **Data Quality Assessment**: Ph√¢n t√≠ch dataset th·ª±c t·∫ø, x·ª≠ l√Ω missing values
2. **EDA Project**: Kh√°m ph√° dataset, t·∫°o b√°o c√°o insights
3. **Dashboard Creation**: X√¢y d·ª±ng dashboard t∆∞∆°ng t√°c v·ªõi Plotly Dash
4. **A/B Test Design**: Thi·∫øt k·∫ø v√† ph√¢n t√≠ch A/B test
5. **Executive Report**: T·∫°o b√°o c√°o chuy√™n nghi·ªáp cho stakeholders

## üöÄ B∆∞·ªõc ti·∫øp theo

Sau khi ho√†n th√†nh Data Analyst, b·∫°n s·∫Ω:
- C√≥ kh·∫£ nƒÉng ph√¢n t√≠ch d·ªØ li·ªáu m·ªôt c√°ch c√≥ h·ªá th·ªëng
- Bi·∫øt c√°ch t·∫°o visualization v√† dashboard chuy√™n nghi·ªáp
- Hi·ªÉu v·ªÅ A/B testing v√† causal inference
- C√≥ th·ªÉ t·∫°o b√°o c√°o insights cho business stakeholders
- S·∫µn s√†ng h·ªçc Machine Learning v√† Data Science

---

*Ch√∫c b·∫°n tr·ªü th√†nh Data Analyst xu·∫•t s·∫Øc! üéâ*

## üß© Ch∆∞∆°ng tr√¨nh 50/50 (L√Ω thuy·∫øt : Th·ª±c h√†nh)

- M·ª•c ti√™u: 50% th·ªùi l∆∞·ª£ng cho l√Ω thuy·∫øt (khung ph∆∞∆°ng ph√°p, kh√°i ni·ªám th·ªëng k√™), 50% cho th·ª±c h√†nh (notebook, dashboard, b√°o c√°o)

| M√¥-ƒëun | L√Ω thuy·∫øt (50%) | Th·ª±c h√†nh (50%) |
|---|---|---|
| CRISP-DM | Pha, deliverables, ti√™u ch√≠ | Mapping project th·∫≠t, checklist quy tr√¨nh |
| EDA | Th·ªëng k√™ m√¥ t·∫£, ph√¢n ph·ªëi, ki·ªÉm ƒë·ªãnh | Notebook EDA chu·∫©n ho√° + k·∫øt lu·∫≠n |
| Visualization | Nguy√™n t·∫Øc ch·ªçn bi·ªÉu ƒë·ªì, storytelling | Dashboard Plotly Dash/BI c√≥ filter |
| A/B Testing | Thi·∫øt k·∫ø th√≠ nghi·ªám, power, ch·ªçn test | Ph√¢n t√≠ch A/B (t-test/MWU) + report |
| Reporting | Executive summary, narrative | 1-pager v√† deck tr√¨nh b√†y |

Rubric (100ƒë/module): L√Ω thuy·∫øt 30 | Code/Notebook 30 | K·∫øt qu·∫£ ƒë√∫ng 30 | B√°o c√°o 10

---


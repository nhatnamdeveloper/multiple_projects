# üöÄ Learning AI ‚Äî Split Documentation Index

> **M·ª•c ti√™u**: Cung c·∫•p t√†i li·ªáu h·ªçc t·∫≠p AI/ML/Data Science ƒë∆∞·ª£c chia nh·ªè, d·ªÖ hi·ªÉu v√† c√≥ c·∫•u tr√∫c r√µ r√†ng

## üß† **Global Theory Index & Academic Framework**

### **1. Theoretical Foundation Overview**

**Academic Structure:**
- **50% Theory / 50% Practice**: Balanced curriculum across all modules
- **Mathematical Foundations**: Linear algebra, calculus, probability, statistics
- **Computer Science Theory**: Algorithms, data structures, complexity analysis
- **Machine Learning Theory**: Learning theory, optimization, generalization
- **Deep Learning Theory**: Neural networks, backpropagation, attention mechanisms

**Research Areas & Publications:**
- **Core Papers**: Foundational research papers in AI/ML
- **Recent Advances**: State-of-the-art developments and breakthroughs
- **Implementation Guides**: Practical applications and best practices
- **Evaluation Metrics**: Standardized benchmarks and assessment criteria

### **2. Mathematical Theory Framework**

**Linear Algebra & Calculus:**
```python
class MathematicalTheoryFramework:
    """Theoretical framework cho mathematical foundations"""
    
    @staticmethod
    def explain_mathematical_foundations():
        """Explain mathematical foundations for AI/ML"""
        print("""
        **Mathematical Foundations for AI/ML:**
        
        1. **Linear Algebra:**
           - **Vector Spaces**: Basis, dimension, linear independence
           - **Matrix Operations**: Eigenvalues, eigenvectors, SVD
           - **Linear Transformations**: Projections, rotations, scaling
           - **Applications**: PCA, dimensionality reduction, feature extraction
        
        2. **Calculus & Optimization:**
           - **Multivariate Calculus**: Gradients, Hessians, chain rule
           - **Optimization Theory**: Convexity, Lagrange multipliers, KKT conditions
           - **Numerical Methods**: Gradient descent, Newton's method, line search
           - **Applications**: Neural network training, parameter optimization
        
        3. **Probability & Statistics:**
           - **Probability Theory**: Random variables, distributions, Bayes' rule
           - **Statistical Inference**: Hypothesis testing, confidence intervals
           - **Information Theory**: Entropy, mutual information, KL divergence
           - **Applications**: Model uncertainty, decision theory, causal inference
        
        4. **Graph Theory & Algorithms:**
           - **Graph Representations**: Adjacency matrices, edge lists
           - **Graph Algorithms**: Shortest path, minimum spanning tree
           - **Network Analysis**: Centrality, clustering, community detection
           - **Applications**: Social networks, recommendation systems, knowledge graphs
        """)
    
    @staticmethod
    def demonstrate_mathematical_concepts():
        """Demonstrate mathematical concepts with examples"""
        
        import numpy as np
        import matplotlib.pyplot as plt
        from scipy import linalg
        from scipy.stats import norm, multivariate_normal
        
        class MathematicalAnalyzer:
            """Analyze mathematical concepts in AI/ML context"""
            
            def __init__(self):
                self.results = {}
            
            def demonstrate_linear_algebra(self):
                """Demonstrate linear algebra concepts"""
                
                print("**Linear Algebra Demonstration:**")
                
                # Create sample data matrix
                np.random.seed(42)
                data = np.random.randn(100, 10)
                
                # 1. Principal Component Analysis (PCA)
                # Center the data
                data_centered = data - np.mean(data, axis=0)
                
                # Calculate covariance matrix
                cov_matrix = np.cov(data_centered.T)
                
                # Eigenvalue decomposition
                eigenvalues, eigenvectors = linalg.eigh(cov_matrix)
                
                # Sort by eigenvalues (descending)
                sorted_indices = np.argsort(eigenvalues)[::-1]
                eigenvalues = eigenvalues[sorted_indices]
                eigenvectors = eigenvectors[:, sorted_indices]
                
                # Project data onto principal components
                data_pca = data_centered @ eigenvectors
                
                print(f"Original data shape: {data.shape}")
                print(f"PCA data shape: {data_pca.shape}")
                print(f"Explained variance ratio: {eigenvalues / np.sum(eigenvalues)}")
                
                # Visualize explained variance
                plt.figure(figsize=(12, 5))
                
                plt.subplot(1, 2, 1)
                plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, 'bo-', linewidth=2)
                plt.xlabel('Principal Component')
                plt.ylabel('Eigenvalue')
                plt.title('Eigenvalues (Variance)')
                plt.grid(True, alpha=0.3)
                
                plt.subplot(1, 2, 2)
                cumulative_variance = np.cumsum(eigenvalues) / np.sum(eigenvalues)
                plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'ro-', linewidth=2)
                plt.xlabel('Principal Component')
                plt.ylabel('Cumulative Explained Variance')
                plt.title('Cumulative Explained Variance')
                plt.grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.show()
                
                return {
                    'data': data,
                    'eigenvalues': eigenvalues,
                    'eigenvectors': eigenvectors,
                    'data_pca': data_pca
                }
            
            def demonstrate_optimization(self):
                """Demonstrate optimization concepts"""
                
                print("\n**Optimization Demonstration:**")
                
                # Define a simple function to optimize
                def rosenbrock(x, y):
                    """Rosenbrock function: f(x,y) = (1-x)¬≤ + 100(y-x¬≤)¬≤"""
                    return (1 - x)**2 + 100 * (y - x**2)**2
                
                def rosenbrock_gradient(x, y):
                    """Gradient of Rosenbrock function"""
                    dx = -2 * (1 - x) - 400 * x * (y - x**2)
                    dy = 200 * (y - x**2)
                    return np.array([dx, dy])
                
                # Gradient descent optimization
                def gradient_descent(start_point, learning_rate=0.001, max_iterations=1000):
                    """Simple gradient descent implementation"""
                    
                    point = np.array(start_point)
                    trajectory = [point.copy()]
                    function_values = [rosenbrock(*point)]
                    
                    for iteration in range(max_iterations):
                        gradient = rosenbrock_gradient(*point)
                        point = point - learning_rate * gradient
                        
                        trajectory.append(point.copy())
                        function_values.append(rosenbrock(*point))
                        
                        # Check convergence
                        if np.linalg.norm(gradient) < 1e-6:
                            break
                    
                    return np.array(trajectory), function_values
                
                # Run optimization
                start_point = [-1.5, -1.5]
                trajectory, function_values = gradient_descent(start_point)
                
                print(f"Starting point: {start_point}")
                print(f"Final point: {trajectory[-1]}")
                print(f"Final function value: {function_values[-1]:.6f}")
                print(f"Number of iterations: {len(trajectory)}")
                
                # Visualize optimization
                x = np.linspace(-2, 2, 100)
                y = np.linspace(-2, 2, 100)
                X, Y = np.meshgrid(x, y)
                Z = rosenbrock(X, Y)
                
                plt.figure(figsize=(12, 5))
                
                # Contour plot with trajectory
                plt.subplot(1, 2, 1)
                contour = plt.contour(X, Y, Z, levels=20, colors='black', alpha=0.5)
                plt.clabel(contour, inline=True, fontsize=8)
                
                # Plot optimization trajectory
                trajectory = np.array(trajectory)
                plt.plot(trajectory[:, 0], trajectory[:, 1], 'r-o', linewidth=2, markersize=4)
                plt.plot(start_point[0], start_point[1], 'go', markersize=10, label='Start')
                plt.plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', markersize=10, label='End')
                
                plt.xlabel('x')
                plt.ylabel('y')
                plt.title('Gradient Descent on Rosenbrock Function')
                plt.legend()
                plt.grid(True, alpha=0.3)
                
                # Function value convergence
                plt.subplot(1, 2, 2)
                plt.plot(function_values, 'b-', linewidth=2)
                plt.xlabel('Iteration')
                plt.ylabel('Function Value')
                plt.title('Convergence of Function Value')
                plt.yscale('log')
                plt.grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.show()
                
                return {
                    'trajectory': trajectory,
                    'function_values': function_values,
                    'start_point': start_point,
                    'final_point': trajectory[-1]
                }
            
            def demonstrate_probability_statistics(self):
                """Demonstrate probability and statistics concepts"""
                
                print("\n**Probability & Statistics Demonstration:**")
                
                # Generate sample data
                np.random.seed(42)
                n_samples = 1000
                
                # Two normal distributions
                data1 = np.random.normal(0, 1, n_samples)
                data2 = np.random.normal(2, 1.5, n_samples)
                
                # Combine data
                combined_data = np.concatenate([data1, data2])
                
                # Calculate statistics
                mean1, std1 = np.mean(data1), np.std(data1)
                mean2, std2 = np.mean(data2), np.std(data2)
                mean_combined, std_combined = np.mean(combined_data), np.std(combined_data)
                
                print(f"Distribution 1: Mean={mean1:.3f}, Std={std1:.3f}")
                print(f"Distribution 2: Mean={mean2:.3f}, Std={std2:.3f}")
                print(f"Combined: Mean={mean_combined:.3f}, Std={std_combined:.3f}")
                
                # Hypothesis testing: t-test
                from scipy.stats import ttest_ind
                t_stat, p_value = ttest_ind(data1, data2)
                
                print(f"\nT-test results:")
                print(f"  T-statistic: {t_stat:.3f}")
                print(f"  P-value: {p_value:.6f}")
                print(f"  Significant difference: {p_value < 0.05}")
                
                # Information theory: KL divergence
                def kl_divergence(p, q):
                    """Calculate KL divergence between two distributions"""
                    # Add small epsilon to avoid log(0)
                    epsilon = 1e-10
                    p = p + epsilon
                    q = q + epsilon
                    return np.sum(p * np.log(p / q))
                
                # Create histograms for KL divergence
                hist1, bins = np.histogram(data1, bins=30, density=True)
                hist2, _ = np.histogram(data2, bins=bins, density=True)
                
                # Normalize histograms
                hist1 = hist1 / np.sum(hist1)
                hist2 = hist2 / np.sum(hist2)
                
                kl_div = kl_divergence(hist1, hist2)
                print(f"\nKL Divergence (P||Q): {kl_div:.4f}")
                
                # Visualize distributions
                plt.figure(figsize=(15, 5))
                
                # Individual distributions
                plt.subplot(1, 3, 1)
                plt.hist(data1, bins=30, alpha=0.7, density=True, label='Distribution 1')
                plt.hist(data2, bins=30, alpha=0.7, density=True, label='Distribution 2')
                plt.xlabel('Value')
                plt.ylabel('Density')
                plt.title('Individual Distributions')
                plt.legend()
                plt.grid(True, alpha=0.3)
                
                # Combined distribution
                plt.subplot(1, 3, 2)
                plt.hist(combined_data, bins=30, alpha=0.7, density=True, color='green')
                plt.xlabel('Value')
                plt.ylabel('Density')
                plt.title('Combined Distribution')
                plt.grid(True, alpha=0.3)
                
                # Q-Q plot for normality test
                plt.subplot(1, 3, 3)
                from scipy.stats import probplot
                probplot(combined_data, dist="norm", plot=plt)
                plt.title('Q-Q Plot (Normality Test)')
                plt.grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.show()
                
                return {
                    'data1': data1,
                    'data2': data2,
                    'combined_data': combined_data,
                    't_test': (t_stat, p_value),
                    'kl_divergence': kl_div
                }
        
        # Demonstrate mathematical theory
        math_theory = MathematicalTheoryFramework()
        math_theory.explain_mathematical_foundations()
        
        # Demonstrate mathematical analysis
        math_analyzer = MathematicalAnalyzer()
        
        print("**Mathematical Theory Demonstration:**")
        
        # Demonstrate different mathematical concepts
        linear_algebra_results = math_analyzer.demonstrate_linear_algebra()
        optimization_results = math_analyzer.demonstrate_optimization()
        probability_results = math_analyzer.demonstrate_probability_statistics()
        
        return math_analyzer, linear_algebra_results, optimization_results, probability_results

# Demonstrate mathematical theory
math_theory = MathematicalTheoryFramework()
math_theory.explain_mathematical_foundations()

# Demonstrate mathematical analysis
math_analyzer, linear_algebra_results, optimization_results, probability_results = math_theory.demonstrate_mathematical_concepts()
```

### **3. Computer Science Theory Framework**

**Algorithms & Data Structures:**
```python
class ComputerScienceTheoryFramework:
    """Theoretical framework cho computer science foundations"""
    
    @staticmethod
    def explain_cs_foundations():
        """Explain computer science foundations for AI/ML"""
        print("""
        **Computer Science Foundations for AI/ML:**
        
        1. **Algorithm Analysis:**
           - **Time Complexity**: Big O notation, asymptotic analysis
           - **Space Complexity**: Memory usage, storage requirements
           - **Algorithm Design**: Divide and conquer, dynamic programming
           - **Optimization**: Greedy algorithms, approximation algorithms
        
        2. **Data Structures:**
           - **Linear Structures**: Arrays, linked lists, stacks, queues
           - **Tree Structures**: Binary trees, heaps, B-trees, tries
           - **Graph Structures**: Adjacency lists, adjacency matrices
           - **Hash Tables**: Collision resolution, load factor, rehashing
        
        3. **Complexity Theory:**
           - **P vs NP**: Polynomial time, nondeterministic polynomial time
           - **NP-Complete Problems**: Reduction, hardness proofs
           - **Approximation Algorithms**: Performance guarantees, approximation ratios
           - **Randomized Algorithms**: Probabilistic analysis, expected performance
        """)
    
    @staticmethod
    def demonstrate_cs_concepts():
        """Demonstrate computer science concepts with examples"""
        
        import time
        import matplotlib.pyplot as plt
        import numpy as np
        
        class CSTheoryAnalyzer:
            """Analyze computer science concepts in AI/ML context"""
            
            def __init__(self):
                self.results = {}
            
            def demonstrate_algorithm_complexity(self):
                """Demonstrate algorithm complexity analysis"""
                
                print("**Algorithm Complexity Demonstration:**")
                
                # Test different algorithms with varying input sizes
                input_sizes = [100, 500, 1000, 2000, 5000]
                
                # Linear algorithm: O(n)
                def linear_algorithm(n):
                    result = 0
                    for i in range(n):
                        result += i
                    return result
                
                # Quadratic algorithm: O(n¬≤)
                def quadratic_algorithm(n):
                    result = 0
                    for i in range(n):
                        for j in range(n):
                            result += i * j
                    return result
                
                # Logarithmic algorithm: O(log n)
                def logarithmic_algorithm(n):
                    result = 0
                    i = n
                    while i > 1:
                        result += 1
                        i = i // 2
                    return result
                
                # Measure execution times
                algorithms = {
                    'O(n)': linear_algorithm,
                    'O(n¬≤)': quadratic_algorithm,
                    'O(log n)': logarithmic_algorithm
                }
                
                execution_times = {name: [] for name in algorithms.keys()}
                
                for size in input_sizes:
                    for name, algorithm in algorithms.items():
                        start_time = time.time()
                        algorithm(size)
                        end_time = time.time()
                        execution_times[name].append(end_time - start_time)
                
                # Visualize complexity comparison
                plt.figure(figsize=(12, 5))
                
                # Execution time vs input size
                plt.subplot(1, 2, 1)
                for name, times in execution_times.items():
                    plt.plot(input_sizes, times, 'o-', linewidth=2, markersize=8, label=name)
                
                plt.xlabel('Input Size (n)')
                plt.ylabel('Execution Time (seconds)')
                plt.title('Algorithm Complexity Comparison')
                plt.legend()
                plt.grid(True, alpha=0.3)
                plt.yscale('log')
                
                # Theoretical complexity curves
                plt.subplot(1, 2, 2)
                n_theoretical = np.linspace(100, 5000, 100)
                
                # Normalize theoretical curves to match actual times
                plt.plot(n_theoretical, n_theoretical / 1e6, '--', linewidth=2, label='O(n) theoretical')
                plt.plot(n_theoretical, (n_theoretical**2) / 1e8, '--', linewidth=2, label='O(n¬≤) theoretical')
                plt.plot(n_theoretical, np.log2(n_theoretical) / 1e4, '--', linewidth=2, label='O(log n) theoretical')
                
                plt.xlabel('Input Size (n)')
                plt.ylabel('Normalized Time')
                plt.title('Theoretical Complexity Curves')
                plt.legend()
                plt.grid(True, alpha=0.3)
                plt.yscale('log')
                
                plt.tight_layout()
                plt.show()
                
                return {
                    'input_sizes': input_sizes,
                    'execution_times': execution_times,
                    'algorithms': algorithms
                }
            
            def demonstrate_data_structures(self):
                """Demonstrate data structure performance"""
                
                print("\n**Data Structure Performance Demonstration:**")
                
                # Test different data structures
                import random
                
                # Test sizes
                sizes = [1000, 5000, 10000, 50000]
                
                # Data structure performance tests
                def test_list_operations(size):
                    """Test list operations"""
                    # Create list
                    start_time = time.time()
                    test_list = list(range(size))
                    creation_time = time.time() - start_time
                    
                    # Search operation
                    start_time = time.time()
                    target = random.randint(0, size - 1)
                    target in test_list
                    search_time = time.time() - start_time
                    
                    # Insert operation
                    start_time = time.time()
                    test_list.insert(size // 2, -1)
                    insert_time = time.time() - start_time
                    
                    return creation_time, search_time, insert_time
                
                def test_set_operations(size):
                    """Test set operations"""
                    # Create set
                    start_time = time.time()
                    test_set = set(range(size))
                    creation_time = time.time() - start_time
                    
                    # Search operation
                    start_time = time.time()
                    target = random.randint(0, size - 1)
                    target in test_set
                    search_time = time.time() - start_time
                    
                    # Insert operation
                    start_time = time.time()
                    test_set.add(-1)
                    insert_time = time.time() - start_time
                    
                    return creation_time, search_time, insert_time
                
                def test_dict_operations(size):
                    """Test dictionary operations"""
                    # Create dictionary
                    start_time = time.time()
                    test_dict = {i: i for i in range(size)}
                    creation_time = time.time() - start_time
                    
                    # Search operation
                    start_time = time.time()
                    target = random.randint(0, size - 1)
                    target in test_dict
                    search_time = time.time() - start_time
                    
                    # Insert operation
                    start_time = time.time()
                    test_dict[-1] = -1
                    insert_time = time.time() - start_time
                    
                    return creation_time, search_time, insert_time
                
                # Run tests
                data_structures = {
                    'List': test_list_operations,
                    'Set': test_set_operations,
                    'Dict': test_dict_operations
                }
                
                results = {name: {'creation': [], 'search': [], 'insert': []} 
                          for name in data_structures.keys()}
                
                for size in sizes:
                    for name, test_func in data_structures.items():
                        creation_time, search_time, insert_time = test_func(size)
                        results[name]['creation'].append(creation_time)
                        results[name]['search'].append(search_time)
                        results[name]['insert'].append(insert_time)
                
                # Visualize results
                fig, axes = plt.subplots(1, 3, figsize=(15, 5))
                operations = ['creation', 'search', 'insert']
                
                for i, operation in enumerate(operations):
                    ax = axes[i]
                    for name, times in results.items():
                        ax.plot(sizes, times[operation], 'o-', linewidth=2, markersize=6, label=name)
                    
                    ax.set_xlabel('Size')
                    ax.set_ylabel('Time (seconds)')
                    ax.set_title(f'{operation.capitalize()} Performance')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                    ax.set_yscale('log')
                
                plt.tight_layout()
                plt.show()
                
                return {
                    'sizes': sizes,
                    'results': results,
                    'data_structures': data_structures
                }
        
        # Demonstrate CS theory
        cs_theory = ComputerScienceTheoryFramework()
        cs_theory.explain_cs_foundations()
        
        # Demonstrate CS analysis
        cs_analyzer = CSTheoryAnalyzer()
        
        print("**Computer Science Theory Demonstration:**")
        
        # Demonstrate different CS concepts
        complexity_results = cs_analyzer.demonstrate_algorithm_complexity()
        data_structure_results = cs_analyzer.demonstrate_data_structures()
        
        return cs_analyzer, complexity_results, data_structure_results

# Demonstrate CS theory
cs_theory = ComputerScienceTheoryFramework()
cs_theory.explain_cs_foundations()

# Demonstrate CS analysis
cs_analyzer, complexity_results, data_structure_results = cs_theory.demonstrate_cs_concepts()
```

**T√†i li·ªáu tham kh·∫£o chuy√™n s√¢u:**
- **Mathematical Foundations**: [Mathematics for Machine Learning](https://mml-book.github.io/)
- **Linear Algebra**: [Linear Algebra Done Right](https://linear.axler.net/)
- **Optimization**: [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/)
- **Probability**: [Probability Theory: The Logic of Science](https://bayes.wustl.edu/etj/prob/book.pdf)
- **Algorithms**: [Introduction to Algorithms](https://mitpress.mit.edu/books/introduction-algorithms-third-edition)
- **Complexity Theory**: [Computational Complexity](https://theory.cs.princeton.edu/complexity/)

## üìã T·ªïng quan n·ªôi dung

## üìã T·ªïng quan n·ªôi dung

```mermaid
graph TD
    A[üöÄ Learning AI] --> B[üß† N·ªÅn t·∫£ng]
    A --> C[üìä Data Analyst]
    A --> D[ü§ñ Data Science & ML]
    A --> E[üìà Time Series]
    A --> F[üß† Deep Learning]
    A --> G[ü§ñ LLMs & ·ª®ng d·ª•ng]
    A --> H[üöÄ MLOps & S·∫£n xu·∫•t]
    
    B --> B1[Python N√¢ng cao]
    B --> B2[To√°n h·ªçc c∆° b·∫£n]
    B --> B3[SQL & Database]
    B --> B4[Tr·ª±c quan h√≥a d·ªØ li·ªáu]
    B --> B5[Git & CLI Tools]
    
    C --> C1[Quy tr√¨nh ph√¢n t√≠ch]
    C --> C2[EDA & Kh√°m ph√° d·ªØ li·ªáu]
    C --> C3[Tr·ª±c quan h√≥a & Dashboard]
    C --> C4[A/B Testing & Causal Inference]
    C --> C5[B√°o c√°o & Storytelling]
    
    D --> D1[Feature Engineering]
    D --> D2[Supervised Learning]
    D --> D3[Unsupervised Learning]
    D --> D4[Model Evaluation]
    D --> D5[Model Deployment]
    
    E --> E1[Exploratory Analysis]
    E --> E2[Statistical Models]
    E --> E3[Machine Learning]
    E --> E4[Deep Learning]
    E --> E5[Forecasting]
    
    F --> F1[Neural Network Theory]
    F --> F2[Optimization & Training]
    F --> F3[Architecture Design]
    F --> F4[Computer Vision]
    F --> F5[NLP & Transformers]
    
    G --> G1[Language Modeling Theory]
    G --> G2[Supervised Fine-tuning]
    G --> G3[Reinforcement Learning]
    G --> G4[RAG & Vector Search]
    G --> G5[Model Deployment]
    
    H --> H1[Model Development]
    H --> H2[Model Serving & Deployment]
    H --> H3[CI/CD & Pipelines]
    H --> H4[Monitoring & Observability]
    H --> H5[Security & Governance]
```

## üìö Danh s√°ch t√†i li·ªáu

### üß† **N·ªÅn t·∫£ng b·∫Øt bu·ªôc** - [01-foundations.md](./01-foundations.md)
> **M·ª•c ti√™u**: X√¢y d·ª±ng n·ªÅn t·∫£ng v·ªØng ch·∫Øc v·ªÅ Python, to√°n h·ªçc, SQL v√† c√¥ng c·ª• c·∫ßn thi·∫øt

**N·ªôi dung ch√≠nh**:
- **Python N√¢ng cao**: OOP, Packaging, Testing, List comprehensions
- **To√°n h·ªçc c∆° b·∫£n**: ƒê·∫°i s·ªë tuy·∫øn t√≠nh, X√°c su·∫•t & Th·ªëng k√™
- **SQL & Database**: JOINs, Window Functions, Query optimization
- **Tr·ª±c quan h√≥a d·ªØ li·ªáu**: Matplotlib, Seaborn, Plotly Dash
- **Git & CLI Tools**: Version control, Command line operations

### üìä **Data Analyst** - [02-data-analyst.md](./02-data-analyst.md)
> **M·ª•c ti√™u**: Tr·ªü th√†nh Data Analyst chuy√™n nghi·ªáp, c√≥ kh·∫£ nƒÉng ph√¢n t√≠ch d·ªØ li·ªáu v√† t·∫°o b√°o c√°o

**N·ªôi dung ch√≠nh**:
- **Quy tr√¨nh ph√¢n t√≠ch**: CRISP-DM framework, Business understanding
- **EDA & Kh√°m ph√° d·ªØ li·ªáu**: Missing values, Data types, Statistical analysis
- **Tr·ª±c quan h√≥a & Dashboard**: Interactive visualizations, Plotly Dash
- **A/B Testing & Causal Inference**: Experimental design, Statistical tests
- **B√°o c√°o & Storytelling**: Executive summary, Data storytelling

### ü§ñ **Data Science / Machine Learning** - [03-ds-ml.md](./03-ds-ml.md)
> **M·ª•c ti√™u**: Tr·ªü th√†nh Data Scientist, c√≥ kh·∫£ nƒÉng x√¢y d·ª±ng v√† tri·ªÉn khai m√¥ h√¨nh ML

**N·ªôi dung ch√≠nh**:
- **Feature Engineering**: Temporal features, Categorical encoding, Feature selection
- **Supervised Learning**: Linear models, Tree-based models, Regularization
- **Unsupervised Learning**: Clustering, Dimensionality reduction
- **Model Evaluation**: Cross-validation, Performance metrics, Hyperparameter tuning
- **Model Deployment**: Model serving, API development, Monitoring

### üìà **Time Series** - [04-time-series.md](./04-time-series.md)
> **M·ª•c ti√™u**: Tr·ªü th√†nh chuy√™n gia Time Series Analysis, c√≥ kh·∫£ nƒÉng x√¢y d·ª±ng m√¥ h√¨nh d·ª± b√°o

**N·ªôi dung ch√≠nh**:
- **Exploratory Analysis**: Data visualization, Seasonality detection, Trend analysis
- **Statistical Models**: ARIMA models, Exponential smoothing, SARIMA/SARIMAX
- **Machine Learning**: Feature engineering, Random Forest, Gradient Boosting
- **Deep Learning**: LSTM networks, GRU networks, Transformer models
- **Forecasting**: Point forecasts, Interval forecasts, Probabilistic forecasts

### üß† **Deep Learning** - [05-deep-learning.md](./05-deep-learning.md)
> **M·ª•c ti√™u**: Tr·ªü th√†nh chuy√™n gia Deep Learning, hi·ªÉu s√¢u v·ªÅ l√Ω thuy·∫øt m·∫°ng n∆°-ron

**N·ªôi dung ch√≠nh**:
- **Neural Network Theory**: Universal approximation, Backpropagation, Activation functions
- **Optimization & Training**: Initialization strategies, Batch normalization, Regularization
- **Architecture Design**: Feedforward networks, Convolutional networks, Recurrent networks
- **Computer Vision**: Image classification, Object detection, Image segmentation
- **NLP & Transformers**: Attention mechanisms, BERT, GPT architectures

### ü§ñ **LLMs & ·ª®ng d·ª•ng** - [06-llms.md](./06-llms.md)
> **M·ª•c ti√™u**: Tr·ªü th√†nh chuy√™n gia LLMs, c√≥ kh·∫£ nƒÉng fine-tune v√† tri·ªÉn khai ·ª©ng d·ª•ng AI

**N·ªôi dung ch√≠nh**:
- **Language Modeling Theory**: Autoregressive models, Scaling laws, Attention mechanisms
- **Supervised Fine-tuning**: Data preparation, Instruction tuning, LoRA & PEFT
- **Reinforcement Learning**: RLHF framework, PPO algorithm, Reward modeling
- **RAG & Vector Search**: Vector databases, Retrieval methods, Hybrid search
- **Model Deployment**: Model serving, API development, Cost optimization

### üöÄ **MLOps & S·∫£n xu·∫•t** - [07-mlops.md](./07-mlops.md)
> **M·ª•c ti√™u**: Tr·ªü th√†nh MLOps Engineer, c√≥ kh·∫£ nƒÉng x√¢y d·ª±ng h·ªá th·ªëng ML production

**N·ªôi dung ch√≠nh**:
- **Model Development**: Experiment tracking, Model registry, Data versioning
- **Model Serving & Deployment**: REST APIs, Model serving, Containerization
- **CI/CD & Pipelines**: GitOps, Automated testing, Deployment strategies
- **Monitoring & Observability**: Model performance, Data drift detection, Alerting
- **Security & Governance**: Access control, Data privacy, Compliance

### üèÜ **So s√°nh hi·ªáu nƒÉng** - [14-benchmarks.md](./14-benchmarks.md)
> **M·ª•c ti√™u**: Tr·ªü th√†nh chuy√™n gia benchmark v√† evaluation, ƒë√°nh gi√° to√†n di·ªán hi·ªáu su·∫•t m√¥ h√¨nh

**N·ªôi dung ch√≠nh**:
- **Performance Metrics**: Classification metrics, Regression metrics, Custom metrics
- **Speed & Efficiency**: Inference time, Training time, Throughput, Latency
- **Memory & Storage**: Model size, Memory usage, GPU memory, Storage requirements
- **Quality Assessment**: Accuracy, Robustness, Generalization, Fairness
- **Comparative Analysis**: Model comparison, Baseline analysis, Statistical testing

## üîß **T√†i li·ªáu b·ªï sung**

### üìñ **L√Ω thuy·∫øt chuy√™n s√¢u** - [deep-theory.md](./deep-theory.md)
> **M·ª•c ti√™u**: Cung c·∫•p ki·∫øn th·ª©c l√Ω thuy·∫øt s√¢u v·ªÅ c√°c kh√°i ni·ªám AI/ML

**N·ªôi dung**: Mathematical foundations, Theoretical concepts, Advanced algorithms

### üî• **PyTorch chuy√™n ƒë·ªÅ** - [15-pytorch.md](./15-pytorch.md)
> **M·ª•c ti√™u**: L√†m ch·ªß PyTorch t·ª´ c∆° b·∫£n ƒë·∫øn tri·ªÉn khai s·∫£n ph·∫©m (research ‚Üí production)

**N·ªôi dung ch√≠nh**:
- **C∆° b·∫£n**: Tensor, Autograd, Module/nn, Dataset/DataLoader, Training Loop
- **M·∫°ng n∆°-ron**: MLP, CNN, RNN/LSTM/GRU, Transformer kh√°i qu√°t
- **T·ªëi ∆∞u & Regularization**: AdamW, LR schedulers, Dropout, Weight Decay, Initialization
- **N√¢ng cao**: AMP (Mixed Precision), Distributed (DDP), Checkpointing, Logging
- **Hi·ªáu nƒÉng & Tri·ªÉn khai**: Profiling, TorchScript/JIT, ONNX, FastAPI/TorchServe/Triton

### üßÆ **To√°n h·ªçc n√¢ng cao** - [16-advanced-mathematics.md](./16-advanced-mathematics.md)
> **M·ª•c ti√™u**: Hi·ªÉu s√¢u c√°c kh√°i ni·ªám to√°n h·ªçc c·ªët l√µi trong AI/ML

**N·ªôi dung ch√≠nh**:
- **ƒê·∫°i s·ªë tuy·∫øn t√≠nh**: SVD, Eigenvalue theory, Tensor operations
- **Gi·∫£i t√≠ch & T·ªëi ∆∞u**: Convex optimization, Gradient methods, Lagrange multipliers
- **X√°c su·∫•t & Th·ªëng k√™**: Bayesian inference, Statistical learning theory, Information theory
- **L√Ω thuy·∫øt h·ªçc m√°y**: VC dimension, PAC learning, Generalization bounds

### üñºÔ∏è **Computer Vision n√¢ng cao** - [17-computer-vision.md](./17-computer-vision.md)
> **M·ª•c ti√™u**: Hi·ªÉu s√¢u v·ªÅ Computer Vision t·ª´ l√Ω thuy·∫øt ƒë·∫øn tri·ªÉn khai th·ª±c t·∫ø

**N·ªôi dung ch√≠nh**:
- **Image Processing**: Filters, Edge detection, Convolution operations
- **Deep Learning CV**: CNN architectures, Transfer learning, Attention mechanisms
- **Object Detection**: YOLO, R-CNN, Non-maximum suppression
- **Image Segmentation**: U-Net, Mask R-CNN, Dice loss
- **Feature Extraction**: SIFT, Deep features, Feature matching

### üîç **T√¨m ki·∫øm ng·ªØ nghƒ©a** - [search.md](./search.md)
> **M·ª•c ti√™u**: H·ªó tr·ª£ t√¨m ki·∫øm th√¥ng tin trong t√†i li·ªáu m·ªôt c√°ch th√¥ng minh

**T√≠nh nƒÉng**: Semantic search, Content discovery, Knowledge retrieval

### üé® **Real-ESRGAN x4** - [real-esrgan.md](./real-esrgan.md)
> **M·ª•c ti√™u**: H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng Real-ESRGAN ƒë·ªÉ upscale h√¨nh ·∫£nh

**·ª®ng d·ª•ng**: Image super-resolution, AI-powered upscaling

## üó∫Ô∏è **Mind Map t·ªïng quan** - [mindmap.md](./mindmap.md)
> **M·ª•c ti√™u**: Cung c·∫•p c√°i nh√¨n t·ªïng quan v·ªÅ to√†n b·ªô l·ªô tr√¨nh h·ªçc t·∫≠p

**T√≠nh nƒÉng**: Visual roadmap, Learning path, Topic relationships

## üìã **T√†i li·ªáu th·ª±c h√†nh**

### üöÄ **D·ª± √°n th·ª±c h√†nh** - [08-projects.md](./08-projects.md)
> **M·ª•c ti√™u**: √Åp d·ª•ng ki·∫øn th·ª©c v√†o c√°c d·ª± √°n th·ª±c t·∫ø

**N·ªôi dung**: End-to-end projects, Real-world applications, Portfolio building

### üìÖ **L·ªô tr√¨nh 12 tu·∫ßn** - [09-12-week.md](./09-12-week.md)
> **M·ª•c ti√™u**: Cung c·∫•p l·ªô tr√¨nh h·ªçc t·∫≠p c√≥ c·∫•u tr√∫c theo th·ªùi gian

**T√≠nh nƒÉng**: Weekly milestones, Learning objectives, Progress tracking

### üìä **Competency matrix** - [10-competency.md](./10-competency.md)
> **M·ª•c ti√™u**: ƒê√°nh gi√° v√† theo d√µi m·ª©c ƒë·ªô th√†nh th·∫°o c√°c k·ªπ nƒÉng

**T√≠nh nƒÉng**: Skill assessment, Progress tracking, Learning gaps identification

### üõ†Ô∏è **B·ªô c√¥ng c·ª• & m√¥i tr∆∞·ªùng** - [11-tooling.md](./11-tooling.md)
> **M·ª•c ti√™u**: H∆∞·ªõng d·∫´n setup m√¥i tr∆∞·ªùng ph√°t tri·ªÉn AI/ML

**N·ªôi dung**: Development environment, Tools setup, Best practices

### üèóÔ∏è **·ª®ng d·ª•ng theo Stack** - [12-stack.md](./12-stack.md)
> **M·ª•c ti√™u**: H∆∞·ªõng d·∫´n x√¢y d·ª±ng ·ª©ng d·ª•ng theo technology stack c·ª• th·ªÉ

**N·ªôi dung**: Full-stack development, Technology choices, Architecture patterns

### üîó **Interop** - [13-interop.md](./13-interop.md)
> **M·ª•c ti√™u**: H∆∞·ªõng d·∫´n t√≠ch h·ª£p v√† t∆∞∆°ng t√°c gi·ªØa c√°c h·ªá th·ªëng

**N·ªôi dung**: System integration, API design, Data exchange

## üéØ **C√°ch s·ª≠ d·ª•ng t√†i li·ªáu**

### üìñ **ƒê·ªçc tu·∫ßn t·ª±** (Khuy·∫øn ngh·ªã cho ng∆∞·ªùi m·ªõi)
1. B·∫Øt ƒë·∫ßu v·ªõi **01-foundations.md** ƒë·ªÉ x√¢y d·ª±ng n·ªÅn t·∫£ng
2. Ti·∫øp t·ª•c v·ªõi **02-data-analyst.md** ƒë·ªÉ h·ªçc ph√¢n t√≠ch d·ªØ li·ªáu
3. Chuy·ªÉn sang **03-ds-ml.md** ƒë·ªÉ h·ªçc machine learning
4. H·ªçc **04-time-series.md** n·∫øu quan t√¢m ƒë·∫øn d·ª± b√°o
5. Ti·∫øp t·ª•c v·ªõi **05-deep-learning.md** v√† **06-llms.md**
6. Cu·ªëi c√πng h·ªçc **07-mlops.md** ƒë·ªÉ tri·ªÉn khai production

### üîç **ƒê·ªçc theo ch·ªß ƒë·ªÅ** (Cho ng∆∞·ªùi c√≥ kinh nghi·ªám)
- **Computer Vision**: 01-foundations ‚Üí 05-deep-learning
- **NLP & LLMs**: 01-foundations ‚Üí 06-llms
- **MLOps & Production**: 01-foundations ‚Üí 07-mlops
- **Time Series**: 01-foundations ‚Üí 04-time-series

### üìö **T√†i li·ªáu tham kh·∫£o**
- **G·ªëc ƒë·∫ßy ƒë·ªß**: [../learning-ai.md](../learning-ai.md)
- **Mind Map**: [mindmap.md](./mindmap.md)
- **Deep Theory**: [deep-theory.md](./deep-theory.md)

### üß© Ghi ch√∫ 50/50
- To√†n b·ªô t√†i li·ªáu √°p d·ª•ng t·ª∑ l·ªá 50% l√Ω thuy·∫øt : 50% th·ª±c h√†nh
- M·ªói file ch√≠nh ƒë·ªÅu c√≥ b·∫£ng ph√¢n b·ªï 50/50 v√† rubric 100ƒë (30 l√Ω thuy·∫øt, 30 code, 30 k·∫øt qu·∫£, 10 b√°o c√°o)

## üöÄ **B·∫Øt ƒë·∫ßu h·ªçc t·∫≠p**

### ‚úÖ **Ki·ªÉm tra n·ªÅn t·∫£ng**
Tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu, h√£y ƒë·∫£m b·∫£o b·∫°n c√≥:
- Ki·∫øn th·ª©c Python c∆° b·∫£n
- Hi·ªÉu bi·∫øt v·ªÅ to√°n h·ªçc ph·ªï th√¥ng
- Kh·∫£ nƒÉng s·ª≠ d·ª•ng m√°y t√≠nh c∆° b·∫£n
- Tinh th·∫ßn h·ªçc t·∫≠p v√† ki√™n nh·∫´n

### üéØ **M·ª•c ti√™u h·ªçc t·∫≠p**
- **Ng·∫Øn h·∫°n (3-6 th√°ng)**: Ho√†n th√†nh foundations v√† data analyst
- **Trung h·∫°n (6-12 th√°ng)**: Ho√†n th√†nh machine learning v√† deep learning
- **D√†i h·∫°n (1-2 nƒÉm)**: Tr·ªü th√†nh AI/ML Engineer to√†n di·ªán

### üí° **L·ªùi khuy√™n**
- **Th·ª±c h√†nh th∆∞·ªùng xuy√™n**: Code m·ªói ng√†y, d√π ch·ªâ 30 ph√∫t
- **D·ª± √°n th·ª±c t·∫ø**: √Åp d·ª•ng ki·∫øn th·ª©c v√†o c√°c d·ª± √°n c·ª• th·ªÉ
- **C·ªông ƒë·ªìng**: Tham gia c√°c nh√≥m AI/ML ƒë·ªÉ h·ªçc h·ªèi v√† chia s·∫ª
- **C·∫≠p nh·∫≠t**: Theo d√µi xu h∆∞·ªõng m·ªõi trong AI/ML

---

## üìû **H·ªó tr·ª£ v√† li√™n h·ªá**

N·∫øu b·∫°n g·∫∑p kh√≥ khƒÉn ho·∫∑c c√≥ c√¢u h·ªèi:
- **GitHub Issues**: T·∫°o issue tr√™n repository n√†y
- **Discussions**: Tham gia th·∫£o lu·∫≠n trong GitHub Discussions
- **Contributions**: ƒê√≥ng g√≥p c·∫£i thi·ªán t√†i li·ªáu

---

*Ch√∫c b·∫°n th√†nh c√¥ng tr√™n con ƒë∆∞·ªùng h·ªçc t·∫≠p AI/ML! üéâ*

> **L∆∞u √Ω**: T√†i li·ªáu n√†y ƒë∆∞·ª£c c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n. H√£y ki·ªÉm tra phi√™n b·∫£n m·ªõi nh·∫•t ƒë·ªÉ c√≥ th√¥ng tin ch√≠nh x√°c nh·∫•t.


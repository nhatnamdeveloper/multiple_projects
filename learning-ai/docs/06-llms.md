# ğŸ¤– LLMs vÃ  á»©ng dá»¥ng - Large Language Models

> **Má»¥c tiÃªu**: Trá»Ÿ thÃ nh chuyÃªn gia LLMs, hiá»ƒu sÃ¢u vá» kiáº¿n trÃºc Transformer, fine-tuning vÃ  triá»ƒn khai cÃ¡c á»©ng dá»¥ng AI thá»±c táº¿

## ğŸ“š **1. Báº£ng kÃ½ hiá»‡u (Notation)**

### **Language Modeling:**
- **Vocabulary**: $\mathcal{V} = \{w_1, w_2, \ldots, w_V\}$ (táº­p tá»« vá»±ng)
- **Sequence**: $\mathbf{x} = (x_1, x_2, \ldots, x_T)$ (chuá»—i tokens)
- **Context**: $\mathbf{x}_{<t} = (x_1, x_2, \ldots, x_{t-1})$ (context trÆ°á»›c token $t$)
- **Probability**: $P(x_t | \mathbf{x}_{<t})$ (xÃ¡c suáº¥t token $t$ given context)

### **Transformer Architecture:**
- **Input embedding**: $\mathbf{E} \in \mathbb{R}^{V \times d}$ (embedding matrix)
- **Positional encoding**: $\mathbf{P} \in \mathbb{R}^{T \times d}$ (positional encoding)
- **Query/Key/Value**: $\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{T \times d_k}$
- **Attention weights**: $\mathbf{A} = \text{softmax}(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}})$

### **Attention Mechanism:**
- **Scaled Dot-Product**: $\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}})\mathbf{V}$
- **Multi-Head**: $\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}^O$
- **Head**: $\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)$

### **Training & Loss:**
- **Cross-entropy loss**: $\mathcal{L} = -\sum_{t=1}^T \log P(x_t | \mathbf{x}_{<t})$
- **Perplexity**: $\text{PP} = \exp(\frac{1}{T}\sum_{t=1}^T \log P(x_t | \mathbf{x}_{<t}))$
- **Learning rate**: $\alpha$ (step size)
- **Batch size**: $B$ (sá»‘ sequences per update)

### **Fine-tuning:**
- **LoRA**: $\mathbf{W} = \mathbf{W}_0 + \mathbf{B}\mathbf{A}$ vá»›i $\mathbf{B} \in \mathbb{R}^{d \times r}, \mathbf{A} \in \mathbb{R}^{r \times d}$
- **Adapter**: $\mathbf{h}' = \mathbf{h} + \text{Adapter}(\mathbf{h})$
- **Prefix tuning**: $\mathbf{h}' = \text{LM}([\mathbf{P}_1, \ldots, \mathbf{P}_k, \mathbf{x}])$

## ğŸ“– **2. Glossary (Äá»‹nh nghÄ©a cá»‘t lÃµi)**

### **Language Modeling:**
- **Autoregressive**: Tá»± há»“i quy - dá»± Ä‘oÃ¡n token tiáº¿p theo dá»±a trÃªn context
- **Perplexity**: Äá»™ bá»‘i rá»‘i - measure cá»§a model uncertainty
- **Tokenization**: PhÃ¢n Ä‘oáº¡n - chuyá»ƒn text thÃ nh tokens
- **Vocabulary**: Tá»« vá»±ng - táº­p há»£p táº¥t cáº£ tokens cÃ³ thá»ƒ

### **Kiáº¿n trÃºc Transformer (Transformer Architecture)**

Kiáº¿n trÃºc Transformer, Ä‘Æ°á»£c giá»›i thiá»‡u trong bÃ i bÃ¡o "Attention Is All You Need", Ä‘Ã£ táº¡o ra má»™t cuá»™c cÃ¡ch máº¡ng trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn. Cá»‘t lÃµi cá»§a nÃ³ lÃ  cÆ¡ cháº¿ **Self-Attention**, cho phÃ©p mÃ´ hÃ¬nh cÃ¢n nháº¯c táº§m quan trá»ng cá»§a cÃ¡c tá»« khÃ¡c nhau trong má»™t cÃ¢u khi xá»­ lÃ½ má»™t tá»« cá»¥ thá»ƒ.

Má»™t khá»‘i Transformer (Transformer Block) tiÃªu chuáº©n bao gá»“m cÃ¡c thÃ nh pháº§n chÃ­nh sau:

1.  **Multi-Head Self-Attention (Tá»± chÃº Ã½ Ä‘a Ä‘áº§u)**:
    *   **Nhiá»‡m vá»¥**: "NhÃ¬n" vÃ o cÃ¡c tá»« khÃ¡c trong cÃ¢u Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n ngá»¯ cáº£nh cá»§a má»™t tá»«.
    *   **TÆ° duy trá»±c quan**: Khi báº¡n Ä‘á»c cÃ¢u "The cat sat on the mat, it was asleep", Ä‘á»ƒ hiá»ƒu "it" Ã¡m chá»‰ con gÃ¬, báº¡n cáº§n "chÃº Ã½" Ä‘áº¿n "cat". Self-attention lÃ m Ä‘iá»u tÆ°Æ¡ng tá»±. "Multi-head" cÃ³ nghÄ©a lÃ  mÃ´ hÃ¬nh lÃ m Ä‘iá»u nÃ y tá»« nhiá»u "gÃ³c nhÃ¬n" khÃ¡c nhau cÃ¹ng má»™t lÃºc (vÃ­ dá»¥: má»™t "Ä‘áº§u" cÃ³ thá»ƒ táº­p trung vÃ o quan há»‡ cÃº phÃ¡p, má»™t "Ä‘áº§u" khÃ¡c táº­p trung vÃ o quan há»‡ ngá»¯ nghÄ©a).
    *   ÄÃ¢y lÃ  thÃ nh pháº§n giÃºp Transformer xá»­ lÃ½ cÃ¡c má»‘i quan há»‡ xa trong cÃ¢u, má»™t Ä‘iá»ƒm yáº¿u cá»§a cÃ¡c mÃ´ hÃ¬nh RNN/LSTM trÆ°á»›c Ä‘Ã³.

2.  **Add & Norm (Residual Connection vÃ  Layer Normalization)**:
    *   **Nhiá»‡m vá»¥**: GiÃºp viá»‡c huáº¥n luyá»‡n cÃ¡c máº¡ng ráº¥t sÃ¢u trá»Ÿ nÃªn kháº£ thi.
    *   **Add (Residual Connection)**: Táº¡o má»™t "Ä‘Æ°á»ng táº¯t" cho gradient, cho phÃ©p nÃ³ cháº£y ngÆ°á»£c qua cÃ¡c layer má»™t cÃ¡ch dá»… dÃ ng hÆ¡n, trÃ¡nh hiá»‡n tÆ°á»£ng vanishing gradients. Vá» cÆ¡ báº£n, output cá»§a má»™t sub-layer lÃ  `x + SubLayer(x)`.
    *   **Norm (Layer Normalization)**: Chuáº©n hÃ³a output cá»§a má»—i layer Ä‘á»ƒ giá»¯ cho phÃ¢n phá»‘i dá»¯ liá»‡u á»•n Ä‘á»‹nh trong suá»‘t quÃ¡ trÃ¬nh training, giÃºp tÄƒng tá»‘c Ä‘á»™ vÃ  sá»± á»•n Ä‘á»‹nh cá»§a viá»‡c há»c.

3.  **Feed-Forward Network (Máº¡ng truyá»n tháº³ng)**:
    *   **Nhiá»‡m vá»¥**: Xá»­ lÃ½ vÃ  "tiÃªu hÃ³a" thÃ´ng tin Ä‘Ã£ Ä‘Æ°á»£c tá»•ng há»£p tá»« cÆ¡ cháº¿ attention.
    *   **TÆ° duy trá»±c quan**: Sau khi attention Ä‘Ã£ thu tháº­p táº¥t cáº£ ngá»¯ cáº£nh cáº§n thiáº¿t cho má»—i tá»«, Feed-Forward network lÃ  má»™t máº¡ng nÆ¡-ron Ä‘Æ¡n giáº£n (gá»“m 2 lá»›p tuyáº¿n tÃ­nh) hoáº¡t Ä‘á»™ng trÃªn tá»«ng tá»« má»™t cÃ¡ch Ä‘á»™c láº­p Ä‘á»ƒ xá»­ lÃ½ thÃ´ng tin Ä‘Ã³ vÃ  táº¡o ra má»™t biá»ƒu diá»…n má»›i, phong phÃº hÆ¡n.

Má»™t khá»‘i Transformer Ä‘áº§y Ä‘á»§ sáº½ trÃ´ng nhÆ° tháº¿ nÃ y: `Input -> Multi-Head Attention -> Add & Norm -> Feed-Forward Network -> Add & Norm -> Output`. CÃ¡c mÃ´ hÃ¬nh LLM lá»›n chá»‰ Ä‘Æ¡n giáº£n lÃ  xáº¿p chá»“ng ráº¥t nhiá»u cÃ¡c khá»‘i nÃ y lÃªn nhau.

- **Positional Encoding**: MÃ£ hÃ³a vá»‹ trÃ­ - VÃ¬ self-attention khÃ´ng cÃ³ khÃ¡i niá»‡m vá» thá»© tá»± tá»«, ta cáº§n "thÃªm" thÃ´ng tin vá»‹ trÃ­ vÃ o cÃ¡c embedding Ä‘áº§u vÃ o.
- **Self-Attention**: Tá»± chÃº Ã½ - CÆ¡ cháº¿ cho phÃ©p cÃ¡c token trong má»™t chuá»—i tÆ°Æ¡ng tÃ¡c vÃ  cÃ¢n nháº¯c táº§m quan trá»ng cá»§a nhau.
- **Feed-Forward**: Máº¡ng truyá»n tháº³ng - Má»™t máº¡ng nÆ¡-ron nhá» Ä‘Æ°á»£c Ã¡p dá»¥ng cho tá»«ng vá»‹ trÃ­ má»™t cÃ¡ch Ä‘á»™c láº­p.
- **Layer Normalization**: Chuáº©n hÃ³a lá»›p - á»”n Ä‘á»‹nh hÃ³a quÃ¡ trÃ¬nh huáº¥n luyá»‡n báº±ng cÃ¡ch chuáº©n hÃ³a cÃ¡c activation.

### **Ká»¹ thuáº­t Huáº¥n luyá»‡n vÃ  Tinh chá»‰nh (Training & Fine-tuning)**

- **Pre-training (Tiá»n huáº¥n luyá»‡n)**: Giai Ä‘oáº¡n mÃ´ hÃ¬nh há»c cÃ¡c kiáº¿n thá»©c tá»•ng quÃ¡t vá» ngÃ´n ngá»¯ tá»« má»™t kho dá»¯ liá»‡u vÄƒn báº£n khá»•ng lá»“ (vÃ­ dá»¥: toÃ n bá»™ Internet). ÄÃ¢y lÃ  bÆ°á»›c tá»‘n kÃ©m nháº¥t, thÆ°á»ng chá»‰ Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi cÃ¡c cÃ´ng ty lá»›n.
- **Fine-tuning (Tinh chá»‰nh)**: QuÃ¡ trÃ¬nh Ä‘iá»u chá»‰nh má»™t mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c pre-trained Ä‘á»ƒ nÃ³ hoáº¡t Ä‘á»™ng tá»‘t trÃªn má»™t tÃ¡c vá»¥ hoáº·c má»™t bá»™ dá»¯ liá»‡u cá»¥ thá»ƒ.
- **Instruction Tuning (Tinh chá»‰nh theo chá»‰ dáº«n)**: Má»™t dáº¡ng fine-tuning Ä‘áº·c biá»‡t, trong Ä‘Ã³ mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t bá»™ dá»¯ liá»‡u gá»“m cÃ¡c cáº·p `(chá»‰ dáº«n, cÃ¢u tráº£ lá»i mong muá»‘n)`. Ká»¹ thuáº­t nÃ y giÃºp mÃ´ hÃ¬nh há»c cÃ¡ch "lÃ m theo má»‡nh lá»‡nh" vÃ  lÃ  ná»n táº£ng cho cÃ¡c chatbot nhÆ° ChatGPT.

#### Parameter-Efficient Fine-Tuning (PEFT) - Tinh chá»‰nh hiá»‡u quáº£ tham sá»‘

-   **Váº¥n Ä‘á»**: Fine-tuning toÃ n bá»™ má»™t mÃ´ hÃ¬nh cÃ³ hÃ ng tá»· tham sá»‘ (full fine-tuning) Ä‘Ã²i há»i ráº¥t nhiá»u tÃ i nguyÃªn pháº§n cá»©ng (GPU memory) vÃ  thá»i gian.
-   **Giáº£i phÃ¡p (PEFT)**: Thay vÃ¬ cáº­p nháº­t táº¥t cáº£ cÃ¡c trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh, cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT chá»‰ cáº­p nháº­t má»™t pháº§n nhá» cÃ¡c tham sá»‘, hoáº·c thÃªm vÃ o má»™t sá»‘ lÆ°á»£ng nhá» cÃ¡c tham sá»‘ má»›i cÃ³ thá»ƒ huáº¥n luyá»‡n.
-   **Lá»£i Ã­ch**:
    -   Giáº£m Ä‘Ã¡ng ká»ƒ yÃªu cáº§u vá» bá»™ nhá»› vÃ  tÃ­nh toÃ¡n.
    -   Giáº£m nguy cÆ¡ "catastrophic forgetting" (mÃ´ hÃ¬nh quÃªn máº¥t kiáº¿n thá»©c Ä‘Ã£ há»c trong pre-training).
    -   Dá»… dÃ ng quáº£n lÃ½ nhiá»u phiÃªn báº£n fine-tune cho cÃ¡c tÃ¡c vá»¥ khÃ¡c nhau.

#### LoRA (Low-Rank Adaptation) - Má»™t phÆ°Æ¡ng phÃ¡p PEFT phá»• biáº¿n

-   **TÆ° tÆ°á»Ÿng**: Sá»± thay Ä‘á»•i trá»ng sá»‘ cá»§a má»™t mÃ´ hÃ¬nh lá»›n trong quÃ¡ trÃ¬nh fine-tuning cÃ³ thá»ƒ Ä‘Æ°á»£c xáº¥p xá»‰ báº±ng má»™t ma tráº­n cÃ³ **háº¡ng tháº¥p (low-rank)**.
-   **CÃ¡ch hoáº¡t Ä‘á»™ng**:
    1.  Giá»¯ nguyÃªn ma tráº­n trá»ng sá»‘ lá»›n `W` cá»§a mÃ´ hÃ¬nh gá»‘c (frozen).
    2.  Thay vÃ¬ cáº­p nháº­t `W`, LoRA há»c hai ma tráº­n nhá» hÆ¡n nhiá»u lÃ  `A` vÃ  `B`.
    3.  Sá»± thay Ä‘á»•i cá»§a trá»ng sá»‘ (`Î”W`) Ä‘Æ°á»£c tÃ­nh báº±ng tÃ­ch cá»§a hai ma tráº­n nÃ y: `Î”W = B @ A`.
    4.  Khi inference, káº¿t quáº£ Ä‘Æ°á»£c tÃ­nh báº±ng: `output = W @ x + (B @ A) @ x`.
-   **VÃ­ dá»¥ tÆ°Æ¡ng tá»±**: HÃ£y tÆ°á»Ÿng tÆ°á»£ng `W` lÃ  má»™t bá»©c áº£nh chÃ¢n dung cÃ³ Ä‘á»™ phÃ¢n giáº£i cá»±c cao. Full fine-tuning giá»‘ng nhÆ° viá»‡c váº½ láº¡i toÃ n bá»™ bá»©c áº£nh. LoRA giá»‘ng nhÆ° viá»‡c chá»‰ váº½ nhá»¯ng nÃ©t thay Ä‘á»•i (náº¿p nhÄƒn, cáº£m xÃºc) trÃªn má»™t lá»›p giáº¥y má» (layer) riÃªng, sau Ä‘Ã³ Ä‘áº·t lá»›p giáº¥y má» Ä‘Ã³ lÃªn trÃªn bá»©c áº£nh gá»‘c. ChÃºng ta chá»‰ cáº§n lÆ°u láº¡i lá»›p giáº¥y má» thay vÃ¬ má»™t bá»©c áº£nh má»›i hoÃ n toÃ n.
-   `r` (rank): LÃ  má»™t hyperparameter quan trá»ng trong LoRA, quyáº¿t Ä‘á»‹nh kÃ­ch thÆ°á»›c cá»§a hai ma tráº­n `A` vÃ  `B`. `r` cÃ ng nhá», sá»‘ lÆ°á»£ng tham sá»‘ cáº§n huáº¥n luyá»‡n cÃ ng Ã­t.

#### RLHF (Reinforcement Learning from Human Feedback) - Há»c tÄƒng cÆ°á»ng tá»« pháº£n há»“i cá»§a con ngÆ°á»i

ÄÃ¢y lÃ  quy trÃ¬nh 3 bÆ°á»›c giÃºp "cÄƒn chá»‰nh" (align) hÃ nh vi cá»§a LLM vá»›i sá»Ÿ thÃ­ch vÃ  mong muá»‘n cá»§a con ngÆ°á»i, lÃ m cho nÃ³ trá»Ÿ nÃªn há»¯u Ã­ch vÃ  an toÃ n hÆ¡n.

1.  **BÆ°á»›c 1: Supervised Fine-Tuning (SFT)**
    *   Thu tháº­p má»™t bá»™ dá»¯ liá»‡u cháº¥t lÆ°á»£ng cao gá»“m cÃ¡c cáº·p `(prompt, response)` do con ngÆ°á»i viáº¿t.
    *   Fine-tune mÃ´ hÃ¬nh ngÃ´n ngá»¯ cÆ¡ sá»Ÿ (base model) trÃªn bá»™ dá»¯ liá»‡u nÃ y.
    *   **Káº¿t quáº£**: Má»™t mÃ´ hÃ¬nh SFT cÃ³ kháº£ nÄƒng tráº£ lá»i cÃ¡c prompt theo phong cÃ¡ch mong muá»‘n.

2.  **BÆ°á»›c 2: Huáº¥n luyá»‡n Reward Model (MÃ´ hÃ¬nh pháº§n thÆ°á»Ÿng)**
    *   Cho mÃ´ hÃ¬nh SFT táº¡o ra nhiá»u cÃ¢u tráº£ lá»i khÃ¡c nhau cho cÃ¹ng má»™t prompt.
    *   Con ngÆ°á»i (labelers) sáº½ xáº¿p háº¡ng cÃ¡c cÃ¢u tráº£ lá»i nÃ y tá»« tá»‘t nháº¥t Ä‘áº¿n tá»‡ nháº¥t.
    *   DÃ¹ng dá»¯ liá»‡u xáº¿p háº¡ng nÃ y Ä‘á»ƒ huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh riÃªng, gá»i lÃ  **Reward Model**.
    *   **Nhiá»‡m vá»¥ cá»§a Reward Model**: Nháº­n vÃ o má»™t cáº·p `(prompt, response)` vÃ  tráº£ vá» má»™t Ä‘iá»ƒm sá»‘ (reward) thá»ƒ hiá»‡n má»©c Ä‘á»™ "tá»‘t" cá»§a cÃ¢u tráº£ lá»i theo tiÃªu chÃ­ cá»§a con ngÆ°á»i.

3.  **BÆ°á»›c 3: Tá»‘i Æ°u hÃ³a báº±ng Reinforcement Learning (RL)**
    *   Sá»­ dá»¥ng thuáº­t toÃ¡n RL (thÆ°á»ng lÃ  **PPO - Proximal Policy Optimization**) Ä‘á»ƒ tiáº¿p tá»¥c fine-tune mÃ´ hÃ¬nh SFT.
    *   Trong vÃ²ng láº·p nÃ y:
        *   MÃ´ hÃ¬nh nháº­n má»™t prompt vÃ  táº¡o ra má»™t cÃ¢u tráº£ lá»i.
        *   **Reward Model** sáº½ "cháº¥m Ä‘iá»ƒm" cÃ¢u tráº£ lá»i Ä‘Ã³.
        *   Äiá»ƒm sá»‘ nÃ y Ä‘Æ°á»£c dÃ¹ng lÃ m "pháº§n thÆ°á»Ÿng" Ä‘á»ƒ cáº­p nháº­t cÃ¡c trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh LLM.
    *   **Má»¥c tiÃªu**: Tá»‘i Ä‘a hÃ³a Ä‘iá»ƒm thÆ°á»Ÿng tá»« Reward Model, tá»« Ä‘Ã³ khiáº¿n LLM táº¡o ra cÃ¡c cÃ¢u tráº£ lá»i mÃ  con ngÆ°á»i Æ°a thÃ­ch hÆ¡n.

**Káº¿t quáº£ cuá»‘i cÃ¹ng**: Má»™t mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c "cÄƒn chá»‰nh", vá»«a cÃ³ kiáº¿n thá»©c tá»« pre-training, vá»«a cÃ³ kháº£ nÄƒng tuÃ¢n theo chá»‰ dáº«n tá»« SFT, vÃ  quan trá»ng nháº¥t lÃ  hÃ nh xá»­ theo cÃ¡ch con ngÆ°á»i mong Ä‘á»£i nhá» RLHF.

### **Deployment:**
- **Model Serving**: Phá»¥c vá»¥ mÃ´ hÃ¬nh - serve model cho inference
- **Quantization**: LÆ°á»£ng tá»­ hÃ³a - reduce model precision Ä‘á»ƒ save memory
- **Pruning**: Cáº¯t tá»‰a - remove unnecessary weights
- **Distillation**: ChÆ°ng cáº¥t - transfer knowledge tá»« large model sang small model

## ğŸ“ **3. Tháº» thuáº­t toÃ¡n - Self-Attention**

### **1. BÃ i toÃ¡n & dá»¯ liá»‡u:**
- **BÃ i toÃ¡n**: Compute attention weights cho sequence Ä‘á»ƒ capture dependencies
- **Dá»¯ liá»‡u**: Input sequence $\mathbf{X} \in \mathbb{R}^{T \times d}$ (sequence length $T$, dimension $d$)
- **á»¨ng dá»¥ng**: Language modeling, sequence modeling, transformer architecture

### **2. MÃ´ hÃ¬nh & cÃ´ng thá»©c:**
**Self-Attention:**
$$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}})\mathbf{V}$$

**Query/Key/Value Computation:**
$$\mathbf{Q} = \mathbf{X}\mathbf{W}_Q, \quad \mathbf{K} = \mathbf{X}\mathbf{W}_K, \quad \mathbf{V} = \mathbf{X}\mathbf{W}_V$$

**Multi-Head Attention:**
$$\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}^O$$

Trong Ä‘Ã³:
- $\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V \in \mathbb{R}^{d \times d_k}$: Query/Key/Value projection matrices
- $\mathbf{W}^O \in \mathbb{R}^{hd_k \times d}$: Output projection matrix
- $h$: Number of attention heads
- $d_k = d/h$: Dimension per head

### **3. Loss & má»¥c tiÃªu:**
- **Má»¥c tiÃªu**: Capture long-range dependencies trong sequence
- **Loss**: KhÃ´ng cÃ³ loss riÃªng, lÃ  component cá»§a transformer

### **4. Tá»‘i Æ°u hoÃ¡ & cáº­p nháº­t:**
- **Algorithm**: Matrix multiplication vÃ  softmax
- **Cáº­p nháº­t**: KhÃ´ng cÃ³ parameter learning riÃªng

### **5. Hyperparams:**
- **Number of heads**: $h$ (thÆ°á»ng 8, 16, 32)
- **Head dimension**: $d_k = d/h$
- **Sequence length**: $T$ (context window size)

### **6. Äá»™ phá»©c táº¡p:**
- **Time**: $O(T^2 \times d)$ cho attention computation
- **Space**: $O(T^2)$ cho attention matrix storage

### **7. Metrics Ä‘Ã¡nh giÃ¡:**
- **Attention weights**: Distribution vÃ  sparsity
- **Gradient flow**: Vanishing/exploding gradients
- **Memory usage**: Memory consumption
- **Computation speed**: Time per forward pass

### **8. Æ¯u / NhÆ°á»£c:**
**Æ¯u Ä‘iá»ƒm:**
- Captures long-range dependencies
- Parallelizable computation
- Interpretable attention weights
- Scalable architecture

**NhÆ°á»£c Ä‘iá»ƒm:**
- Quadratic complexity $O(T^2)$
- Memory intensive
- May not capture all dependencies
- Position information needed

### **9. Báº«y & máº¹o:**
- **Báº«y**: Quadratic complexity â†’ memory issues vá»›i long sequences
- **Báº«y**: Attention collapse â†’ all tokens attend to same position
- **Máº¹o**: Use relative positional encoding
- **Máº¹o**: Implement attention caching cho inference

### **10. Pseudocode:**
```python
def self_attention(X, W_Q, W_K, W_V, W_O, d_k):
    # Compute Q, K, V
    Q = X @ W_Q
    K = X @ W_K
    V = X @ W_V
    
    # Compute attention scores
    scores = Q @ K.T / np.sqrt(d_k)
    attention_weights = softmax(scores)
    
    # Apply attention to values
    output = attention_weights @ V
    
    # Apply output projection
    output = output @ W_O
    
    return output, attention_weights

def multi_head_attention(X, num_heads, d_model):
    d_k = d_model // num_heads
    outputs = []
    attention_weights = []
    
    for head in range(num_heads):
        # Get head-specific parameters
        W_Q = get_head_weights('Q', head, d_k)
        W_K = get_head_weights('K', head, d_k)
        W_V = get_head_weights('V', head, d_k)
        W_O = get_head_weights('O', head, d_k)
        
        # Compute attention for this head
        output, weights = self_attention(X, W_Q, W_K, W_V, W_O, d_k)
        outputs.append(output)
        attention_weights.append(weights)
    
    # Concatenate outputs
    concatenated = np.concatenate(outputs, axis=-1)
    
    return concatenated, attention_weights
```

### **11. Code máº«u:**
```python
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    """Self-Attention Implementation"""
    
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Linear projections
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.size()
        
        # Linear projections and reshape
        Q = self.W_Q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_K(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_V(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        # Compute attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)
        
        # Apply mask if provided
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Apply softmax
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        output = torch.matmul(attention_weights, V)
        
        # Reshape and apply output projection
        output = output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        output = self.W_O(output)
        
        return output, attention_weights

class TransformerBlock(nn.Module):
    """Transformer Block with Self-Attention"""
    
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = SelfAttention(d_model, num_heads, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
        
    def forward(self, x, mask=None):
        # Self-attention with residual connection
        attn_output, attention_weights = self.attention(x, mask)
        x = self.norm1(x + attn_output)
        
        # Feed-forward with residual connection
        ff_output = self.feed_forward(x)
        x = self.norm2(x + ff_output)
        
        return x, attention_weights

class SimpleTransformer(nn.Module):
    """Simple Transformer for Language Modeling"""
    
    def __init__(self, vocab_size, d_model, num_heads, num_layers, max_seq_len, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        # Embeddings
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        
        # Transformer blocks
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_model * 4, dropout)
            for _ in range(num_layers)
        ])
        
        # Output projection
        self.output_projection = nn.Linear(d_model, vocab_size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        batch_size, seq_len = x.size()
        
        # Create position indices
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)
        
        # Get embeddings
        token_emb = self.token_embedding(x)
        pos_emb = self.position_embedding(positions)
        
        # Combine embeddings
        x = token_emb + pos_emb
        x = self.dropout(x)
        
        # Pass through transformer blocks
        attention_weights = []
        for block in self.transformer_blocks:
            x, attn_weights = block(x, mask)
            attention_weights.append(attn_weights)
        
        # Output projection
        output = self.output_projection(x)
        
        return output, attention_weights

def demonstrate_attention():
    """Demonstrate self-attention mechanism"""
    print("=== Self-Attention Demonstration ===\n")
    
    # Model parameters
    vocab_size = 1000
    d_model = 128
    num_heads = 8
    num_layers = 4
    max_seq_len = 50
    
    # Create model
    model = SimpleTransformer(vocab_size, d_model, num_heads, num_layers, max_seq_len)
    
    # Create sample input
    batch_size = 2
    seq_len = 10
    x = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    print(f"Input shape: {x.shape}")
    print(f"Vocabulary size: {vocab_size}")
    print(f"Model dimension: {d_model}")
    print(f"Number of heads: {num_heads}")
    
    # Forward pass
    output, attention_weights = model(x)
    
    print(f"\nOutput shape: {output.shape}")
    print(f"Number of attention layers: {len(attention_weights)}")
    print(f"Attention weights shape per layer: {attention_weights[0].shape}")
    
    # Analyze attention weights
    print("\n--- Attention Analysis ---")
    
    # Get attention weights from first layer, first head
    first_layer_attention = attention_weights[0][0, 0]  # [seq_len, seq_len]
    
    print(f"Attention matrix shape: {first_layer_attention.shape}")
    print(f"Attention matrix sum per row: {first_layer_attention.sum(dim=-1)}")
    
    # Visualize attention weights
    plt.figure(figsize=(10, 8))
    plt.imshow(first_layer_attention.detach().numpy(), cmap='Blues')
    plt.colorbar()
    plt.title('Self-Attention Weights (Layer 1, Head 1)')
    plt.xlabel('Key Position')
    plt.ylabel('Query Position')
    plt.show()
    
    # Compute perplexity
    logits = output.view(-1, vocab_size)
    targets = x.view(-1)
    loss = F.cross_entropy(logits, targets)
    perplexity = torch.exp(loss)
    
    print(f"\nLoss: {loss.item():.4f}")
    print(f"Perplexity: {perplexity.item():.4f}")
    
    return model, attention_weights

def analyze_attention_patterns(attention_weights, layer_idx=0, head_idx=0):
    """Analyze attention patterns"""
    attention = attention_weights[layer_idx][0, head_idx]  # [seq_len, seq_len]
    
    plt.figure(figsize=(15, 5))
    
    # Plot attention matrix
    plt.subplot(1, 3, 1)
    plt.imshow(attention.detach().numpy(), cmap='Blues')
    plt.title(f'Attention Matrix (Layer {layer_idx+1}, Head {head_idx+1})')
    plt.xlabel('Key Position')
    plt.ylabel('Query Position')
    
    # Plot attention distribution
    plt.subplot(1, 3, 2)
    attention_flat = attention.detach().numpy().flatten()
    plt.hist(attention_flat, bins=50, alpha=0.7)
    plt.title('Attention Weight Distribution')
    plt.xlabel('Attention Weight')
    plt.ylabel('Frequency')
    
    # Plot average attention per position
    plt.subplot(1, 3, 3)
    avg_attention = attention.mean(dim=0).detach().numpy()
    plt.bar(range(len(avg_attention)), avg_attention)
    plt.title('Average Attention per Position')
    plt.xlabel('Position')
    plt.ylabel('Average Attention')
    
    plt.tight_layout()
    plt.show()
```

### **12. Checklist kiá»ƒm tra nhanh:**
- [ ] Attention weights cÃ³ sum to 1?
- [ ] Multi-head cÃ³ capture different patterns?
- [ ] Positional encoding cÃ³ work properly?
- [ ] Memory usage cÃ³ acceptable?
- [ ] Attention cÃ³ capture dependencies?

---

# ğŸ¤– LLMs vÃ  á»©ng dá»¥ng - Large Language Models

> **Má»¥c tiÃªu**: Trá»Ÿ thÃ nh chuyÃªn gia LLMs, hiá»ƒu sÃ¢u vá» kiáº¿n trÃºc Transformer, fine-tuning vÃ  triá»ƒn khai cÃ¡c á»©ng dá»¥ng AI thá»±c táº¿

## ğŸ“‹ Tá»•ng quan ná»™i dung

```mermaid
graph TD
    A[ğŸ¤– Large Language Models] --> B[ğŸ”¬ Language Modeling Theory]
    A --> C[ğŸ¯ Supervised Fine-tuning]
    A --> D[ğŸ”„ Reinforcement Learning]
    A --> E[ğŸ” RAG & Vector Search]
    A --> F[ğŸš€ Model Deployment]
    A --> G[âš¡ Optimization & Quantization]
    
    B --> B1[Autoregressive Models]
    B --> B2[Scaling Laws]
    B --> B3[Attention Mechanisms]
    B --> B4[Transformer Architecture]
    
    C --> C1[Data Preparation]
    C --> C2[Instruction Tuning]
    C --> C3[LoRA & PEFT]
    C --> C4[Evaluation Metrics]
    
    D --> D1[RLHF Framework]
    D --> D2[PPO Algorithm]
    D --> D3[Reward Modeling]
    D --> D4[Human Feedback]
    
    E --> E1[Vector Databases]
    E --> E2[Retrieval Methods]
    E --> E3[Reranking]
    E --> E4[Hybrid Search]
    
    F --> F1[Model Serving]
    F --> F2[API Development]
    F --> F3[Monitoring & Scaling]
    F --> F4[Cost Optimization]
    
    G --> G1[Quantization 8/4-bit]
    G --> G2[Pruning & Distillation]
    G --> G3[Model Compression]
    G --> G4[Hardware Optimization]
```

![LLMs Architecture](assets/llms-architecture.svg)

![LLMs Architecture PNG](assets/llms-architecture.png)

**ğŸ“ [Xem file PNG trá»±c tiáº¿p](assets/llms-architecture.png)**

**ğŸ“ [Xem file PNG trá»±c tiáº¿p](assets/llms-architecture.png)**

**ğŸ“ [Xem file PNG trá»±c tiáº¿p](assets/llms-architecture.png)**

## ğŸ§© ChÆ°Æ¡ng trÃ¬nh 50/50 (LÃ½ thuyáº¿t : Thá»±c hÃ nh)

- Má»¥c tiÃªu: 50% lÃ½ thuyáº¿t (nguyÃªn lÃ½ mÃ´ hÃ¬nh hoÃ¡ ngÃ´n ngá»¯, Attention/Transformer, Scaling laws), 50% thá»±c hÃ nh (fine-tune nhá», RAG, Ä‘Ã¡nh giÃ¡, triá»ƒn khai)

| MÃ´-Ä‘un | LÃ½ thuyáº¿t (50%) | Thá»±c hÃ nh (50%) |
|---|---|---|
| Language Modeling | PhÃ¢n rÃ£ xÃ¡c suáº¥t, perplexity, CE loss | Train tiny LM, Ä‘o perplexity |
| Scaling & Attention | Chinchilla, attention/positional | Thá»­ head/dim nhá», so sÃ¡nh loss |
| SFT & PEFT | Data quality, objectives, LoRA | Fine-tune instruction nhá» |
| RAG | Retrieval, rerank, hybrid | Build RAG + Ä‘Ã¡nh giÃ¡ quality |
| Serving | vLLM, quantization | Triá»ƒn khai + benchmark chi phÃ­ |

Rubric (100Ä‘/module): LÃ½ thuyáº¿t 30 | Code 30 | Káº¿t quáº£ 30 | BÃ¡o cÃ¡o 10

---

## ğŸ”¬ 1. Language Modeling Theory - LÃ½ thuyáº¿t mÃ´ hÃ¬nh ngÃ´n ngá»¯

### 1.1 Autoregressive Models - MÃ´ hÃ¬nh tá»± há»“i quy

> **Autoregressive Models** lÃ  cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ dá»± Ä‘oÃ¡n tá»« tiáº¿p theo dá»±a trÃªn cÃ¡c tá»« Ä‘Ã£ xuáº¥t hiá»‡n trÆ°á»›c Ä‘Ã³.

#### Probability Decomposition - PhÃ¢n rÃ£ xÃ¡c suáº¥t

**LÃ½ thuyáº¿t cÆ¡ báº£n:**
- **Chain Rule of Probability**: P(A,B) = P(A|B)P(B)
- **Markov Property**: P(xáµ¢|xâ‚,...,xáµ¢â‚‹â‚) â‰ˆ P(xáµ¢|xáµ¢â‚‹â‚–,...,xáµ¢â‚‹â‚) for k < i
- **Conditional Independence**: Words are conditionally independent given context
- **Entropy and Information Theory**: Measure of uncertainty in language

**Mathematical Foundations:**

**1. Chain Rule Derivation:**
```python
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple
import math

class LanguageModelingTheory:
    """Theoretical framework cho language modeling"""
    
    @staticmethod
    def explain_chain_rule():
        """Explain chain rule of probability mathematically"""
        print("""
        **Chain Rule of Probability:**
        
        For any sequence of events xâ‚, xâ‚‚, ..., xâ‚™:
        
        P(xâ‚, xâ‚‚, ..., xâ‚™) = P(xâ‚) Ã— P(xâ‚‚|xâ‚) Ã— P(xâ‚ƒ|xâ‚,xâ‚‚) Ã— ... Ã— P(xâ‚™|xâ‚,xâ‚‚,...,xâ‚™â‚‹â‚)
        
        **Mathematical Proof:**
        
        By definition of conditional probability:
        P(A|B) = P(A,B) / P(B)
        
        Therefore: P(A,B) = P(A|B) Ã— P(B)
        
        Applying recursively:
        P(xâ‚,xâ‚‚,xâ‚ƒ) = P(xâ‚ƒ|xâ‚,xâ‚‚) Ã— P(xâ‚,xâ‚‚)
                    = P(xâ‚ƒ|xâ‚,xâ‚‚) Ã— P(xâ‚‚|xâ‚) Ã— P(xâ‚)
        
        **Generalization:**
        P(xâ‚,...,xâ‚™) = Î áµ¢â‚Œâ‚â¿ P(xáµ¢|xâ‚,...,xáµ¢â‚‹â‚)
        
        This is the foundation of autoregressive language modeling.
        """)
    
    @staticmethod
    def demonstrate_chain_rule():
        """Demonstrate chain rule with concrete examples"""
        
        # Example: Simple language model with 3 words
        vocabulary = ['the', 'cat', 'sat', 'on', 'mat']
        
        # Define conditional probabilities (simplified)
        # P(word|context) - in practice, these come from training data
        conditional_probs = {
            'the': {'': 0.3},  # Start of sentence
            'cat': {'the': 0.4, 'on': 0.1},
            'sat': {'cat': 0.6, 'the': 0.1},
            'on': {'sat': 0.3, 'cat': 0.2},
            'mat': {'on': 0.5, 'sat': 0.1}
        }
        
        # Calculate probability of sequence "the cat sat on mat"
        sequence = ['the', 'cat', 'sat', 'on', 'mat']
        
        print("**Chain Rule Demonstration:**")
        print(f"Sequence: {' '.join(sequence)}")
        print("\nCalculating P(the, cat, sat, on, mat):")
        
        total_prob = 1.0
        context = ""
        
        for i, word in enumerate(sequence):
            if context in conditional_probs[word]:
                prob = conditional_probs[word][context]
            else:
                prob = 0.01  # Small probability for unseen contexts
            
            print(f"P({word}|{context if context else 'START'}) = {prob:.3f}")
            total_prob *= prob
            context = word
        
        print(f"\nTotal probability: {total_prob:.6f}")
        print(f"Log probability: {math.log(total_prob):.6f}")
        
        # Demonstrate with different sequences
        sequences = [
            ['the', 'cat', 'sat'],
            ['the', 'cat', 'sat', 'on'],
            ['the', 'cat', 'sat', 'on', 'mat']
        ]
        
        print("\n**Probability Comparison:**")
        print("Sequence\t\tProbability\tLog Probability")
        print("-" * 50)
        
        for seq in sequences:
            prob = 1.0
            context = ""
            
            for word in seq:
                if context in conditional_probs[word]:
                    prob *= conditional_probs[word][context]
                else:
                    prob *= 0.01
                context = word
            
            log_prob = math.log(prob)
            print(f"{' '.join(seq):15}\t{prob:.6f}\t{log_prob:.6f}")
        
        return {
            'vocabulary': vocabulary,
            'conditional_probs': conditional_probs,
            'sequences': sequences
        }
    
    @staticmethod
    def analyze_markov_property():
        """Analyze Markov property in language modeling"""
        
        print("""
        **Markov Property in Language Modeling:**
        
        **Definition:**
        A sequence has the k-th order Markov property if:
        P(xáµ¢|xâ‚,...,xáµ¢â‚‹â‚) = P(xáµ¢|xáµ¢â‚‹â‚–,...,xáµ¢â‚‹â‚)
        
        **Implications:**
        - Only the last k words matter for predicting the next word
        - Reduces computational complexity from O(n) to O(k)
        - Trade-off between context length and model size
        
        **Examples:**
        - k=1 (First-order): P(xáµ¢|xáµ¢â‚‹â‚) - only previous word matters
        - k=2 (Second-order): P(xáµ¢|xáµ¢â‚‹â‚‚,xáµ¢â‚‹â‚) - last 2 words matter
        - k=âˆ (Full context): P(xáµ¢|xâ‚,...,xáµ¢â‚‹â‚) - all previous words matter
        """)
        
        # Demonstrate Markov property with n-gram models
        def create_ngram_model(text: str, n: int) -> Dict[str, Dict[str, float]]:
            """Create n-gram language model"""
            words = text.split()
            ngrams = {}
            
            for i in range(len(words) - n + 1):
                context = ' '.join(words[i:i+n-1])
                next_word = words[i+n-1]
                
                if context not in ngrams:
                    ngrams[context] = {}
                
                if next_word not in ngrams[context]:
                    ngrams[context][next_word] = 0
                
                ngrams[context][next_word] += 1
            
            # Convert counts to probabilities
            for context in ngrams:
                total = sum(ngrams[context].values())
                for word in ngrams[context]:
                    ngrams[context][word] /= total
            
            return ngrams
        
        # Sample text
        sample_text = "the cat sat on the mat the cat ran fast the dog barked loud"
        
        # Create different order n-gram models
        unigram_model = create_ngram_model(sample_text, 1)
        bigram_model = create_ngram_model(sample_text, 2)
        trigram_model = create_ngram_model(sample_text, 3)
        
        print("\n**N-gram Model Comparison:**")
        print(f"Sample text: {sample_text}")
        
        print("\n**Unigram Model (k=0):**")
        for word, prob in unigram_model[''].items():
            print(f"  P({word}) = {prob:.3f}")
        
        print("\n**Bigram Model (k=1):**")
        for context, probs in bigram_model.items():
            print(f"  Context: '{context}'")
            for word, prob in probs.items():
                print(f"    P({word}|{context}) = {prob:.3f}")
        
        print("\n**Trigram Model (k=2):**")
        for context, probs in trigram_model.items():
            print(f"  Context: '{context}'")
            for word, prob in probs.items():
                print(f"    P({word}|{context}) = {prob:.3f}")
        
        return {
            'unigram': unigram_model,
            'bigram': bigram_model,
            'trigram': trigram_model
        }
    
    @staticmethod
    def entropy_analysis():
        """Analyze entropy and information theory in language modeling"""
        
        print("""
        **Entropy and Information Theory:**
        
        **Entropy H(X):**
        - Measures uncertainty in a random variable
        - H(X) = -Î£áµ¢ P(xáµ¢) logâ‚‚ P(xáµ¢)
        - Higher entropy = more uncertainty
        
        **Cross-entropy H(P,Q):**
        - Measures difference between true distribution P and predicted Q
        - H(P,Q) = -Î£áµ¢ P(xáµ¢) logâ‚‚ Q(xáµ¢)
        - Used as loss function in language modeling
        
        **Perplexity:**
        - Perplexity = 2^H(P,Q)
        - Lower perplexity = better model
        - Perplexity = 1 means perfect prediction
        """)
        
        # Demonstrate entropy calculation
        def calculate_entropy(probabilities: List[float]) -> float:
            """Calculate entropy of probability distribution"""
            entropy = 0.0
            for p in probabilities:
                if p > 0:
                    entropy -= p * math.log2(p)
            return entropy
        
        def calculate_cross_entropy(true_probs: List[float], pred_probs: List[float]) -> float:
            """Calculate cross-entropy between true and predicted distributions"""
            cross_entropy = 0.0
            for p_true, p_pred in zip(true_probs, pred_probs):
                if p_true > 0 and p_pred > 0:
                    cross_entropy -= p_true * math.log2(p_pred)
            return cross_entropy
        
        # Example: Different probability distributions
        distributions = {
            'Uniform': [0.25, 0.25, 0.25, 0.25],
            'Skewed': [0.7, 0.2, 0.08, 0.02],
            'Deterministic': [1.0, 0.0, 0.0, 0.0]
        }
        
        print("\n**Entropy Analysis:**")
        print("Distribution\tEntropy\tPerplexity")
        print("-" * 40)
        
        for name, probs in distributions.items():
            entropy = calculate_entropy(probs)
            perplexity = 2**entropy
            print(f"{name:15}\t{entropy:.3f}\t{perplexity:.3f}")
        
        # Demonstrate cross-entropy
        true_dist = [0.5, 0.3, 0.2]
        perfect_pred = [0.5, 0.3, 0.2]
        poor_pred = [0.1, 0.1, 0.8]
        
        print("\n**Cross-Entropy Analysis:**")
        print("Prediction\tCross-Entropy\tPerplexity")
        print("-" * 45)
        
        for name, pred in [('Perfect', perfect_pred), ('Poor', poor_pred)]:
            cross_ent = calculate_cross_entropy(true_dist, pred)
            perplexity = 2**cross_ent
            print(f"{name:15}\t{cross_ent:.3f}\t\t{perplexity:.3f}")
        
        return distributions

# Demonstrate language modeling theory
lm_theory = LanguageModelingTheory()
lm_theory.explain_chain_rule()

# Demonstrate chain rule
chain_rule_results = lm_theory.demonstrate_chain_rule()

# Analyze Markov property
markov_results = lm_theory.analyze_markov_property()

# Analyze entropy
entropy_results = lm_theory.entropy_analysis()
```

**2. Scaling Laws Theory:**
```python
class ScalingLawsTheory:
    """Theoretical framework cho scaling laws in language models"""
    
    @staticmethod
    def explain_scaling_laws():
        """Explain scaling laws mathematically"""
        print("""
        **Scaling Laws in Language Models:**
        
        **Chinchilla Scaling Laws (Hoffmann et al., 2022):**
        
        For optimal performance, models should follow:
        
        N_opt = 20 Ã— D_opt
        
        Where:
        - N_opt: Optimal number of parameters
        - D_opt: Optimal number of training tokens
        
        **Loss Scaling:**
        
        L(N,D) = L_âˆ + A Ã— (N^Î± Ã— D^Î²)^(-1)
        
        Where:
        - L_âˆ: Irreducible loss (Bayes error)
        - A: Scaling coefficient
        - Î±, Î²: Scaling exponents (typically Î± â‰ˆ 0.5, Î² â‰ˆ 0.5)
        - N: Number of parameters
        - D: Number of training tokens
        
        **Compute Optimal Scaling:**
        
        C_opt = 6 Ã— N_opt Ã— D_opt
        
        Where C_opt is the optimal compute budget.
        """)
    
    @staticmethod
    def demonstrate_scaling_effects():
        """Demonstrate scaling effects with simulations"""
        
        # Parameters for scaling law simulation
        L_inf = 1.0  # Irreducible loss
        A = 100.0    # Scaling coefficient
        alpha = 0.5  # Parameter scaling exponent
        beta = 0.5   # Data scaling exponent
        
        def scaling_law(N: float, D: float) -> float:
            """Calculate loss according to scaling law"""
            return L_inf + A * (N**alpha * D**beta)**(-1)
        
        # Generate scaling data
        N_values = np.logspace(6, 9, 50)  # 1M to 1B parameters
        D_values = np.logspace(7, 10, 50)  # 10M to 10B tokens
        
        # Create meshgrid for 3D plotting
        N_mesh, D_mesh = np.meshgrid(N_values, D_values)
        L_mesh = scaling_law(N_mesh, D_mesh)
        
        # Find optimal scaling line
        optimal_ratio = 20  # N_opt = 20 Ã— D_opt
        D_opt = np.logspace(7, 10, 100)
        N_opt = optimal_ratio * D_opt
        L_opt = scaling_law(N_opt, D_opt)
        
        # Visualization
        fig = plt.figure(figsize=(15, 10))
        
        # 3D surface plot
        ax1 = fig.add_subplot(2, 2, 1, projection='3d')
        surface = ax1.plot_surface(np.log10(N_mesh), np.log10(D_mesh), L_mesh, 
                                 cmap='viridis', alpha=0.8)
        ax1.plot(np.log10(N_opt), np.log10(D_opt), L_opt, 'r-', linewidth=3, label='Optimal Scaling')
        ax1.set_xlabel('logâ‚â‚€(Parameters)')
        ax1.set_ylabel('logâ‚â‚€(Tokens)')
        ax1.set_zlabel('Loss')
        ax1.set_title('Scaling Law Surface')
        ax1.legend()
        
        # 2D contour plot
        ax2 = fig.add_subplot(2, 2, 2)
        contour = ax2.contour(np.log10(N_mesh), np.log10(D_mesh), L_mesh, levels=20)
        ax2.plot(np.log10(N_opt), np.log10(D_opt), 'r-', linewidth=3, label='Optimal Scaling')
        ax2.clabel(contour, inline=True, fontsize=8)
        ax2.set_xlabel('logâ‚â‚€(Parameters)')
        ax2.set_ylabel('logâ‚â‚€(Tokens)')
        ax2.set_title('Loss Contours')
        ax2.legend()
        
        # Loss vs Parameters (fixed data)
        ax3 = fig.add_subplot(2, 2, 3)
        fixed_D = 1e9  # 1B tokens
        L_vs_N = scaling_law(N_values, fixed_D)
        ax3.loglog(N_values, L_vs_N, 'b-', linewidth=2, label=f'D = {fixed_D:.0e}')
        ax3.set_xlabel('Parameters (N)')
        ax3.set_ylabel('Loss')
        ax3.set_title('Loss vs Parameters (Fixed Data)')
        ax3.grid(True)
        ax3.legend()
        
        # Loss vs Data (fixed parameters)
        ax4 = fig.add_subplot(2, 2, 4)
        fixed_N = 1e8  # 100M parameters
        L_vs_D = scaling_law(fixed_N, D_values)
        ax4.loglog(D_values, L_vs_D, 'g-', linewidth=2, label=f'N = {fixed_N:.0e}')
        ax4.set_xlabel('Training Tokens (D)')
        ax4.set_ylabel('Loss')
        ax4.set_title('Loss vs Data (Fixed Parameters)')
        ax4.grid(True)
        ax4.legend()
        
        plt.tight_layout()
        plt.show()
        
        # Analyze optimal scaling
        print("\n**Optimal Scaling Analysis:**")
        print("D (tokens)\tN (parameters)\tRatio\tLoss")
        print("-" * 50)
        
        for i in range(0, len(D_opt), 20):
            print(f"{D_opt[i]:.1e}\t{N_opt[i]:.1e}\t{N_opt[i]/D_opt[i]:.1f}\t{L_opt[i]:.4f}")
        
        return {
            'N_values': N_values,
            'D_values': D_values,
            'L_mesh': L_mesh,
            'N_opt': N_opt,
            'D_opt': D_opt,
            'L_opt': L_opt
        }
    
    @staticmethod
    def analyze_compute_efficiency():
        """Analyze compute efficiency of different scaling strategies"""
        
        print("""
        **Compute Efficiency Analysis:**
        
        **Compute Budget:**
        C = 6 Ã— N Ã— D (approximate FLOPs)
        
        **Efficiency Strategies:**
        1. **Chinchilla Optimal**: N = 20D (balanced)
        2. **Parameter-Heavy**: N >> 20D (over-parameterized)
        3. **Data-Heavy**: N << 20D (under-parameterized)
        """)
        
        # Define different scaling strategies
        D_base = 1e9  # 1B tokens
        
        strategies = {
            'Chinchilla Optimal': 20,
            'Parameter-Heavy (2x)': 40,
            'Parameter-Heavy (5x)': 100,
            'Data-Heavy (0.5x)': 10,
            'Data-Heavy (0.2x)': 4
        }
        
        # Calculate compute and loss for each strategy
        results = {}
        
        for name, ratio in strategies.items():
            N = ratio * D_base
            D = D_base
            C = 6 * N * D
            L = 1.0 + 100.0 * (N**0.5 * D**0.5)**(-1)
            
            results[name] = {
                'N': N,
                'D': D,
                'C': C,
                'L': L,
                'ratio': ratio
            }
        
        # Display results
        print("\n**Scaling Strategy Comparison:**")
        print("Strategy\t\tN\t\tD\t\tC\t\tLoss\tRatio")
        print("-" * 80)
        
        for name, result in results.items():
            print(f"{name:20}\t{result['N']:.1e}\t{result['D']:.1e}\t{result['C']:.1e}\t{result['L']:.4f}\t{result['ratio']:.1f}")
        
        # Find most efficient strategy
        best_strategy = min(results.keys(), key=lambda x: results[x]['L'])
        print(f"\n**Most Efficient Strategy:** {best_strategy}")
        print(f"Loss: {results[best_strategy]['L']:.4f}")
        
        return results

# Demonstrate scaling laws theory
scaling_theory = ScalingLawsTheory()
scaling_theory.explain_scaling_laws()

# Demonstrate scaling effects
scaling_results = scaling_theory.demonstrate_scaling_effects()

# Analyze compute efficiency
efficiency_results = scaling_theory.analyze_compute_efficiency()
```

**3. Attention Mechanism Theory:**
```python
class AttentionMechanismTheory:
    """Theoretical framework cho attention mechanisms"""
    
    @staticmethod
    def explain_attention_mathematics():
        """Explain attention mechanism mathematically"""
        print("""
        **Attention Mechanism Mathematics:**
        
        **Query-Key-Value Framework:**
        
        Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V
        
        Where:
        - Q: Query matrix (n_queries Ã— d_k)
        - K: Key matrix (n_keys Ã— d_k)
        - V: Value matrix (n_keys Ã— d_v)
        - d_k: Key dimension
        - âˆšd_k: Scaling factor (prevents softmax saturation)
        
        **Multi-Head Attention:**
        
        MultiHead(Q,K,V) = Concat(headâ‚,...,headâ‚•)W^O
        
        Where each head is:
        headáµ¢ = Attention(QWáµ¢^Q, KWáµ¢^K, VWáµ¢^V)
        
        **Positional Encoding:**
        
        PE(pos,2i) = sin(pos/10000^(2i/d_model))
        PE(pos,2i+1) = cos(pos/10000^(2i/d_model))
        
        This provides position information to the model.
        """)
    
    @staticmethod
    def implement_attention_mechanism():
        """Implement attention mechanism step by step"""
        
        import torch
        import torch.nn.functional as F
        
        class AttentionImplementation:
            def __init__(self, d_model: int, d_k: int, d_v: int):
                self.d_model = d_model
                self.d_k = d_k
                self.d_v = d_v
                
                # Linear projections
                self.W_q = torch.randn(d_model, d_k)
                self.W_k = torch.randn(d_model, d_k)
                self.W_v = torch.randn(d_model, d_v)
                self.W_o = torch.randn(d_v, d_model)
            
            def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
                """Compute scaled dot-product attention"""
                
                # Step 1: Compute attention scores
                scores = torch.matmul(Q, K.transpose(-2, -1))
                
                # Step 2: Scale scores
                scores = scores / math.sqrt(self.d_k)
                
                # Step 3: Apply softmax
                attention_weights = F.softmax(scores, dim=-1)
                
                # Step 4: Apply attention to values
                output = torch.matmul(attention_weights, V)
                
                return output, attention_weights
            
            def forward(self, x: torch.Tensor) -> torch.Tensor:
                """Forward pass through attention mechanism"""
                
                # Project inputs to Q, K, V
                Q = torch.matmul(x, self.W_q)
                K = torch.matmul(x, self.W_k)
                V = torch.matmul(x, self.W_v)
                
                # Apply attention
                attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V)
                
                # Project output
                output = torch.matmul(attention_output, self.W_o)
                
                return output, attention_weights
        
        # Test attention mechanism
        d_model, d_k, d_v = 64, 32, 32
        seq_len = 10
        
        attention = AttentionImplementation(d_model, d_k, d_v)
        
        # Create input sequence
        x = torch.randn(seq_len, d_model)
        
        # Apply attention
        output, weights = attention.forward(x)
        
        print(f"Input shape: {x.shape}")
        print(f"Output shape: {output.shape}")
        print(f"Attention weights shape: {weights.shape}")
        
        # Visualize attention weights
        plt.figure(figsize=(10, 8))
        plt.imshow(weights.detach().numpy(), cmap='viridis')
        plt.colorbar()
        plt.title('Attention Weights Matrix')
        plt.xlabel('Key Position')
        plt.ylabel('Query Position')
        plt.show()
        
        return attention, x, output, weights

# Demonstrate attention theory
attention_theory = AttentionMechanismTheory()
attention_theory.explain_attention_mathematics()

# Implement attention mechanism
attention_model, input_seq, output_seq, attention_weights = attention_theory.implement_attention_mechanism()
```

**TÃ i liá»‡u tham kháº£o chuyÃªn sÃ¢u:**
- **Language Modeling**: [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- **Scaling Laws**: [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)
- **Attention Mechanisms**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- **Information Theory**: [Elements of Information Theory](https://www.wiley.com/en-us/Elements+of+Information+Theory,+2nd+Edition-p-9780471241959)

#### Perplexity - Äá»™ phá»©c táº¡p

**CÃ´ng thá»©c tÃ­nh perplexity**:
```
Perplexity = exp(-(1/n)Î£áµ¢ log P(xáµ¢|xâ‚,...,xáµ¢â‚‹â‚))
```

**Giáº£i thÃ­ch kÃ½ hiá»‡u:**
- **Perplexity**: Äá»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh (cÃ ng tháº¥p cÃ ng tá»‘t)
- **exp()**: HÃ m mÅ© tá»± nhiÃªn
- **n**: Sá»‘ lÆ°á»£ng tá»« trong chuá»—i
- **log P(xáµ¢|xâ‚,...,xáµ¢â‚‹â‚)**: Logarit cá»§a xÃ¡c suáº¥t dá»± Ä‘oÃ¡n

**Ã nghÄ©a cá»§a Perplexity**:
- **Perplexity = 1**: MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n hoÃ n háº£o
- **Perplexity = 2**: MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n nhÆ° random guessing
- **Perplexity cÃ ng tháº¥p**: MÃ´ hÃ¬nh cÃ ng hiá»ƒu ngÃ´n ngá»¯ tá»‘t

#### Cross-entropy Loss - HÃ m máº¥t mÃ¡t entropy chÃ©o

**CÃ´ng thá»©c cross-entropy loss**:
```
L = -(1/n)Î£áµ¢ log P(xáµ¢|xâ‚,...,xáµ¢â‚‹â‚)
```

**Giáº£i thÃ­ch kÃ½ hiá»‡u:**
- **L**: Loss value (giÃ¡ trá»‹ máº¥t mÃ¡t)
- **n**: Sá»‘ lÆ°á»£ng tá»« trong batch
- **log P(xáµ¢|xâ‚,...,xáµ¢â‚‹â‚)**: Logarit cá»§a xÃ¡c suáº¥t dá»± Ä‘oÃ¡n Ä‘Ãºng

**Má»‘i quan há»‡ vá»›i Perplexity**:
```
Perplexity = exp(L)
```

**Ã nghÄ©a thá»±c táº¿**:
- Loss cÃ ng tháº¥p, mÃ´ hÃ¬nh cÃ ng dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c
- Perplexity cÃ ng tháº¥p, mÃ´ hÃ¬nh cÃ ng hiá»ƒu ngÃ´n ngá»¯ tá»‘t
- Cáº£ hai metric Ä‘á»u Ä‘o lÆ°á»ng cháº¥t lÆ°á»£ng cá»§a language model

#### Implementation chi tiáº¿t

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class SimpleLanguageModel(nn.Module):
    """
    Language model Ä‘Æ¡n giáº£n sá»­ dá»¥ng LSTM
    
    Architecture:
    Embedding â†’ LSTM â†’ Linear â†’ Softmax
    """
    
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1):
        super().__init__()
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # Embedding layer: chuyá»ƒn tá»« indices thÃ nh vectors
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # LSTM layer: xá»­ lÃ½ chuá»—i tuáº§n tá»±
        self.lstm = nn.LSTM(
            embed_dim, 
            hidden_dim, 
            num_layers=num_layers, 
            batch_first=True,
            dropout=0.1 if num_layers > 1 else 0
        )
        
        # Output layer: dá»± Ä‘oÃ¡n tá»« tiáº¿p theo
        self.fc = nn.Linear(hidden_dim, vocab_size)
        
        # Dropout Ä‘á»ƒ trÃ¡nh overfitting
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x, hidden=None):
        """
        Forward pass cá»§a language model
        
        Parameters:
        x (torch.Tensor): Input sequence shape (batch_size, seq_len)
        hidden (tuple): Hidden state tá»« LSTM (optional)
        
        Returns:
        tuple: (output, hidden_state)
        """
        batch_size, seq_len = x.size()
        
        # 1. Embedding: chuyá»ƒn tá»« indices thÃ nh vectors
        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)
        embedded = self.dropout(embedded)
        
        # 2. LSTM: xá»­ lÃ½ chuá»—i tuáº§n tá»±
        lstm_out, hidden = self.lstm(embedded, hidden)
        # lstm_out: (batch_size, seq_len, hidden_dim)
        # hidden: (h_n, c_n) vá»›i h_n, c_n: (num_layers, batch_size, hidden_dim)
        
        # 3. Linear transformation: dá»± Ä‘oÃ¡n tá»« tiáº¿p theo
        output = self.fc(lstm_out)  # (batch_size, seq_len, vocab_size)
        
        return output, hidden
    
    def generate(self, start_tokens, max_length, temperature=1.0, top_k=50):
        """
        Generate text tá»« start tokens
        
        Parameters:
        start_tokens (list): Danh sÃ¡ch tokens báº¯t Ä‘áº§u
        max_length (int): Äá»™ dÃ i tá»‘i Ä‘a cá»§a text Ä‘Æ°á»£c generate
        temperature (float): Äiá»u chá»‰nh randomness (cÃ ng tháº¥p cÃ ng deterministic)
        top_k (int): Chá»‰ xem xÃ©t top-k tokens cÃ³ xÃ¡c suáº¥t cao nháº¥t
        
        Returns:
        list: Generated tokens
        """
        self.eval()
        generated = start_tokens.copy()
        
        with torch.no_grad():
            # Chuyá»ƒn start tokens thÃ nh tensor
            x = torch.tensor([start_tokens], dtype=torch.long)
            
            for _ in range(max_length - len(start_tokens)):
                # Forward pass
                output, _ = self.forward(x)
                
                # Láº¥y logits cá»§a token cuá»‘i cÃ¹ng
                logits = output[0, -1, :]  # (vocab_size,)
                
                # Apply temperature
                logits = logits / temperature
                
                # Top-k sampling
                if top_k > 0:
                    top_k_logits, top_k_indices = torch.topk(logits, top_k)
                    logits = torch.full_like(logits, float('-inf'))
                    logits[top_k_indices] = top_k_logits
                
                # Softmax Ä‘á»ƒ cÃ³ xÃ¡c suáº¥t
                probs = F.softmax(logits, dim=-1)
                
                # Sample tá»« distribution
                next_token = torch.multinomial(probs, 1).item()
                
                # ThÃªm token má»›i vÃ o sequence
                generated.append(next_token)
                x = torch.cat([x, torch.tensor([[next_token]], dtype=torch.long)], dim=1)
        
        return generated

def train_step(model, optimizer, criterion, data, target):
    """
    Training step cho language model
    
    Parameters:
    model: Language model
    optimizer: Optimizer (Adam, SGD, etc.)
    criterion: Loss function (CrossEntropyLoss)
    data (torch.Tensor): Input sequence
    target (torch.Tensor): Target sequence
    
    Returns:
    float: Loss value
    """
    # Zero gradients
    optimizer.zero_grad()
    
    # Forward pass
    output, _ = model(data)
    
    # Reshape output vÃ  target cho loss calculation
    # output: (batch_size, seq_len, vocab_size) â†’ (batch_size * seq_len, vocab_size)
    # target: (batch_size, seq_len) â†’ (batch_size * seq_len)
    output_reshaped = output.view(-1, output.size(-1))
    target_reshaped = target.view(-1)
    
    # Calculate loss
    loss = criterion(output_reshaped, target_reshaped)
    
    # Backward pass
    loss.backward()
    
    # Gradient clipping Ä‘á»ƒ trÃ¡nh gradient explosion
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    # Update parameters
    optimizer.step()
    
    return loss.item()

def calculate_perplexity(model, data_loader, criterion):
    """
    TÃ­nh perplexity cá»§a model trÃªn dataset
    
    Parameters:
    model: Language model
    data_loader: DataLoader chá»©a test data
    criterion: Loss function
    
    Returns:
    float: Perplexity value
    """
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(data_loader):
            output, _ = model(data)
            
            # Reshape cho loss calculation
            output_reshaped = output.view(-1, output.size(-1))
            target_reshaped = target.view(-1)
            
            # Calculate loss
            loss = criterion(output_reshaped, target_reshaped)
            
            # Accumulate loss vÃ  count tokens
            total_loss += loss.item() * target_reshaped.size(0)
            total_tokens += target_reshaped.size(0)
    
    # TÃ­nh average loss
    avg_loss = total_loss / total_tokens
    
    # TÃ­nh perplexity
    perplexity = np.exp(avg_loss)
    
    return perplexity, avg_loss

# VÃ­ dá»¥ sá»­ dá»¥ng
def demonstrate_language_model():
    """
    Minh há»a cÃ¡ch sá»­ dá»¥ng language model
    """
    
    # Hyperparameters
    vocab_size = 10000
    embed_dim = 256
    hidden_dim = 512
    num_layers = 2
    
    # Khá»Ÿi táº¡o model
    model = SimpleLanguageModel(vocab_size, embed_dim, hidden_dim, num_layers)
    
    print("ğŸ¤– LANGUAGE MODEL DEMONSTRATION")
    print("=" * 50)
    print(f"Vocabulary size: {vocab_size:,}")
    print(f"Embedding dimension: {embed_dim}")
    print(f"Hidden dimension: {hidden_dim}")
    print(f"Number of LSTM layers: {num_layers}")
    print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # Táº¡o dá»¯ liá»‡u máº«u
    batch_size = 4
    seq_len = 10
    
    # Random data (trong thá»±c táº¿ sáº½ lÃ  text Ä‘Ã£ Ä‘Æ°á»£c tokenize)
    data = torch.randint(0, vocab_size, (batch_size, seq_len))
    target = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    print(f"\nğŸ“Š Sample Data:")
    print(f"Input shape: {data.shape}")
    print(f"Target shape: {target.shape}")
    
    # Test forward pass
    print(f"\nâ¡ï¸ Forward Pass Test:")
    output, hidden = model(data)
    print(f"Output shape: {output.shape}")
    print(f"Hidden state shape: {hidden[0].shape}")  # h_n shape
    
    # Test training step
    print(f"\nğŸ¯ Training Step Test:")
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    loss = train_step(model, optimizer, criterion, data, target)
    print(f"Training loss: {loss:.4f}")
    
    # Test text generation
    print(f"\nâœï¸ Text Generation Test:")
    start_tokens = [1, 2, 3]  # Start tokens
    generated = model.generate(start_tokens, max_length=10, temperature=0.8)
    print(f"Generated tokens: {generated}")
    
    return model

# VÃ­ dá»¥ sá»­ dá»¥ng
# model = demonstrate_language_model()
```

**Giáº£i thÃ­ch cÃ¡c khÃ¡i niá»‡m:**
- **Embedding**: Chuyá»ƒn Ä‘á»•i tá»« indices thÃ nh vectors cÃ³ Ã½ nghÄ©a
- **LSTM**: Long Short-Term Memory, xá»­ lÃ½ chuá»—i tuáº§n tá»±
- **Hidden State**: Tráº¡ng thÃ¡i áº©n cá»§a LSTM, chá»©a thÃ´ng tin tá»« quÃ¡ khá»©
- **Top-k Sampling**: Chá»‰ xem xÃ©t k tokens cÃ³ xÃ¡c suáº¥t cao nháº¥t khi generate

### 1.2 Scaling Laws - Quy luáº­t má»Ÿ rá»™ng

> **Scaling Laws** lÃ  cÃ¡c quy luáº­t toÃ¡n há»c mÃ´ táº£ má»‘i quan há»‡ giá»¯a kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh, lÆ°á»£ng dá»¯ liá»‡u training vÃ  hiá»‡u suáº¥t.

#### Chinchilla Scaling - Quy luáº­t Chinchilla

**CÃ´ng thá»©c Chinchilla**:
```
Optimal model size: N = 20D
Optimal training tokens: D = 1.4 Ã— 10â¶
```

**Giáº£i thÃ­ch kÃ½ hiá»‡u:**
- **N**: Sá»‘ lÆ°á»£ng parameters cá»§a mÃ´ hÃ¬nh
- **D**: Sá»‘ lÆ°á»£ng training tokens (tá»«)
- **20**: Há»‡ sá»‘ tá»· lá»‡ tá»‘i Æ°u
- **1.4 Ã— 10â¶**: Sá»‘ tokens tá»‘i Æ°u cho training

**Ã nghÄ©a thá»±c táº¿**:
- MÃ´ hÃ¬nh cÃ ng lá»›n cáº§n cÃ ng nhiá»u dá»¯ liá»‡u training
- Tá»· lá»‡ 20:1 giá»¯a parameters vÃ  tokens lÃ  tá»‘i Æ°u
- Over-parameterization (mÃ´ hÃ¬nh quÃ¡ lá»›n) khÃ´ng hiá»‡u quáº£

#### Performance Scaling - Má»Ÿ rá»™ng hiá»‡u suáº¥t

**CÃ´ng thá»©c tá»•ng quÃ¡t**:
```
Loss = A + B/N^Î± + C/D^Î²
```

**Giáº£i thÃ­ch kÃ½ hiá»‡u:**
- **Loss**: HÃ m máº¥t mÃ¡t cá»§a mÃ´ hÃ¬nh
- **A**: Loss cÆ¡ báº£n khÃ´ng thá»ƒ giáº£m Ä‘Æ°á»£c
- **B/N^Î±**: ThÃ nh pháº§n phá»¥ thuá»™c vÃ o kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh
- **C/D^Î²**: ThÃ nh pháº§n phá»¥ thuá»™c vÃ o lÆ°á»£ng dá»¯ liá»‡u
- **Î±, Î²**: CÃ¡c há»‡ sá»‘ mÅ© (thÆ°á»ng Î± â‰ˆ 0.1, Î² â‰ˆ 0.5)

**Ã nghÄ©a thá»±c táº¿**:
- Loss giáº£m khi tÄƒng kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh hoáº·c dá»¯ liá»‡u
- CÃ³ giá»›i háº¡n vá» hiá»‡u suáº¥t cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c
- Cáº§n cÃ¢n báº±ng giá»¯a model size vÃ  data size

#### Empirical Findings - Nhá»¯ng phÃ¡t hiá»‡n thá»±c nghiá»‡m

**CÃ¡c nguyÃªn lÃ½ quan trá»ng**:

1. **Model size vÃ  training data pháº£i cÃ¢n báº±ng**
   - MÃ´ hÃ¬nh lá»›n cáº§n nhiá»u dá»¯ liá»‡u
   - MÃ´ hÃ¬nh nhá» vá»›i nhiá»u dá»¯ liá»‡u cÃ³ thá»ƒ overfit

2. **Over-parameterization khÃ´ng hiá»‡u quáº£**
   - TÄƒng parameters quÃ¡ má»©c khÃ´ng cáº£i thiá»‡n hiá»‡u suáº¥t
   - Tá»‘n kÃ©m vá» compute vÃ  memory

3. **Data quality quan trá»ng hÆ¡n quantity**
   - 1M tokens cháº¥t lÆ°á»£ng cao tá»‘t hÆ¡n 10M tokens cháº¥t lÆ°á»£ng tháº¥p
   - Cáº§n táº­p trung vÃ o viá»‡c lÃ m sáº¡ch vÃ  chuáº©n hÃ³a dá»¯ liá»‡u

#### Implementation vÃ  Visualization

```python
import matplotlib.pyplot as plt
import numpy as np

def demonstrate_scaling_laws():
    """
    Minh há»a scaling laws trong language models
    """
    
    print("ğŸ“Š SCALING LAWS DEMONSTRATION")
    print("=" * 50)
    
    # Parameters cho scaling laws
    A = 1.0      # Base loss
    B = 1000     # Model size coefficient
    C = 500      # Data size coefficient
    alpha = 0.1  # Model size exponent
    beta = 0.5   # Data size exponent
    
    # Táº¡o data cho visualization
    model_sizes = np.logspace(6, 9, 100)  # 1M to 1B parameters
    data_sizes = np.logspace(5, 7, 100)   # 100K to 10M tokens
    
    # TÃ­nh loss theo model size (vá»›i data size cá»‘ Ä‘á»‹nh)
    fixed_data_size = 1e6  # 1M tokens
    loss_vs_model = A + B / (model_sizes ** alpha) + C / (fixed_data_size ** beta)
    
    # TÃ­nh loss theo data size (vá»›i model size cá»‘ Ä‘á»‹nh)
    fixed_model_size = 1e8  # 100M parameters
    loss_vs_data = A + B / (fixed_model_size ** alpha) + C / (data_sizes ** beta)
    
    # Chinchilla optimal line
    optimal_model_sizes = np.logspace(6, 9, 50)
    optimal_data_sizes = optimal_model_sizes / 20
    
    # Visualization
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Loss vs Model Size
    axes[0, 0].loglog(model_sizes, loss_vs_model, 'b-', linewidth=2)
    axes[0, 0].set_xlabel('Model Size (Parameters)')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Loss vs Model Size (Fixed Data: 1M tokens)')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Loss vs Data Size
    axes[0, 1].loglog(data_sizes, loss_vs_data, 'r-', linewidth=2)
    axes[0, 1].set_xlabel('Data Size (Tokens)')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].set_title('Loss vs Data Size (Fixed Model: 100M parameters)')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Chinchilla Optimal Line
    axes[1, 0].loglog(optimal_model_sizes, optimal_data_sizes, 'g-', linewidth=2, label='Optimal N=20D')
    axes[1, 0].set_xlabel('Model Size (Parameters)')
    axes[1, 0].set_ylabel('Data Size (Tokens)')
    axes[1, 0].set_title('Chinchilla Optimal Scaling (N=20D)')
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].legend()
    
    # 4. 3D Surface Plot
    X, Y = np.meshgrid(model_sizes, data_sizes)
    Z = A + B / (X ** alpha) + C / (Y ** beta)
    
    surf = axes[1, 1].contourf(np.log10(X), np.log10(Y), Z, levels=20, cmap='viridis')
    axes[1, 1].set_xlabel('log10(Model Size)')
    axes[1, 1].set_ylabel('log10(Data Size)')
    axes[1, 1].set_title('Loss Surface (Model Size vs Data Size)')
    plt.colorbar(surf, ax=axes[1, 1], label='Loss')
    
    plt.tight_layout()
    plt.show()
    
    # TÃ­nh toÃ¡n vÃ­ dá»¥ cá»¥ thá»ƒ
    print(f"\nğŸ” SCALING ANALYSIS:")
    print(f"Base loss (A): {A:.2f}")
    print(f"Model size coefficient (B): {B:,}")
    print(f"Data size coefficient (C): {C:,}")
    print(f"Alpha (model size exponent): {alpha}")
    print(f"Beta (data size exponent): {beta}")
    
    # VÃ­ dá»¥ vá»›i model 100M parameters
    model_size_example = 1e8
    data_size_example = 1e6
    
    loss_example = A + B / (model_size_example ** alpha) + C / (data_size_example ** beta)
    
    print(f"\nğŸ“Š EXAMPLE CALCULATION:")
    print(f"Model size: {model_size_example:,} parameters")
    print(f"Data size: {data_size_example:,} tokens")
    print(f"Calculated loss: {loss_example:.4f}")
    
    # Chinchilla optimal cho model size nÃ y
    optimal_data = model_size_example / 20
    print(f"Chinchilla optimal data size: {optimal_data:,.0f} tokens")
    
    return {
        'model_sizes': model_sizes,
        'data_sizes': data_sizes,
        'loss_vs_model': loss_vs_model,
        'loss_vs_data': loss_vs_data
    }

# VÃ­ dá»¥ sá»­ dá»¥ng
# scaling_data = demonstrate_scaling_laws()
```

**Giáº£i thÃ­ch káº¿t quáº£:**
- **Loss vs Model Size**: Loss giáº£m khi tÄƒng kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh, nhÆ°ng cÃ³ giá»›i háº¡n
- **Loss vs Data Size**: Loss giáº£m khi tÄƒng dá»¯ liá»‡u training
- **Chinchilla Optimal**: ÄÆ°á»ng tháº³ng N=20D cho tháº¥y tá»· lá»‡ tá»‘i Æ°u
- **Loss Surface**: Bá» máº·t 3D cho tháº¥y má»‘i quan há»‡ giá»¯a cáº£ hai yáº¿u tá»‘

## ğŸ“š TÃ i liá»‡u tham kháº£o

### LÃ½ thuyáº¿t Language Modeling
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2 paper
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) - Scaling laws paper

### Scaling vÃ  Optimization
- [Chinchilla: Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) - Chinchilla paper
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) - Original scaling laws

### Implementation
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/) - ThÆ° viá»‡n transformers
- [PyTorch Language Modeling Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) - PyTorch tutorial

## ğŸ¯ BÃ i táº­p thá»±c hÃ nh

1. **Language Model**: Implement language model Ä‘Æ¡n giáº£n vá»›i LSTM/Transformer
2. **Scaling Analysis**: PhÃ¢n tÃ­ch scaling laws trÃªn dataset thá»±c táº¿
3. **Text Generation**: Táº¡o text generation pipeline vá»›i temperature vÃ  top-k sampling
4. **Perplexity Calculation**: TÃ­nh perplexity trÃªn test set
5. **Model Optimization**: Tá»‘i Æ°u hÃ³a hyperparameters cho language model

## ğŸš€ BÆ°á»›c tiáº¿p theo

Sau khi hoÃ n thÃ nh LLMs cÆ¡ báº£n, báº¡n sáº½:
- Hiá»ƒu sÃ¢u vá» kiáº¿n trÃºc Transformer vÃ  attention mechanisms
- Biáº¿t cÃ¡ch fine-tune LLMs cho cÃ¡c tÃ¡c vá»¥ cá»¥ thá»ƒ
- CÃ³ thá»ƒ triá»ƒn khai RAG vÃ  vector search
- Sáºµn sÃ ng há»c advanced techniques nhÆ° RLHF vÃ  model compression

---

*ChÃºc báº¡n trá»Ÿ thÃ nh LLM Engineer xuáº¥t sáº¯c! ğŸ‰*


# ü§ñ LLMs v√† ·ª©ng d·ª•ng - Large Language Models

> **M·ª•c ti√™u**: Tr·ªü th√†nh chuy√™n gia LLMs, hi·ªÉu s√¢u v·ªÅ ki·∫øn tr√∫c Transformer, fine-tuning v√† tri·ªÉn khai c√°c ·ª©ng d·ª•ng AI th·ª±c t·∫ø

## üìã T·ªïng quan n·ªôi dung

```mermaid
graph TD
    A[ü§ñ Large Language Models] --> B[üî¨ Language Modeling Theory]
    A --> C[üéØ Supervised Fine-tuning]
    A --> D[üîÑ Reinforcement Learning]
    A --> E[üîç RAG & Vector Search]
    A --> F[üöÄ Model Deployment]
    A --> G[‚ö° Optimization & Quantization]
    
    B --> B1[Autoregressive Models]
    B --> B2[Scaling Laws]
    B --> B3[Attention Mechanisms]
    B --> B4[Transformer Architecture]
    
    C --> C1[Data Preparation]
    C --> C2[Instruction Tuning]
    C --> C3[LoRA & PEFT]
    C --> C4[Evaluation Metrics]
    
    D --> D1[RLHF Framework]
    D --> D2[PPO Algorithm]
    D --> D3[Reward Modeling]
    D --> D4[Human Feedback]
    
    E --> E1[Vector Databases]
    E --> E2[Retrieval Methods]
    E --> E3[Reranking]
    E --> E4[Hybrid Search]
    
    F --> F1[Model Serving]
    F --> F2[API Development]
    F --> F3[Monitoring & Scaling]
    F --> F4[Cost Optimization]
    
    G --> G1[Quantization 8/4-bit]
    G --> G2[Pruning & Distillation]
    G --> G3[Model Compression]
    G --> G4[Hardware Optimization]
```

![LLMs Architecture](assets/llms-architecture.svg)

![LLMs Architecture PNG](assets/llms-architecture.png)

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/llms-architecture.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/llms-architecture.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/llms-architecture.png)**

## üß© Ch∆∞∆°ng tr√¨nh 50/50 (L√Ω thuy·∫øt : Th·ª±c h√†nh)

- M·ª•c ti√™u: 50% l√Ω thuy·∫øt (nguy√™n l√Ω m√¥ h√¨nh ho√° ng√¥n ng·ªØ, Attention/Transformer, Scaling laws), 50% th·ª±c h√†nh (fine-tune nh·ªè, RAG, ƒë√°nh gi√°, tri·ªÉn khai)

| M√¥-ƒëun | L√Ω thuy·∫øt (50%) | Th·ª±c h√†nh (50%) |
|---|---|---|
| Language Modeling | Ph√¢n r√£ x√°c su·∫•t, perplexity, CE loss | Train tiny LM, ƒëo perplexity |
| Scaling & Attention | Chinchilla, attention/positional | Th·ª≠ head/dim nh·ªè, so s√°nh loss |
| SFT & PEFT | Data quality, objectives, LoRA | Fine-tune instruction nh·ªè |
| RAG | Retrieval, rerank, hybrid | Build RAG + ƒë√°nh gi√° quality |
| Serving | vLLM, quantization | Tri·ªÉn khai + benchmark chi ph√≠ |

Rubric (100ƒë/module): L√Ω thuy·∫øt 30 | Code 30 | K·∫øt qu·∫£ 30 | B√°o c√°o 10

---

## üî¨ 1. Language Modeling Theory - L√Ω thuy·∫øt m√¥ h√¨nh ng√¥n ng·ªØ

### 1.1 Autoregressive Models - M√¥ h√¨nh t·ª± h·ªìi quy

> **Autoregressive Models** l√† c√°c m√¥ h√¨nh ng√¥n ng·ªØ d·ª± ƒëo√°n t·ª´ ti·∫øp theo d·ª±a tr√™n c√°c t·ª´ ƒë√£ xu·∫•t hi·ªán tr∆∞·ªõc ƒë√≥.

#### Probability Decomposition - Ph√¢n r√£ x√°c su·∫•t

**L√Ω thuy·∫øt c∆° b·∫£n:**
- **Chain Rule of Probability**: P(A,B) = P(A|B)P(B)
- **Markov Property**: P(x·µ¢|x‚ÇÅ,...,x·µ¢‚Çã‚ÇÅ) ‚âà P(x·µ¢|x·µ¢‚Çã‚Çñ,...,x·µ¢‚Çã‚ÇÅ) for k < i
- **Conditional Independence**: Words are conditionally independent given context
- **Entropy and Information Theory**: Measure of uncertainty in language

**Mathematical Foundations:**

**1. Chain Rule Derivation:**
```python
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple
import math

class LanguageModelingTheory:
    """Theoretical framework cho language modeling"""
    
    @staticmethod
    def explain_chain_rule():
        """Explain chain rule of probability mathematically"""
        print("""
        **Chain Rule of Probability:**
        
        For any sequence of events x‚ÇÅ, x‚ÇÇ, ..., x‚Çô:
        
        P(x‚ÇÅ, x‚ÇÇ, ..., x‚Çô) = P(x‚ÇÅ) √ó P(x‚ÇÇ|x‚ÇÅ) √ó P(x‚ÇÉ|x‚ÇÅ,x‚ÇÇ) √ó ... √ó P(x‚Çô|x‚ÇÅ,x‚ÇÇ,...,x‚Çô‚Çã‚ÇÅ)
        
        **Mathematical Proof:**
        
        By definition of conditional probability:
        P(A|B) = P(A,B) / P(B)
        
        Therefore: P(A,B) = P(A|B) √ó P(B)
        
        Applying recursively:
        P(x‚ÇÅ,x‚ÇÇ,x‚ÇÉ) = P(x‚ÇÉ|x‚ÇÅ,x‚ÇÇ) √ó P(x‚ÇÅ,x‚ÇÇ)
                    = P(x‚ÇÉ|x‚ÇÅ,x‚ÇÇ) √ó P(x‚ÇÇ|x‚ÇÅ) √ó P(x‚ÇÅ)
        
        **Generalization:**
        P(x‚ÇÅ,...,x‚Çô) = Œ†·µ¢‚Çå‚ÇÅ‚Åø P(x·µ¢|x‚ÇÅ,...,x·µ¢‚Çã‚ÇÅ)
        
        This is the foundation of autoregressive language modeling.
        """)
    
    @staticmethod
    def demonstrate_chain_rule():
        """Demonstrate chain rule with concrete examples"""
        
        # Example: Simple language model with 3 words
        vocabulary = ['the', 'cat', 'sat', 'on', 'mat']
        
        # Define conditional probabilities (simplified)
        # P(word|context) - in practice, these come from training data
        conditional_probs = {
            'the': {'': 0.3},  # Start of sentence
            'cat': {'the': 0.4, 'on': 0.1},
            'sat': {'cat': 0.6, 'the': 0.1},
            'on': {'sat': 0.3, 'cat': 0.2},
            'mat': {'on': 0.5, 'sat': 0.1}
        }
        
        # Calculate probability of sequence "the cat sat on mat"
        sequence = ['the', 'cat', 'sat', 'on', 'mat']
        
        print("**Chain Rule Demonstration:**")
        print(f"Sequence: {' '.join(sequence)}")
        print("\nCalculating P(the, cat, sat, on, mat):")
        
        total_prob = 1.0
        context = ""
        
        for i, word in enumerate(sequence):
            if context in conditional_probs[word]:
                prob = conditional_probs[word][context]
            else:
                prob = 0.01  # Small probability for unseen contexts
            
            print(f"P({word}|{context if context else 'START'}) = {prob:.3f}")
            total_prob *= prob
            context = word
        
        print(f"\nTotal probability: {total_prob:.6f}")
        print(f"Log probability: {math.log(total_prob):.6f}")
        
        # Demonstrate with different sequences
        sequences = [
            ['the', 'cat', 'sat'],
            ['the', 'cat', 'sat', 'on'],
            ['the', 'cat', 'sat', 'on', 'mat']
        ]
        
        print("\n**Probability Comparison:**")
        print("Sequence\t\tProbability\tLog Probability")
        print("-" * 50)
        
        for seq in sequences:
            prob = 1.0
            context = ""
            
            for word in seq:
                if context in conditional_probs[word]:
                    prob *= conditional_probs[word][context]
                else:
                    prob *= 0.01
                context = word
            
            log_prob = math.log(prob)
            print(f"{' '.join(seq):15}\t{prob:.6f}\t{log_prob:.6f}")
        
        return {
            'vocabulary': vocabulary,
            'conditional_probs': conditional_probs,
            'sequences': sequences
        }
    
    @staticmethod
    def analyze_markov_property():
        """Analyze Markov property in language modeling"""
        
        print("""
        **Markov Property in Language Modeling:**
        
        **Definition:**
        A sequence has the k-th order Markov property if:
        P(x·µ¢|x‚ÇÅ,...,x·µ¢‚Çã‚ÇÅ) = P(x·µ¢|x·µ¢‚Çã‚Çñ,...,x·µ¢‚Çã‚ÇÅ)
        
        **Implications:**
        - Only the last k words matter for predicting the next word
        - Reduces computational complexity from O(n) to O(k)
        - Trade-off between context length and model size
        
        **Examples:**
        - k=1 (First-order): P(x·µ¢|x·µ¢‚Çã‚ÇÅ) - only previous word matters
        - k=2 (Second-order): P(x·µ¢|x·µ¢‚Çã‚ÇÇ,x·µ¢‚Çã‚ÇÅ) - last 2 words matter
        - k=‚àû (Full context): P(x·µ¢|x‚ÇÅ,...,x·µ¢‚Çã‚ÇÅ) - all previous words matter
        """)
        
        # Demonstrate Markov property with n-gram models
        def create_ngram_model(text: str, n: int) -> Dict[str, Dict[str, float]]:
            """Create n-gram language model"""
            words = text.split()
            ngrams = {}
            
            for i in range(len(words) - n + 1):
                context = ' '.join(words[i:i+n-1])
                next_word = words[i+n-1]
                
                if context not in ngrams:
                    ngrams[context] = {}
                
                if next_word not in ngrams[context]:
                    ngrams[context][next_word] = 0
                
                ngrams[context][next_word] += 1
            
            # Convert counts to probabilities
            for context in ngrams:
                total = sum(ngrams[context].values())
                for word in ngrams[context]:
                    ngrams[context][word] /= total
            
            return ngrams
        
        # Sample text
        sample_text = "the cat sat on the mat the cat ran fast the dog barked loud"
        
        # Create different order n-gram models
        unigram_model = create_ngram_model(sample_text, 1)
        bigram_model = create_ngram_model(sample_text, 2)
        trigram_model = create_ngram_model(sample_text, 3)
        
        print("\n**N-gram Model Comparison:**")
        print(f"Sample text: {sample_text}")
        
        print("\n**Unigram Model (k=0):**")
        for word, prob in unigram_model[''].items():
            print(f"  P({word}) = {prob:.3f}")
        
        print("\n**Bigram Model (k=1):**")
        for context, probs in bigram_model.items():
            print(f"  Context: '{context}'")
            for word, prob in probs.items():
                print(f"    P({word}|{context}) = {prob:.3f}")
        
        print("\n**Trigram Model (k=2):**")
        for context, probs in trigram_model.items():
            print(f"  Context: '{context}'")
            for word, prob in probs.items():
                print(f"    P({word}|{context}) = {prob:.3f}")
        
        return {
            'unigram': unigram_model,
            'bigram': bigram_model,
            'trigram': trigram_model
        }
    
    @staticmethod
    def entropy_analysis():
        """Analyze entropy and information theory in language modeling"""
        
        print("""
        **Entropy and Information Theory:**
        
        **Entropy H(X):**
        - Measures uncertainty in a random variable
        - H(X) = -Œ£·µ¢ P(x·µ¢) log‚ÇÇ P(x·µ¢)
        - Higher entropy = more uncertainty
        
        **Cross-entropy H(P,Q):**
        - Measures difference between true distribution P and predicted Q
        - H(P,Q) = -Œ£·µ¢ P(x·µ¢) log‚ÇÇ Q(x·µ¢)
        - Used as loss function in language modeling
        
        **Perplexity:**
        - Perplexity = 2^H(P,Q)
        - Lower perplexity = better model
        - Perplexity = 1 means perfect prediction
        """)
        
        # Demonstrate entropy calculation
        def calculate_entropy(probabilities: List[float]) -> float:
            """Calculate entropy of probability distribution"""
            entropy = 0.0
            for p in probabilities:
                if p > 0:
                    entropy -= p * math.log2(p)
            return entropy
        
        def calculate_cross_entropy(true_probs: List[float], pred_probs: List[float]) -> float:
            """Calculate cross-entropy between true and predicted distributions"""
            cross_entropy = 0.0
            for p_true, p_pred in zip(true_probs, pred_probs):
                if p_true > 0 and p_pred > 0:
                    cross_entropy -= p_true * math.log2(p_pred)
            return cross_entropy
        
        # Example: Different probability distributions
        distributions = {
            'Uniform': [0.25, 0.25, 0.25, 0.25],
            'Skewed': [0.7, 0.2, 0.08, 0.02],
            'Deterministic': [1.0, 0.0, 0.0, 0.0]
        }
        
        print("\n**Entropy Analysis:**")
        print("Distribution\tEntropy\tPerplexity")
        print("-" * 40)
        
        for name, probs in distributions.items():
            entropy = calculate_entropy(probs)
            perplexity = 2**entropy
            print(f"{name:15}\t{entropy:.3f}\t{perplexity:.3f}")
        
        # Demonstrate cross-entropy
        true_dist = [0.5, 0.3, 0.2]
        perfect_pred = [0.5, 0.3, 0.2]
        poor_pred = [0.1, 0.1, 0.8]
        
        print("\n**Cross-Entropy Analysis:**")
        print("Prediction\tCross-Entropy\tPerplexity")
        print("-" * 45)
        
        for name, pred in [('Perfect', perfect_pred), ('Poor', poor_pred)]:
            cross_ent = calculate_cross_entropy(true_dist, pred)
            perplexity = 2**cross_ent
            print(f"{name:15}\t{cross_ent:.3f}\t\t{perplexity:.3f}")
        
        return distributions

# Demonstrate language modeling theory
lm_theory = LanguageModelingTheory()
lm_theory.explain_chain_rule()

# Demonstrate chain rule
chain_rule_results = lm_theory.demonstrate_chain_rule()

# Analyze Markov property
markov_results = lm_theory.analyze_markov_property()

# Analyze entropy
entropy_results = lm_theory.entropy_analysis()
```

**2. Scaling Laws Theory:**
```python
class ScalingLawsTheory:
    """Theoretical framework cho scaling laws in language models"""
    
    @staticmethod
    def explain_scaling_laws():
        """Explain scaling laws mathematically"""
        print("""
        **Scaling Laws in Language Models:**
        
        **Chinchilla Scaling Laws (Hoffmann et al., 2022):**
        
        For optimal performance, models should follow:
        
        N_opt = 20 √ó D_opt
        
        Where:
        - N_opt: Optimal number of parameters
        - D_opt: Optimal number of training tokens
        
        **Loss Scaling:**
        
        L(N,D) = L_‚àû + A √ó (N^Œ± √ó D^Œ≤)^(-1)
        
        Where:
        - L_‚àû: Irreducible loss (Bayes error)
        - A: Scaling coefficient
        - Œ±, Œ≤: Scaling exponents (typically Œ± ‚âà 0.5, Œ≤ ‚âà 0.5)
        - N: Number of parameters
        - D: Number of training tokens
        
        **Compute Optimal Scaling:**
        
        C_opt = 6 √ó N_opt √ó D_opt
        
        Where C_opt is the optimal compute budget.
        """)
    
    @staticmethod
    def demonstrate_scaling_effects():
        """Demonstrate scaling effects with simulations"""
        
        # Parameters for scaling law simulation
        L_inf = 1.0  # Irreducible loss
        A = 100.0    # Scaling coefficient
        alpha = 0.5  # Parameter scaling exponent
        beta = 0.5   # Data scaling exponent
        
        def scaling_law(N: float, D: float) -> float:
            """Calculate loss according to scaling law"""
            return L_inf + A * (N**alpha * D**beta)**(-1)
        
        # Generate scaling data
        N_values = np.logspace(6, 9, 50)  # 1M to 1B parameters
        D_values = np.logspace(7, 10, 50)  # 10M to 10B tokens
        
        # Create meshgrid for 3D plotting
        N_mesh, D_mesh = np.meshgrid(N_values, D_values)
        L_mesh = scaling_law(N_mesh, D_mesh)
        
        # Find optimal scaling line
        optimal_ratio = 20  # N_opt = 20 √ó D_opt
        D_opt = np.logspace(7, 10, 100)
        N_opt = optimal_ratio * D_opt
        L_opt = scaling_law(N_opt, D_opt)
        
        # Visualization
        fig = plt.figure(figsize=(15, 10))
        
        # 3D surface plot
        ax1 = fig.add_subplot(2, 2, 1, projection='3d')
        surface = ax1.plot_surface(np.log10(N_mesh), np.log10(D_mesh), L_mesh, 
                                 cmap='viridis', alpha=0.8)
        ax1.plot(np.log10(N_opt), np.log10(D_opt), L_opt, 'r-', linewidth=3, label='Optimal Scaling')
        ax1.set_xlabel('log‚ÇÅ‚ÇÄ(Parameters)')
        ax1.set_ylabel('log‚ÇÅ‚ÇÄ(Tokens)')
        ax1.set_zlabel('Loss')
        ax1.set_title('Scaling Law Surface')
        ax1.legend()
        
        # 2D contour plot
        ax2 = fig.add_subplot(2, 2, 2)
        contour = ax2.contour(np.log10(N_mesh), np.log10(D_mesh), L_mesh, levels=20)
        ax2.plot(np.log10(N_opt), np.log10(D_opt), 'r-', linewidth=3, label='Optimal Scaling')
        ax2.clabel(contour, inline=True, fontsize=8)
        ax2.set_xlabel('log‚ÇÅ‚ÇÄ(Parameters)')
        ax2.set_ylabel('log‚ÇÅ‚ÇÄ(Tokens)')
        ax2.set_title('Loss Contours')
        ax2.legend()
        
        # Loss vs Parameters (fixed data)
        ax3 = fig.add_subplot(2, 2, 3)
        fixed_D = 1e9  # 1B tokens
        L_vs_N = scaling_law(N_values, fixed_D)
        ax3.loglog(N_values, L_vs_N, 'b-', linewidth=2, label=f'D = {fixed_D:.0e}')
        ax3.set_xlabel('Parameters (N)')
        ax3.set_ylabel('Loss')
        ax3.set_title('Loss vs Parameters (Fixed Data)')
        ax3.grid(True)
        ax3.legend()
        
        # Loss vs Data (fixed parameters)
        ax4 = fig.add_subplot(2, 2, 4)
        fixed_N = 1e8  # 100M parameters
        L_vs_D = scaling_law(fixed_N, D_values)
        ax4.loglog(D_values, L_vs_D, 'g-', linewidth=2, label=f'N = {fixed_N:.0e}')
        ax4.set_xlabel('Training Tokens (D)')
        ax4.set_ylabel('Loss')
        ax4.set_title('Loss vs Data (Fixed Parameters)')
        ax4.grid(True)
        ax4.legend()
        
        plt.tight_layout()
        plt.show()
        
        # Analyze optimal scaling
        print("\n**Optimal Scaling Analysis:**")
        print("D (tokens)\tN (parameters)\tRatio\tLoss")
        print("-" * 50)
        
        for i in range(0, len(D_opt), 20):
            print(f"{D_opt[i]:.1e}\t{N_opt[i]:.1e}\t{N_opt[i]/D_opt[i]:.1f}\t{L_opt[i]:.4f}")
        
        return {
            'N_values': N_values,
            'D_values': D_values,
            'L_mesh': L_mesh,
            'N_opt': N_opt,
            'D_opt': D_opt,
            'L_opt': L_opt
        }
    
    @staticmethod
    def analyze_compute_efficiency():
        """Analyze compute efficiency of different scaling strategies"""
        
        print("""
        **Compute Efficiency Analysis:**
        
        **Compute Budget:**
        C = 6 √ó N √ó D (approximate FLOPs)
        
        **Efficiency Strategies:**
        1. **Chinchilla Optimal**: N = 20D (balanced)
        2. **Parameter-Heavy**: N >> 20D (over-parameterized)
        3. **Data-Heavy**: N << 20D (under-parameterized)
        """)
        
        # Define different scaling strategies
        D_base = 1e9  # 1B tokens
        
        strategies = {
            'Chinchilla Optimal': 20,
            'Parameter-Heavy (2x)': 40,
            'Parameter-Heavy (5x)': 100,
            'Data-Heavy (0.5x)': 10,
            'Data-Heavy (0.2x)': 4
        }
        
        # Calculate compute and loss for each strategy
        results = {}
        
        for name, ratio in strategies.items():
            N = ratio * D_base
            D = D_base
            C = 6 * N * D
            L = 1.0 + 100.0 * (N**0.5 * D**0.5)**(-1)
            
            results[name] = {
                'N': N,
                'D': D,
                'C': C,
                'L': L,
                'ratio': ratio
            }
        
        # Display results
        print("\n**Scaling Strategy Comparison:**")
        print("Strategy\t\tN\t\tD\t\tC\t\tLoss\tRatio")
        print("-" * 80)
        
        for name, result in results.items():
            print(f"{name:20}\t{result['N']:.1e}\t{result['D']:.1e}\t{result['C']:.1e}\t{result['L']:.4f}\t{result['ratio']:.1f}")
        
        # Find most efficient strategy
        best_strategy = min(results.keys(), key=lambda x: results[x]['L'])
        print(f"\n**Most Efficient Strategy:** {best_strategy}")
        print(f"Loss: {results[best_strategy]['L']:.4f}")
        
        return results

# Demonstrate scaling laws theory
scaling_theory = ScalingLawsTheory()
scaling_theory.explain_scaling_laws()

# Demonstrate scaling effects
scaling_results = scaling_theory.demonstrate_scaling_effects()

# Analyze compute efficiency
efficiency_results = scaling_theory.analyze_compute_efficiency()
```

**3. Attention Mechanism Theory:**
```python
class AttentionMechanismTheory:
    """Theoretical framework cho attention mechanisms"""
    
    @staticmethod
    def explain_attention_mathematics():
        """Explain attention mechanism mathematically"""
        print("""
        **Attention Mechanism Mathematics:**
        
        **Query-Key-Value Framework:**
        
        Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V
        
        Where:
        - Q: Query matrix (n_queries √ó d_k)
        - K: Key matrix (n_keys √ó d_k)
        - V: Value matrix (n_keys √ó d_v)
        - d_k: Key dimension
        - ‚àöd_k: Scaling factor (prevents softmax saturation)
        
        **Multi-Head Attention:**
        
        MultiHead(Q,K,V) = Concat(head‚ÇÅ,...,head‚Çï)W^O
        
        Where each head is:
        head·µ¢ = Attention(QW·µ¢^Q, KW·µ¢^K, VW·µ¢^V)
        
        **Positional Encoding:**
        
        PE(pos,2i) = sin(pos/10000^(2i/d_model))
        PE(pos,2i+1) = cos(pos/10000^(2i/d_model))
        
        This provides position information to the model.
        """)
    
    @staticmethod
    def implement_attention_mechanism():
        """Implement attention mechanism step by step"""
        
        import torch
        import torch.nn.functional as F
        
        class AttentionImplementation:
            def __init__(self, d_model: int, d_k: int, d_v: int):
                self.d_model = d_model
                self.d_k = d_k
                self.d_v = d_v
                
                # Linear projections
                self.W_q = torch.randn(d_model, d_k)
                self.W_k = torch.randn(d_model, d_k)
                self.W_v = torch.randn(d_model, d_v)
                self.W_o = torch.randn(d_v, d_model)
            
            def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
                """Compute scaled dot-product attention"""
                
                # Step 1: Compute attention scores
                scores = torch.matmul(Q, K.transpose(-2, -1))
                
                # Step 2: Scale scores
                scores = scores / math.sqrt(self.d_k)
                
                # Step 3: Apply softmax
                attention_weights = F.softmax(scores, dim=-1)
                
                # Step 4: Apply attention to values
                output = torch.matmul(attention_weights, V)
                
                return output, attention_weights
            
            def forward(self, x: torch.Tensor) -> torch.Tensor:
                """Forward pass through attention mechanism"""
                
                # Project inputs to Q, K, V
                Q = torch.matmul(x, self.W_q)
                K = torch.matmul(x, self.W_k)
                V = torch.matmul(x, self.W_v)
                
                # Apply attention
                attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V)
                
                # Project output
                output = torch.matmul(attention_output, self.W_o)
                
                return output, attention_weights
        
        # Test attention mechanism
        d_model, d_k, d_v = 64, 32, 32
        seq_len = 10
        
        attention = AttentionImplementation(d_model, d_k, d_v)
        
        # Create input sequence
        x = torch.randn(seq_len, d_model)
        
        # Apply attention
        output, weights = attention.forward(x)
        
        print(f"Input shape: {x.shape}")
        print(f"Output shape: {output.shape}")
        print(f"Attention weights shape: {weights.shape}")
        
        # Visualize attention weights
        plt.figure(figsize=(10, 8))
        plt.imshow(weights.detach().numpy(), cmap='viridis')
        plt.colorbar()
        plt.title('Attention Weights Matrix')
        plt.xlabel('Key Position')
        plt.ylabel('Query Position')
        plt.show()
        
        return attention, x, output, weights

# Demonstrate attention theory
attention_theory = AttentionMechanismTheory()
attention_theory.explain_attention_mathematics()

# Implement attention mechanism
attention_model, input_seq, output_seq, attention_weights = attention_theory.implement_attention_mechanism()
```

**T√†i li·ªáu tham kh·∫£o chuy√™n s√¢u:**
- **Language Modeling**: [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- **Scaling Laws**: [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)
- **Attention Mechanisms**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- **Information Theory**: [Elements of Information Theory](https://www.wiley.com/en-us/Elements+of+Information+Theory,+2nd+Edition-p-9780471241959)

#### Perplexity - ƒê·ªô ph·ª©c t·∫°p

**C√¥ng th·ª©c t√≠nh perplexity**:
```
Perplexity = exp(-(1/n)Œ£·µ¢ log P(x·µ¢|x‚ÇÅ,...,x·µ¢‚Çã‚ÇÅ))
```

**Gi·∫£i th√≠ch k√Ω hi·ªáu:**
- **Perplexity**: ƒê·ªô ph·ª©c t·∫°p c·ªßa m√¥ h√¨nh (c√†ng th·∫•p c√†ng t·ªët)
- **exp()**: H√†m m≈© t·ª± nhi√™n
- **n**: S·ªë l∆∞·ª£ng t·ª´ trong chu·ªói
- **log P(x·µ¢|x‚ÇÅ,...,x·µ¢‚Çã‚ÇÅ)**: Logarit c·ªßa x√°c su·∫•t d·ª± ƒëo√°n

**√ù nghƒ©a c·ªßa Perplexity**:
- **Perplexity = 1**: M√¥ h√¨nh d·ª± ƒëo√°n ho√†n h·∫£o
- **Perplexity = 2**: M√¥ h√¨nh d·ª± ƒëo√°n nh∆∞ random guessing
- **Perplexity c√†ng th·∫•p**: M√¥ h√¨nh c√†ng hi·ªÉu ng√¥n ng·ªØ t·ªët

#### Cross-entropy Loss - H√†m m·∫•t m√°t entropy ch√©o

**C√¥ng th·ª©c cross-entropy loss**:
```
L = -(1/n)Œ£·µ¢ log P(x·µ¢|x‚ÇÅ,...,x·µ¢‚Çã‚ÇÅ)
```

**Gi·∫£i th√≠ch k√Ω hi·ªáu:**
- **L**: Loss value (gi√° tr·ªã m·∫•t m√°t)
- **n**: S·ªë l∆∞·ª£ng t·ª´ trong batch
- **log P(x·µ¢|x‚ÇÅ,...,x·µ¢‚Çã‚ÇÅ)**: Logarit c·ªßa x√°c su·∫•t d·ª± ƒëo√°n ƒë√∫ng

**M·ªëi quan h·ªá v·ªõi Perplexity**:
```
Perplexity = exp(L)
```

**√ù nghƒ©a th·ª±c t·∫ø**:
- Loss c√†ng th·∫•p, m√¥ h√¨nh c√†ng d·ª± ƒëo√°n ch√≠nh x√°c
- Perplexity c√†ng th·∫•p, m√¥ h√¨nh c√†ng hi·ªÉu ng√¥n ng·ªØ t·ªët
- C·∫£ hai metric ƒë·ªÅu ƒëo l∆∞·ªùng ch·∫•t l∆∞·ª£ng c·ªßa language model

#### Implementation chi ti·∫øt

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class SimpleLanguageModel(nn.Module):
    """
    Language model ƒë∆°n gi·∫£n s·ª≠ d·ª•ng LSTM
    
    Architecture:
    Embedding ‚Üí LSTM ‚Üí Linear ‚Üí Softmax
    """
    
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1):
        super().__init__()
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # Embedding layer: chuy·ªÉn t·ª´ indices th√†nh vectors
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # LSTM layer: x·ª≠ l√Ω chu·ªói tu·∫ßn t·ª±
        self.lstm = nn.LSTM(
            embed_dim, 
            hidden_dim, 
            num_layers=num_layers, 
            batch_first=True,
            dropout=0.1 if num_layers > 1 else 0
        )
        
        # Output layer: d·ª± ƒëo√°n t·ª´ ti·∫øp theo
        self.fc = nn.Linear(hidden_dim, vocab_size)
        
        # Dropout ƒë·ªÉ tr√°nh overfitting
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x, hidden=None):
        """
        Forward pass c·ªßa language model
        
        Parameters:
        x (torch.Tensor): Input sequence shape (batch_size, seq_len)
        hidden (tuple): Hidden state t·ª´ LSTM (optional)
        
        Returns:
        tuple: (output, hidden_state)
        """
        batch_size, seq_len = x.size()
        
        # 1. Embedding: chuy·ªÉn t·ª´ indices th√†nh vectors
        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)
        embedded = self.dropout(embedded)
        
        # 2. LSTM: x·ª≠ l√Ω chu·ªói tu·∫ßn t·ª±
        lstm_out, hidden = self.lstm(embedded, hidden)
        # lstm_out: (batch_size, seq_len, hidden_dim)
        # hidden: (h_n, c_n) v·ªõi h_n, c_n: (num_layers, batch_size, hidden_dim)
        
        # 3. Linear transformation: d·ª± ƒëo√°n t·ª´ ti·∫øp theo
        output = self.fc(lstm_out)  # (batch_size, seq_len, vocab_size)
        
        return output, hidden
    
    def generate(self, start_tokens, max_length, temperature=1.0, top_k=50):
        """
        Generate text t·ª´ start tokens
        
        Parameters:
        start_tokens (list): Danh s√°ch tokens b·∫Øt ƒë·∫ßu
        max_length (int): ƒê·ªô d√†i t·ªëi ƒëa c·ªßa text ƒë∆∞·ª£c generate
        temperature (float): ƒêi·ªÅu ch·ªânh randomness (c√†ng th·∫•p c√†ng deterministic)
        top_k (int): Ch·ªâ xem x√©t top-k tokens c√≥ x√°c su·∫•t cao nh·∫•t
        
        Returns:
        list: Generated tokens
        """
        self.eval()
        generated = start_tokens.copy()
        
        with torch.no_grad():
            # Chuy·ªÉn start tokens th√†nh tensor
            x = torch.tensor([start_tokens], dtype=torch.long)
            
            for _ in range(max_length - len(start_tokens)):
                # Forward pass
                output, _ = self.forward(x)
                
                # L·∫•y logits c·ªßa token cu·ªëi c√πng
                logits = output[0, -1, :]  # (vocab_size,)
                
                # Apply temperature
                logits = logits / temperature
                
                # Top-k sampling
                if top_k > 0:
                    top_k_logits, top_k_indices = torch.topk(logits, top_k)
                    logits = torch.full_like(logits, float('-inf'))
                    logits[top_k_indices] = top_k_logits
                
                # Softmax ƒë·ªÉ c√≥ x√°c su·∫•t
                probs = F.softmax(logits, dim=-1)
                
                # Sample t·ª´ distribution
                next_token = torch.multinomial(probs, 1).item()
                
                # Th√™m token m·ªõi v√†o sequence
                generated.append(next_token)
                x = torch.cat([x, torch.tensor([[next_token]], dtype=torch.long)], dim=1)
        
        return generated

def train_step(model, optimizer, criterion, data, target):
    """
    Training step cho language model
    
    Parameters:
    model: Language model
    optimizer: Optimizer (Adam, SGD, etc.)
    criterion: Loss function (CrossEntropyLoss)
    data (torch.Tensor): Input sequence
    target (torch.Tensor): Target sequence
    
    Returns:
    float: Loss value
    """
    # Zero gradients
    optimizer.zero_grad()
    
    # Forward pass
    output, _ = model(data)
    
    # Reshape output v√† target cho loss calculation
    # output: (batch_size, seq_len, vocab_size) ‚Üí (batch_size * seq_len, vocab_size)
    # target: (batch_size, seq_len) ‚Üí (batch_size * seq_len)
    output_reshaped = output.view(-1, output.size(-1))
    target_reshaped = target.view(-1)
    
    # Calculate loss
    loss = criterion(output_reshaped, target_reshaped)
    
    # Backward pass
    loss.backward()
    
    # Gradient clipping ƒë·ªÉ tr√°nh gradient explosion
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    # Update parameters
    optimizer.step()
    
    return loss.item()

def calculate_perplexity(model, data_loader, criterion):
    """
    T√≠nh perplexity c·ªßa model tr√™n dataset
    
    Parameters:
    model: Language model
    data_loader: DataLoader ch·ª©a test data
    criterion: Loss function
    
    Returns:
    float: Perplexity value
    """
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(data_loader):
            output, _ = model(data)
            
            # Reshape cho loss calculation
            output_reshaped = output.view(-1, output.size(-1))
            target_reshaped = target.view(-1)
            
            # Calculate loss
            loss = criterion(output_reshaped, target_reshaped)
            
            # Accumulate loss v√† count tokens
            total_loss += loss.item() * target_reshaped.size(0)
            total_tokens += target_reshaped.size(0)
    
    # T√≠nh average loss
    avg_loss = total_loss / total_tokens
    
    # T√≠nh perplexity
    perplexity = np.exp(avg_loss)
    
    return perplexity, avg_loss

# V√≠ d·ª• s·ª≠ d·ª•ng
def demonstrate_language_model():
    """
    Minh h·ªça c√°ch s·ª≠ d·ª•ng language model
    """
    
    # Hyperparameters
    vocab_size = 10000
    embed_dim = 256
    hidden_dim = 512
    num_layers = 2
    
    # Kh·ªüi t·∫°o model
    model = SimpleLanguageModel(vocab_size, embed_dim, hidden_dim, num_layers)
    
    print("ü§ñ LANGUAGE MODEL DEMONSTRATION")
    print("=" * 50)
    print(f"Vocabulary size: {vocab_size:,}")
    print(f"Embedding dimension: {embed_dim}")
    print(f"Hidden dimension: {hidden_dim}")
    print(f"Number of LSTM layers: {num_layers}")
    print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # T·∫°o d·ªØ li·ªáu m·∫´u
    batch_size = 4
    seq_len = 10
    
    # Random data (trong th·ª±c t·∫ø s·∫Ω l√† text ƒë√£ ƒë∆∞·ª£c tokenize)
    data = torch.randint(0, vocab_size, (batch_size, seq_len))
    target = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    print(f"\nüìä Sample Data:")
    print(f"Input shape: {data.shape}")
    print(f"Target shape: {target.shape}")
    
    # Test forward pass
    print(f"\n‚û°Ô∏è Forward Pass Test:")
    output, hidden = model(data)
    print(f"Output shape: {output.shape}")
    print(f"Hidden state shape: {hidden[0].shape}")  # h_n shape
    
    # Test training step
    print(f"\nüéØ Training Step Test:")
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    loss = train_step(model, optimizer, criterion, data, target)
    print(f"Training loss: {loss:.4f}")
    
    # Test text generation
    print(f"\n‚úçÔ∏è Text Generation Test:")
    start_tokens = [1, 2, 3]  # Start tokens
    generated = model.generate(start_tokens, max_length=10, temperature=0.8)
    print(f"Generated tokens: {generated}")
    
    return model

# V√≠ d·ª• s·ª≠ d·ª•ng
# model = demonstrate_language_model()
```

**Gi·∫£i th√≠ch c√°c kh√°i ni·ªám:**
- **Embedding**: Chuy·ªÉn ƒë·ªïi t·ª´ indices th√†nh vectors c√≥ √Ω nghƒ©a
- **LSTM**: Long Short-Term Memory, x·ª≠ l√Ω chu·ªói tu·∫ßn t·ª±
- **Hidden State**: Tr·∫°ng th√°i ·∫©n c·ªßa LSTM, ch·ª©a th√¥ng tin t·ª´ qu√° kh·ª©
- **Top-k Sampling**: Ch·ªâ xem x√©t k tokens c√≥ x√°c su·∫•t cao nh·∫•t khi generate

### 1.2 Scaling Laws - Quy lu·∫≠t m·ªü r·ªông

> **Scaling Laws** l√† c√°c quy lu·∫≠t to√°n h·ªçc m√¥ t·∫£ m·ªëi quan h·ªá gi·ªØa k√≠ch th∆∞·ªõc m√¥ h√¨nh, l∆∞·ª£ng d·ªØ li·ªáu training v√† hi·ªáu su·∫•t.

#### Chinchilla Scaling - Quy lu·∫≠t Chinchilla

**C√¥ng th·ª©c Chinchilla**:
```
Optimal model size: N = 20D
Optimal training tokens: D = 1.4 √ó 10‚Å∂
```

**Gi·∫£i th√≠ch k√Ω hi·ªáu:**
- **N**: S·ªë l∆∞·ª£ng parameters c·ªßa m√¥ h√¨nh
- **D**: S·ªë l∆∞·ª£ng training tokens (t·ª´)
- **20**: H·ªá s·ªë t·ª∑ l·ªá t·ªëi ∆∞u
- **1.4 √ó 10‚Å∂**: S·ªë tokens t·ªëi ∆∞u cho training

**√ù nghƒ©a th·ª±c t·∫ø**:
- M√¥ h√¨nh c√†ng l·ªõn c·∫ßn c√†ng nhi·ªÅu d·ªØ li·ªáu training
- T·ª∑ l·ªá 20:1 gi·ªØa parameters v√† tokens l√† t·ªëi ∆∞u
- Over-parameterization (m√¥ h√¨nh qu√° l·ªõn) kh√¥ng hi·ªáu qu·∫£

#### Performance Scaling - M·ªü r·ªông hi·ªáu su·∫•t

**C√¥ng th·ª©c t·ªïng qu√°t**:
```
Loss = A + B/N^Œ± + C/D^Œ≤
```

**Gi·∫£i th√≠ch k√Ω hi·ªáu:**
- **Loss**: H√†m m·∫•t m√°t c·ªßa m√¥ h√¨nh
- **A**: Loss c∆° b·∫£n kh√¥ng th·ªÉ gi·∫£m ƒë∆∞·ª£c
- **B/N^Œ±**: Th√†nh ph·∫ßn ph·ª• thu·ªôc v√†o k√≠ch th∆∞·ªõc m√¥ h√¨nh
- **C/D^Œ≤**: Th√†nh ph·∫ßn ph·ª• thu·ªôc v√†o l∆∞·ª£ng d·ªØ li·ªáu
- **Œ±, Œ≤**: C√°c h·ªá s·ªë m≈© (th∆∞·ªùng Œ± ‚âà 0.1, Œ≤ ‚âà 0.5)

**√ù nghƒ©a th·ª±c t·∫ø**:
- Loss gi·∫£m khi tƒÉng k√≠ch th∆∞·ªõc m√¥ h√¨nh ho·∫∑c d·ªØ li·ªáu
- C√≥ gi·ªõi h·∫°n v·ªÅ hi·ªáu su·∫•t c√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c
- C·∫ßn c√¢n b·∫±ng gi·ªØa model size v√† data size

#### Empirical Findings - Nh·ªØng ph√°t hi·ªán th·ª±c nghi·ªám

**C√°c nguy√™n l√Ω quan tr·ªçng**:

1. **Model size v√† training data ph·∫£i c√¢n b·∫±ng**
   - M√¥ h√¨nh l·ªõn c·∫ßn nhi·ªÅu d·ªØ li·ªáu
   - M√¥ h√¨nh nh·ªè v·ªõi nhi·ªÅu d·ªØ li·ªáu c√≥ th·ªÉ overfit

2. **Over-parameterization kh√¥ng hi·ªáu qu·∫£**
   - TƒÉng parameters qu√° m·ª©c kh√¥ng c·∫£i thi·ªán hi·ªáu su·∫•t
   - T·ªën k√©m v·ªÅ compute v√† memory

3. **Data quality quan tr·ªçng h∆°n quantity**
   - 1M tokens ch·∫•t l∆∞·ª£ng cao t·ªët h∆°n 10M tokens ch·∫•t l∆∞·ª£ng th·∫•p
   - C·∫ßn t·∫≠p trung v√†o vi·ªác l√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu

#### Implementation v√† Visualization

```python
import matplotlib.pyplot as plt
import numpy as np

def demonstrate_scaling_laws():
    """
    Minh h·ªça scaling laws trong language models
    """
    
    print("üìä SCALING LAWS DEMONSTRATION")
    print("=" * 50)
    
    # Parameters cho scaling laws
    A = 1.0      # Base loss
    B = 1000     # Model size coefficient
    C = 500      # Data size coefficient
    alpha = 0.1  # Model size exponent
    beta = 0.5   # Data size exponent
    
    # T·∫°o data cho visualization
    model_sizes = np.logspace(6, 9, 100)  # 1M to 1B parameters
    data_sizes = np.logspace(5, 7, 100)   # 100K to 10M tokens
    
    # T√≠nh loss theo model size (v·ªõi data size c·ªë ƒë·ªãnh)
    fixed_data_size = 1e6  # 1M tokens
    loss_vs_model = A + B / (model_sizes ** alpha) + C / (fixed_data_size ** beta)
    
    # T√≠nh loss theo data size (v·ªõi model size c·ªë ƒë·ªãnh)
    fixed_model_size = 1e8  # 100M parameters
    loss_vs_data = A + B / (fixed_model_size ** alpha) + C / (data_sizes ** beta)
    
    # Chinchilla optimal line
    optimal_model_sizes = np.logspace(6, 9, 50)
    optimal_data_sizes = optimal_model_sizes / 20
    
    # Visualization
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Loss vs Model Size
    axes[0, 0].loglog(model_sizes, loss_vs_model, 'b-', linewidth=2)
    axes[0, 0].set_xlabel('Model Size (Parameters)')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Loss vs Model Size (Fixed Data: 1M tokens)')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Loss vs Data Size
    axes[0, 1].loglog(data_sizes, loss_vs_data, 'r-', linewidth=2)
    axes[0, 1].set_xlabel('Data Size (Tokens)')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].set_title('Loss vs Data Size (Fixed Model: 100M parameters)')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Chinchilla Optimal Line
    axes[1, 0].loglog(optimal_model_sizes, optimal_data_sizes, 'g-', linewidth=2, label='Optimal N=20D')
    axes[1, 0].set_xlabel('Model Size (Parameters)')
    axes[1, 0].set_ylabel('Data Size (Tokens)')
    axes[1, 0].set_title('Chinchilla Optimal Scaling (N=20D)')
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].legend()
    
    # 4. 3D Surface Plot
    X, Y = np.meshgrid(model_sizes, data_sizes)
    Z = A + B / (X ** alpha) + C / (Y ** beta)
    
    surf = axes[1, 1].contourf(np.log10(X), np.log10(Y), Z, levels=20, cmap='viridis')
    axes[1, 1].set_xlabel('log10(Model Size)')
    axes[1, 1].set_ylabel('log10(Data Size)')
    axes[1, 1].set_title('Loss Surface (Model Size vs Data Size)')
    plt.colorbar(surf, ax=axes[1, 1], label='Loss')
    
    plt.tight_layout()
    plt.show()
    
    # T√≠nh to√°n v√≠ d·ª• c·ª• th·ªÉ
    print(f"\nüîç SCALING ANALYSIS:")
    print(f"Base loss (A): {A:.2f}")
    print(f"Model size coefficient (B): {B:,}")
    print(f"Data size coefficient (C): {C:,}")
    print(f"Alpha (model size exponent): {alpha}")
    print(f"Beta (data size exponent): {beta}")
    
    # V√≠ d·ª• v·ªõi model 100M parameters
    model_size_example = 1e8
    data_size_example = 1e6
    
    loss_example = A + B / (model_size_example ** alpha) + C / (data_size_example ** beta)
    
    print(f"\nüìä EXAMPLE CALCULATION:")
    print(f"Model size: {model_size_example:,} parameters")
    print(f"Data size: {data_size_example:,} tokens")
    print(f"Calculated loss: {loss_example:.4f}")
    
    # Chinchilla optimal cho model size n√†y
    optimal_data = model_size_example / 20
    print(f"Chinchilla optimal data size: {optimal_data:,.0f} tokens")
    
    return {
        'model_sizes': model_sizes,
        'data_sizes': data_sizes,
        'loss_vs_model': loss_vs_model,
        'loss_vs_data': loss_vs_data
    }

# V√≠ d·ª• s·ª≠ d·ª•ng
# scaling_data = demonstrate_scaling_laws()
```

**Gi·∫£i th√≠ch k·∫øt qu·∫£:**
- **Loss vs Model Size**: Loss gi·∫£m khi tƒÉng k√≠ch th∆∞·ªõc m√¥ h√¨nh, nh∆∞ng c√≥ gi·ªõi h·∫°n
- **Loss vs Data Size**: Loss gi·∫£m khi tƒÉng d·ªØ li·ªáu training
- **Chinchilla Optimal**: ƒê∆∞·ªùng th·∫≥ng N=20D cho th·∫•y t·ª∑ l·ªá t·ªëi ∆∞u
- **Loss Surface**: B·ªÅ m·∫∑t 3D cho th·∫•y m·ªëi quan h·ªá gi·ªØa c·∫£ hai y·∫øu t·ªë

## üìö T√†i li·ªáu tham kh·∫£o

### L√Ω thuy·∫øt Language Modeling
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2 paper
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) - Scaling laws paper

### Scaling v√† Optimization
- [Chinchilla: Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) - Chinchilla paper
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) - Original scaling laws

### Implementation
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/) - Th∆∞ vi·ªán transformers
- [PyTorch Language Modeling Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) - PyTorch tutorial

## üéØ B√†i t·∫≠p th·ª±c h√†nh

1. **Language Model**: Implement language model ƒë∆°n gi·∫£n v·ªõi LSTM/Transformer
2. **Scaling Analysis**: Ph√¢n t√≠ch scaling laws tr√™n dataset th·ª±c t·∫ø
3. **Text Generation**: T·∫°o text generation pipeline v·ªõi temperature v√† top-k sampling
4. **Perplexity Calculation**: T√≠nh perplexity tr√™n test set
5. **Model Optimization**: T·ªëi ∆∞u h√≥a hyperparameters cho language model

## üöÄ B∆∞·ªõc ti·∫øp theo

Sau khi ho√†n th√†nh LLMs c∆° b·∫£n, b·∫°n s·∫Ω:
- Hi·ªÉu s√¢u v·ªÅ ki·∫øn tr√∫c Transformer v√† attention mechanisms
- Bi·∫øt c√°ch fine-tune LLMs cho c√°c t√°c v·ª• c·ª• th·ªÉ
- C√≥ th·ªÉ tri·ªÉn khai RAG v√† vector search
- S·∫µn s√†ng h·ªçc advanced techniques nh∆∞ RLHF v√† model compression

---

*Ch√∫c b·∫°n tr·ªü th√†nh LLM Engineer xu·∫•t s·∫Øc! üéâ*


# üß† Deep Learning (DL) - H·ªçc s√¢u v√† m·∫°ng n∆°-ron

> **M·ª•c ti√™u**: Tr·ªü th√†nh chuy√™n gia Deep Learning, hi·ªÉu s√¢u v·ªÅ l√Ω thuy·∫øt m·∫°ng n∆°-ron v√† c√≥ kh·∫£ nƒÉng x√¢y d·ª±ng c√°c m√¥ h√¨nh DL ph·ª©c t·∫°p

## üìö **1. B·∫£ng k√Ω hi·ªáu (Notation)**

### **Neural Networks:**
- **Input**: $\mathbf{x} \in \mathbb{R}^d$ (vector ƒë·∫ßu v√†o)
- **Weight matrix**: $\mathbf{W}^{(l)} \in \mathbb{R}^{n_{l-1} \times n_l}$ (ma tr·∫≠n tr·ªçng s·ªë layer $l$)
- **Bias**: $\mathbf{b}^{(l)} \in \mathbb{R}^{n_l}$ (bias vector layer $l$)
- **Activation**: $\mathbf{a}^{(l)} = \sigma(\mathbf{z}^{(l)})$ (activation output layer $l$)
- **Pre-activation**: $\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}$

### **Forward Pass:**
- **Layer output**: $\mathbf{a}^{(l)} = \sigma(\mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)})$
- **Network output**: $f_\theta(\mathbf{x}) = \mathbf{a}^{(L)}$
- **Parameters**: $\theta = \{\mathbf{W}^{(l)}, \mathbf{b}^{(l)}\}_{l=1}^L$

### **Backpropagation:**
- **Loss gradient**: $\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}$
- **Error signal**: $\delta^{(l)} = \frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(l)}}$
- **Weight update**: $\mathbf{W}^{(l)} \leftarrow \mathbf{W}^{(l)} - \alpha \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}$

### **Activation Functions:**
- **ReLU**: $\sigma(x) = \max(0, x)$
- **Sigmoid**: $\sigma(x) = \frac{1}{1 + e^{-x}}$
- **Tanh**: $\sigma(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
- **Softmax**: $\sigma(\mathbf{x})_i = \frac{e^{x_i}}{\sum_j e^{x_j}}$

### **Loss Functions:**
- **Cross-entropy**: $\mathcal{L} = -\sum_i y_i \log(\hat{y}_i)$
- **MSE**: $\mathcal{L} = \frac{1}{n}\sum_i (y_i - \hat{y}_i)^2$
- **Binary cross-entropy**: $\mathcal{L} = -[y \log(\hat{y}) + (1-y)\log(1-\hat{y})]$

## üìñ **2. Glossary (ƒê·ªãnh nghƒ©a c·ªët l√µi)**

### **Neural Network Components:**
- **Neuron**: ƒê∆°n v·ªã c∆° b·∫£n c·ªßa neural network - nh·∫≠n input, t√≠nh weighted sum, apply activation
- **Layer**: T·∫≠p h·ª£p c√°c neurons c√πng level - input layer, hidden layers, output layer
- **Weight**: Tham s·ªë h·ªçc ƒë∆∞·ª£c - strength c·ªßa connection gi·ªØa neurons
- **Bias**: Tham s·ªë offset - gi√∫p shift activation function

### **Training Concepts:**
- **Forward Pass**: T√≠nh output t·ª´ input qua network
- **Backward Pass**: T√≠nh gradients t·ª´ output v·ªÅ input
- **Backpropagation**: Algorithm ƒë·ªÉ t√≠nh gradients efficiently
- **Gradient Descent**: Optimization algorithm ƒë·ªÉ update parameters

### **Activation Functions:**
- **Linear**: $f(x) = x$ - kh√¥ng c√≥ non-linearity
- **Non-linear**: ReLU, Sigmoid, Tanh - introduce non-linearity
- **Saturation**: Sigmoid/Tanh c√≥ th·ªÉ saturate ‚Üí vanishing gradients
- **Sparsity**: ReLU c√≥ th·ªÉ create sparse representations

### **Optimization:**
- **Learning Rate**: Step size trong gradient descent
- **Momentum**: Accumulate gradients ƒë·ªÉ accelerate convergence
- **Adaptive Learning**: Adam, RMSprop - adjust learning rate automatically
- **Regularization**: Techniques ƒë·ªÉ prevent overfitting

## üìê **3. Th·∫ª thu·∫≠t to√°n - Backpropagation**

### **1. B√†i to√°n & d·ªØ li·ªáu:**
- **B√†i to√°n**: T√≠nh gradients c·ªßa loss function v·ªõi respect to network parameters
- **D·ªØ li·ªáu**: Neural network v·ªõi parameters $\theta$, loss function $\mathcal{L}$
- **·ª®ng d·ª•ng**: Training neural networks, gradient-based optimization

### **2. M√¥ h√¨nh & c√¥ng th·ª©c:**
**Forward Pass:**
$$\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}$$
$$\mathbf{a}^{(l)} = \sigma(\mathbf{z}^{(l)})$$

**Backward Pass:**
$$\delta^{(l)} = \frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(l)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{a}^{(l)}} \odot \sigma'(\mathbf{z}^{(l)})$$

**Weight Gradients:**
$$\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}} = \delta^{(l)} (\mathbf{a}^{(l-1)})^T$$
$$\frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(l)}} = \delta^{(l)}$$

### **3. Loss & m·ª•c ti√™u:**
- **M·ª•c ti√™u**: Compute gradients efficiently ƒë·ªÉ update parameters
- **Loss**: $\mathcal{L}(\theta)$ - loss function c·∫ßn minimize

### **4. T·ªëi ∆∞u ho√° & c·∫≠p nh·∫≠t:**
- **Algorithm**: Chain rule application
- **C·∫≠p nh·∫≠t**: $\theta \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta)$

### **5. Hyperparams:**
- **Learning rate**: $\alpha$ (step size)
- **Batch size**: Number of samples per update
- **Number of epochs**: Training iterations

### **6. ƒê·ªô ph·ª©c t·∫°p:**
- **Time**: $O(L \times n^2)$ v·ªõi $L$ layers, $n$ neurons per layer
- **Space**: $O(L \times n^2)$ cho storing activations v√† gradients

### **7. Metrics ƒë√°nh gi√°:**
- **Gradient norm**: $\|\nabla_\theta \mathcal{L}\|$
- **Training loss**: $\mathcal{L}(\theta)$
- **Validation accuracy**: Performance on validation set
- **Convergence speed**: Rate of loss decrease

### **8. ∆Øu / Nh∆∞·ª£c:**
**∆Øu ƒëi·ªÉm:**
- Computationally efficient
- Automatic differentiation
- Scales to large networks
- Well-established theory

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Vanishing/exploding gradients
- Local minima
- Requires careful initialization
- Sensitive to hyperparameters

### **9. B·∫´y & m·∫πo:**
- **B·∫´y**: Vanishing gradients ‚Üí use ReLU, proper initialization
- **B·∫´y**: Exploding gradients ‚Üí gradient clipping
- **M·∫πo**: Use batch normalization
- **M·∫πo**: Monitor gradient norms

### **10. Pseudocode:**
```python
def backpropagation(network, x, y, loss_function):
    # Forward pass
    activations = forward_pass(network, x)
    
    # Compute loss
    loss = loss_function(activations[-1], y)
    
    # Initialize gradients
    gradients = {}
    
    # Backward pass
    delta = compute_output_gradient(activations[-1], y, loss_function)
    
    for layer in reversed(network.layers):
        # Compute weight gradients
        gradients[layer.weights] = delta @ activations[layer-1].T
        gradients[layer.bias] = delta
        
        # Compute error signal for next layer
        if layer > 0:
            delta = layer.weights.T @ delta * layer.activation_derivative(activations[layer])
    
    return gradients
```

### **11. Code m·∫´u:**
```python
import numpy as np
import matplotlib.pyplot as plt

class NeuralNetwork:
    """Simple Neural Network with Backpropagation"""
    
    def __init__(self, layer_sizes, activation='relu'):
        self.layer_sizes = layer_sizes
        self.activation = activation
        self.weights = []
        self.biases = []
        self.initialize_parameters()
    
    def initialize_parameters(self):
        """Initialize weights and biases"""
        for i in range(len(self.layer_sizes) - 1):
            # He initialization for ReLU
            if self.activation == 'relu':
                w = np.random.randn(self.layer_sizes[i+1], self.layer_sizes[i]) * np.sqrt(2.0 / self.layer_sizes[i])
            else:
                w = np.random.randn(self.layer_sizes[i+1], self.layer_sizes[i]) * 0.01
            
            b = np.zeros((self.layer_sizes[i+1], 1))
            
            self.weights.append(w)
            self.biases.append(b)
    
    def activation_function(self, z, derivative=False):
        """Activation function"""
        if self.activation == 'relu':
            if derivative:
                return np.where(z > 0, 1, 0)
            return np.maximum(0, z)
        elif self.activation == 'sigmoid':
            if derivative:
                s = 1 / (1 + np.exp(-z))
                return s * (1 - s)
            return 1 / (1 + np.exp(-z))
        elif self.activation == 'tanh':
            if derivative:
                return 1 - np.tanh(z)**2
            return np.tanh(z)
    
    def forward_pass(self, X):
        """Forward pass through the network"""
        activations = [X]
        z_values = []
        
        for i in range(len(self.weights)):
            z = np.dot(self.weights[i], activations[-1]) + self.biases[i]
            z_values.append(z)
            a = self.activation_function(z)
            activations.append(a)
        
        return activations, z_values
    
    def compute_loss(self, y_pred, y_true):
        """Compute cross-entropy loss"""
        m = y_true.shape[1]
        loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / m
        return loss
    
    def compute_loss_gradient(self, y_pred, y_true):
        """Compute gradient of loss with respect to output"""
        return y_pred - y_true
    
    def backward_pass(self, X, y_true, activations, z_values):
        """Backward pass to compute gradients"""
        m = X.shape[1]
        num_layers = len(self.weights)
        
        # Initialize gradients
        weight_gradients = [np.zeros_like(w) for w in self.weights]
        bias_gradients = [np.zeros_like(b) for b in self.biases]
        
        # Compute output gradient
        delta = self.compute_loss_gradient(activations[-1], y_true)
        
        # Backpropagate through layers
        for layer in range(num_layers - 1, -1, -1):
            # Compute gradients for current layer
            weight_gradients[layer] = np.dot(delta, activations[layer].T) / m
            bias_gradients[layer] = np.sum(delta, axis=1, keepdims=True) / m
            
            # Compute delta for previous layer
            if layer > 0:
                delta = np.dot(self.weights[layer].T, delta) * self.activation_function(z_values[layer-1], derivative=True)
        
        return weight_gradients, bias_gradients
    
    def update_parameters(self, weight_gradients, bias_gradients, learning_rate):
        """Update parameters using gradients"""
        for i in range(len(self.weights)):
            self.weights[i] -= learning_rate * weight_gradients[i]
            self.biases[i] -= learning_rate * bias_gradients[i]
    
    def train(self, X, y, learning_rate=0.01, epochs=1000, batch_size=32):
        """Train the neural network"""
        losses = []
        
        for epoch in range(epochs):
            # Mini-batch training
            for i in range(0, X.shape[1], batch_size):
                X_batch = X[:, i:i+batch_size]
                y_batch = y[:, i:i+batch_size]
                
                # Forward pass
                activations, z_values = self.forward_pass(X_batch)
                
                # Backward pass
                weight_gradients, bias_gradients = self.backward_pass(X_batch, y_batch, activations, z_values)
                
                # Update parameters
                self.update_parameters(weight_gradients, bias_gradients, learning_rate)
            
            # Compute loss for monitoring
            if epoch % 100 == 0:
                activations, _ = self.forward_pass(X)
                loss = self.compute_loss(activations[-1], y)
                losses.append(loss)
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
        
        return losses
    
    def predict(self, X):
        """Make predictions"""
        activations, _ = self.forward_pass(X)
        return activations[-1]
    
    def evaluate(self, X, y):
        """Evaluate model performance"""
        predictions = self.predict(X)
        loss = self.compute_loss(predictions, y)
        
        # For classification
        if y.shape[0] == 1:  # Binary classification
            predictions_binary = (predictions > 0.5).astype(int)
            accuracy = np.mean(predictions_binary == y)
        else:  # Multi-class classification
            predictions_class = np.argmax(predictions, axis=0)
            y_class = np.argmax(y, axis=0)
            accuracy = np.mean(predictions_class == y_class)
        
        return loss, accuracy

# Example usage
def demonstrate_backpropagation():
    """Demonstrate backpropagation with XOR problem"""
    print("=== Backpropagation Demonstration (XOR Problem) ===\n")
    
    # XOR data
    X = np.array([[0, 0, 1, 1],
                  [0, 1, 0, 1]])
    y = np.array([[0, 1, 1, 0]])
    
    # Create neural network
    nn = NeuralNetwork([2, 4, 1], activation='sigmoid')
    
    # Train network
    losses = nn.train(X, y, learning_rate=0.1, epochs=5000)
    
    # Evaluate
    loss, accuracy = nn.evaluate(X, y)
    print(f"\nFinal Loss: {loss:.4f}")
    print(f"Accuracy: {accuracy:.4f}")
    
    # Make predictions
    predictions = nn.predict(X)
    print("\nPredictions:")
    for i in range(X.shape[1]):
        print(f"Input: {X[:, i]}, Target: {y[0, i]:.0f}, Prediction: {predictions[0, i]:.4f}")
    
    # Plot training loss
    plt.figure(figsize=(10, 6))
    plt.plot(range(0, 5000, 100), losses)
    plt.title('Training Loss Over Time')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.show()
    
    return nn, losses
```

### **12. Checklist ki·ªÉm tra nhanh:**
- [ ] Gradients c√≥ ƒë∆∞·ª£c compute correctly?
- [ ] Parameters c√≥ ƒë∆∞·ª£c update properly?
- [ ] Loss c√≥ decrease over time?
- [ ] Network c√≥ converge?
- [ ] Performance c√≥ acceptable?

---

# üß† Deep Learning (DL) - H·ªçc s√¢u v√† m·∫°ng n∆°-ron

> **M·ª•c ti√™u**: Tr·ªü th√†nh chuy√™n gia Deep Learning, hi·ªÉu s√¢u v·ªÅ l√Ω thuy·∫øt m·∫°ng n∆°-ron v√† c√≥ kh·∫£ nƒÉng x√¢y d·ª±ng c√°c m√¥ h√¨nh DL ph·ª©c t·∫°p

## üìã T·ªïng quan n·ªôi dung

```mermaid
graph TD
    A[üß† Deep Learning] --> B[üî¨ Neural Network Theory]
    A --> C[‚ö° Optimization & Training]
    A --> D[üèóÔ∏è Architecture Design]
    A --> E[üñºÔ∏è Computer Vision]
    A --> F[üìù Natural Language Processing]
    A --> G[üöÄ Advanced Techniques]
    
    B --> B1[Universal Approximation]
    B --> B2[Backpropagation]
    B --> B3[Activation Functions]
    B --> B4[Loss Functions]
    
    C --> C1[Initialization Strategies]
    C --> C2[Batch Normalization]
    C --> C3[Optimization Algorithms]
    C --> C4[Regularization]
    
    D --> D1[Feedforward Networks]
    D --> D2[Convolutional Networks]
    D --> D3[Recurrent Networks]
    D --> D4[Transformer Architecture]
    
    E --> E1[Image Classification]
    E --> E2[Object Detection]
    E --> E3[Image Segmentation]
    E --> E4[Style Transfer]
    
    F --> F1[Text Classification]
    F --> F2[Sequence Modeling]
    F --> F3[Language Models]
    F --> F4[Attention Mechanisms]
    
    G --> G1[Transfer Learning]
    G --> G2[Few-shot Learning]
    G --> G3[Meta Learning]
    G --> G4[Neural Architecture Search]
```

![Deep Learning Architecture](assets/deep-learning-architecture.svg)

![Deep Learning Architecture PNG](assets/deep-learning-architecture.png)

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/deep-learning-architecture.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/deep-learning-architecture.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/deep-learning-architecture.png)**

## üß© Ch∆∞∆°ng tr√¨nh 50/50 (L√Ω thuy·∫øt : Th·ª±c h√†nh)

- M·ª•c ti√™u: 50% l√Ω thuy·∫øt (ƒë·ªãnh l√Ω, c√¥ng th·ª©c t·ªëi ∆∞u h√≥a, ƒë·∫∑c t√≠nh ki·∫øn tr√∫c), 50% th·ª±c h√†nh (th·ª±c nghi·ªám c√≥ ki·ªÉm so√°t, so s√°nh, b√°o c√°o)

| M√¥-ƒëun | L√Ω thuy·∫øt (50%) | Th·ª±c h√†nh (50%) |
|---|---|---|
| NN Theory | UAT, Backprop, Activation/Loss | Th·ª≠ activation/loss kh√°c nhau |
| Optimization | Init, BN, AdamW/SGD, LR schedule | So s√°nh h·ªôi t·ª•/overfit/regularize |
| Architectures | CNN/RNN/Transformer fundamentals | Train c√°c b√†i to√°n nh·ªè (CIFAR/IMDB) |
| Advanced | Transfer/Few-shot/NAS | Fine-tune + report, ablation |

Rubric (100ƒë/module): L√Ω thuy·∫øt 30 | Code 30 | K·∫øt qu·∫£ 30 | B√°o c√°o 10

---

## üî¨ 1. Neural Network Theory - L√Ω thuy·∫øt m·∫°ng n∆°-ron

### 1.1 Universal Approximation Theorem - ƒê·ªãnh l√Ω x·∫•p x·ªâ ph·ªï qu√°t

> **Universal Approximation Theorem** l√† ƒë·ªãnh l√Ω c∆° b·∫£n trong Deep Learning, ch·ª©ng minh r·∫±ng neural networks c√≥ th·ªÉ x·∫•p x·ªâ b·∫•t k·ª≥ h√†m li√™n t·ª•c n√†o.

#### ƒê·ªãnh l√Ω v√† √ù nghƒ©a

**L√Ω thuy·∫øt c∆° b·∫£n:**
- **Universal Approximation Theorem (UAT)**: Neural networks v·ªõi m·ªôt hidden layer c√≥ th·ªÉ x·∫•p x·ªâ b·∫•t k·ª≥ h√†m li√™n t·ª•c n√†o
- **Stone-Weierstrass Theorem**: M·ªçi h√†m li√™n t·ª•c tr√™n compact set c√≥ th·ªÉ x·∫•p x·ªâ b·ªüi ƒëa th·ª©c
- **Density Properties**: Neural networks t·∫°o ra class of functions dense trong space of continuous functions

**Mathematical Foundations:**

**1. Formal Statement c·ªßa UAT:**
```python
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from scipy.special import expit
from typing import Callable, List, Tuple

class UniversalApproximationTheory:
    """Theoretical framework cho Universal Approximation Theorem"""
    
    @staticmethod
    def formal_statement():
        """Formal mathematical statement c·ªßa UAT"""
        print("""
        **Universal Approximation Theorem (Cybenko, 1989):**
        
        Let œÉ be any continuous sigmoidal function. Then finite sums of the form:
        
            G(x) = Œ£·µ¢ Œ±·µ¢ œÉ(w·µ¢·µÄx + b·µ¢)
        
        are dense in C([0,1]‚Åø).
        
        **Mathematical Meaning:**
        - For any continuous function f: [0,1]‚Åø ‚Üí ‚Ñù
        - For any Œµ > 0
        - There exists a neural network G(x) such that:
            |f(x) - G(x)| < Œµ for all x ‚àà [0,1]‚Åø
        
        **Key Components:**
        - œÉ: Activation function (sigmoidal)
        - w·µ¢: Weight vectors
        - b·µ¢: Bias terms
        - Œ±·µ¢: Output weights
        - Dense: Can approximate any function arbitrarily well
        """)
    
    @staticmethod
    def prove_uat_for_simple_case():
        """Prove UAT cho simple case: 1D continuous function"""
        print("""
        **Proof Sketch cho 1D Case:**
        
        1. **Step 1: Function Approximation by Step Functions**
           - Any continuous function on [0,1] can be approximated by step functions
           - Step functions can be written as linear combinations of indicator functions
        
        2. **Step 2: Indicator Functions by Neural Networks**
           - Indicator function I[a,b](x) can be approximated by:
             œÉ(wx + b) - œÉ(wx + b') where w ‚Üí ‚àû
           - This creates a "step" at x = -b/w
        
        3. **Step 3: Linear Combination**
           - Any step function = Œ£·µ¢ Œ±·µ¢ I[a·µ¢,b·µ¢](x)
           - Can be approximated by Œ£·µ¢ Œ±·µ¢ œÉ(w·µ¢x + b·µ¢)
        
        4. **Step 4: Arbitrary Precision**
           - By increasing number of neurons, can achieve arbitrary precision
           - Error bound: |f(x) - G(x)| < Œµ for all x
        """)
    
    @staticmethod
    def demonstrate_approximation_capability():
        """Demonstrate neural network approximation capability"""
        
        # Target function: f(x) = sin(2œÄx) + 0.5*sin(4œÄx)
        def target_function(x):
            return np.sin(2 * np.pi * x) + 0.5 * np.sin(4 * np.pi * x)
        
        # Generate training data
        x_train = np.linspace(0, 1, 1000).reshape(-1, 1)
        y_train = target_function(x_train)
        
        # Neural network architecture
        class ApproximationNetwork(nn.Module):
            def __init__(self, hidden_size: int):
                super().__init__()
                self.hidden = nn.Linear(1, hidden_size)
                self.output = nn.Linear(hidden_size, 1)
                self.activation = nn.Tanh()  # Sigmoidal activation
                
            def forward(self, x):
                x = self.activation(self.hidden(x))
                x = self.output(x)
                return x
        
        # Train networks with different hidden sizes
        hidden_sizes = [5, 10, 20, 50]
        trained_networks = []
        training_errors = []
        
        for hidden_size in hidden_sizes:
            # Initialize network
            net = ApproximationNetwork(hidden_size)
            criterion = nn.MSELoss()
            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)
            
            # Training loop
            x_tensor = torch.FloatTensor(x_train)
            y_tensor = torch.FloatTensor(y_train)
            
            for epoch in range(5000):
                optimizer.zero_grad()
                outputs = net(x_tensor)
                loss = criterion(outputs, y_tensor)
                loss.backward()
                optimizer.step()
                
                if epoch % 1000 == 0:
                    print(f"Hidden size {hidden_size}, Epoch {epoch}, Loss: {loss.item():.6f}")
            
            # Evaluate
            with torch.no_grad():
                y_pred = net(x_tensor).numpy().flatten()
                mse = np.mean((y_train - y_pred)**2)
                training_errors.append(mse)
                trained_networks.append(net)
            
            print(f"Hidden size {hidden_size}, Final MSE: {mse:.6f}")
        
        # Visualization
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        for i, (hidden_size, net, error) in enumerate(zip(hidden_sizes, trained_networks, training_errors)):
            row = i // 2
            col = i % 2
            
            # Plot original vs approximated
            with torch.no_grad():
                y_pred = net(torch.FloatTensor(x_train)).numpy().flatten()
            
            axes[row, col].plot(x_train, y_train, 'b-', label='Target Function', linewidth=2)
            axes[row, col].plot(x_train, y_pred, 'r--', label=f'NN (hidden={hidden_size})', linewidth=2)
            axes[row, col].set_title(f'Approximation with {hidden_size} Hidden Neurons\nMSE: {error:.6f}')
            axes[row, col].set_xlabel('x')
            axes[row, col].set_ylabel('f(x)')
            axes[row, col].legend()
            axes[row, col].grid(True)
        
        plt.tight_layout()
        plt.show()
        
        # Error analysis
        print("\n**Approximation Error Analysis:**")
        for hidden_size, error in zip(hidden_sizes, training_errors):
            print(f"Hidden size {hidden_size}: MSE = {error:.6f}")
        
        return {
            'hidden_sizes': hidden_sizes,
            'training_errors': training_errors,
            'networks': trained_networks
        }
    
    @staticmethod
    def analyze_approximation_properties():
        """Analyze mathematical properties c·ªßa neural network approximation"""
        
        print("""
        **Mathematical Properties c·ªßa Neural Network Approximation:**
        
        1. **Density Property:**
           - Neural networks form a dense subset in C([0,1]‚Åø)
           - Can approximate any continuous function arbitrarily well
        
        2. **Approximation Rate:**
           - Error decreases as number of neurons increases
           - Rate depends on smoothness of target function
        
        3. **Curse of Dimensionality:**
           - Number of neurons needed grows exponentially with input dimension
           - Practical limitation for high-dimensional problems
        
        4. **Activation Function Requirements:**
           - Must be sigmoidal (bounded, non-constant, monotonically increasing)
           - Examples: tanh, sigmoid, ReLU (with proper scaling)
        
        5. **Weight Initialization:**
           - Weights must be properly initialized for good approximation
           - Random initialization often sufficient for large networks
        """)
        
        # Demonstrate curse of dimensionality
        dimensions = [1, 2, 3, 5, 10]
        neurons_needed = []
        
        for dim in dimensions:
            # Rough estimate: neurons needed scales exponentially
            neurons_needed.append(int(10 * (2**dim)))
        
        print("\n**Curse of Dimensionality Analysis:**")
        for dim, neurons in zip(dimensions, neurons_needed):
            print(f"Input dimension {dim}: Estimated neurons needed = {neurons}")
        
        # Visualization
        plt.figure(figsize=(10, 6))
        plt.plot(dimensions, neurons_needed, 'bo-', linewidth=2, markersize=8)
        plt.xlabel('Input Dimension')
        plt.ylabel('Estimated Neurons Needed')
        plt.title('Curse of Dimensionality: Neurons vs Input Dimension')
        plt.grid(True)
        plt.yscale('log')
        plt.show()

# Demonstrate UAT theory
uat_theory = UniversalApproximationTheory()
uat_theory.formal_statement()
uat_theory.prove_uat_for_simple_case()

# Demonstrate approximation capability
approximation_results = uat_theory.demonstrate_approximation_capability()

# Analyze properties
uat_theory.analyze_approximation_properties()
```

**2. Backpropagation Theory:**
```python
class BackpropagationTheory:
    """Theoretical framework cho backpropagation algorithm"""
    
    @staticmethod
    def explain_backpropagation():
        """Explain backpropagation mathematically"""
        print("""
        **Backpropagation Algorithm:**
        
        **Forward Pass:**
        - Input: x, weights W, biases b
        - Hidden layers: h·µ¢ = œÉ(W·µ¢h_{i-1} + b·µ¢)
        - Output: ≈∑ = œÉ(W‚Çñh_{k-1} + b‚Çñ)
        
        **Backward Pass (Chain Rule):**
        - Loss: L(≈∑, y)
        - Gradient w.r.t. output: ‚àÇL/‚àÇ≈∑
        - Gradient w.r.t. weights: ‚àÇL/‚àÇW·µ¢ = ‚àÇL/‚àÇh·µ¢ √ó ‚àÇh·µ¢/‚àÇW·µ¢
        - Gradient w.r.t. hidden: ‚àÇL/‚àÇh·µ¢ = Œ£‚±º ‚àÇL/‚àÇh_{i+1} √ó ‚àÇh_{i+1}/‚àÇh·µ¢
        
        **Mathematical Formulation:**
        - ‚àÇL/‚àÇW·µ¢ = ‚àÇL/‚àÇ≈∑ √ó ‚àè_{j=i+1}^k ‚àÇh‚±º/‚àÇh_{j-1} √ó ‚àÇh·µ¢/‚àÇW·µ¢
        - This is the chain rule applied recursively
        """)
    
    @staticmethod
    def demonstrate_chain_rule():
        """Demonstrate chain rule in backpropagation"""
        
        # Simple example: f(x) = sin(x¬≤)
        def f(x):
            return np.sin(x**2)
        
        def df_dx(x):
            # Chain rule: d/dx[sin(x¬≤)] = cos(x¬≤) √ó 2x
            return np.cos(x**2) * 2 * x
        
        # Generate data
        x_values = np.linspace(-2, 2, 100)
        y_values = f(x_values)
        dy_dx_values = df_dx(x_values)
        
        # Visualization
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Function plot
        ax1.plot(x_values, y_values, 'b-', linewidth=2, label='f(x) = sin(x¬≤)')
        ax1.set_xlabel('x')
        ax1.set_ylabel('f(x)')
        ax1.set_title('Function: f(x) = sin(x¬≤)')
        ax1.grid(True)
        ax1.legend()
        
        # Derivative plot
        ax2.plot(x_values, dy_dx_values, 'r-', linewidth=2, label="f'(x) = 2x cos(x¬≤)")
        ax2.set_xlabel('x')
        ax2.set_ylabel("f'(x)")
        ax2.set_title('Derivative using Chain Rule')
        ax2.grid(True)
        ax2.legend()
        
        plt.tight_layout()
        plt.show()
        
        # Numerical verification
        def numerical_derivative(f, x, h=1e-6):
            return (f(x + h) - f(x - h)) / (2 * h)
        
        test_points = [-1.5, -0.5, 0, 0.5, 1.5]
        print("\n**Numerical Verification of Chain Rule:**")
        print("x\t\tAnalytical\t\tNumerical\t\tDifference")
        print("-" * 70)
        
        for x in test_points:
            analytical = df_dx(x)
            numerical = numerical_derivative(f, x)
            diff = abs(analytical - numerical)
            print(f"{x:6.1f}\t\t{analytical:10.6f}\t\t{numerical:10.6f}\t\t{diff:10.6f}")
    
    @staticmethod
    def implement_backpropagation():
        """Implement backpropagation manually cho simple network"""
        
        class SimpleNeuralNetwork:
            def __init__(self, input_size: int, hidden_size: int, output_size: int):
                # Initialize weights and biases
                self.W1 = np.random.randn(input_size, hidden_size) * 0.01
                self.b1 = np.zeros((1, hidden_size))
                self.W2 = np.random.randn(hidden_size, output_size) * 0.01
                self.b2 = np.zeros((1, output_size))
                
                # Learning rate
                self.lr = 0.1
            
            def sigmoid(self, x):
                return 1 / (1 + np.exp(-x))
            
            def sigmoid_derivative(self, x):
                return x * (1 - x)
            
            def forward(self, X):
                # Forward pass
                self.z1 = np.dot(X, self.W1) + self.b1
                self.a1 = self.sigmoid(self.z1)
                self.z2 = np.dot(self.a1, self.W2) + self.b2
                self.a2 = self.sigmoid(self.z2)
                return self.a2
            
            def backward(self, X, y, output):
                # Backward pass
                self.error = y - output
                self.delta2 = self.error * self.sigmoid_derivative(output)
                
                self.error_hidden = np.dot(self.delta2, self.W2.T)
                self.delta1 = self.error_hidden * self.sigmoid_derivative(self.a1)
                
                # Update weights and biases
                self.W2 += self.lr * np.dot(self.a1.T, self.delta2)
                self.b2 += self.lr * np.sum(self.delta2, axis=0, keepdims=True)
                self.W1 += self.lr * np.dot(X.T, self.delta1)
                self.b1 += self.lr * np.sum(self.delta1, axis=0, keepdims=True)
            
            def train(self, X, y, epochs):
                losses = []
                for epoch in range(epochs):
                    # Forward pass
                    output = self.forward(X)
                    
                    # Backward pass
                    self.backward(X, y, output)
                    
                    # Calculate loss
                    loss = np.mean(np.square(y - output))
                    losses.append(loss)
                    
                    if epoch % 1000 == 0:
                        print(f"Epoch {epoch}, Loss: {loss:.6f}")
                
                return losses
        
        # Generate simple dataset
        X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
        y = np.array([[0], [1], [1], [0]])  # XOR problem
        
        # Train network
        nn = SimpleNeuralNetwork(2, 4, 1)
        losses = nn.train(X, y, epochs=10000)
        
        # Test network
        predictions = nn.forward(X)
        print("\n**XOR Problem Results:**")
        print("Input\t\tTarget\t\tPrediction")
        print("-" * 40)
        for i in range(len(X)):
            print(f"{X[i]}\t\t{y[i][0]}\t\t{predictions[i][0]:.4f}")
        
        # Plot training loss
        plt.figure(figsize=(10, 6))
        plt.plot(losses)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training Loss over Epochs')
        plt.grid(True)
        plt.yscale('log')
        plt.show()
        
        return nn, losses

# Demonstrate backpropagation theory
backprop_theory = BackpropagationTheory()
backprop_theory.explain_backpropagation()
backprop_theory.demonstrate_chain_rule()

# Implement and test backpropagation
trained_nn, training_losses = backprop_theory.implement_backpropagation()
```

**3. Activation Functions Theory:**
```python
class ActivationFunctionTheory:
    """Theoretical analysis c·ªßa activation functions"""
    
    @staticmethod
    def analyze_activation_functions():
        """Analyze mathematical properties c·ªßa activation functions"""
        
        # Define activation functions
        def sigmoid(x):
            return 1 / (1 + np.exp(-x))
        
        def tanh(x):
            return np.tanh(x)
        
        def relu(x):
            return np.maximum(0, x)
        
        def leaky_relu(x, alpha=0.01):
            return np.where(x > 0, x, alpha * x)
        
        def swish(x, beta=1.0):
            return x * sigmoid(beta * x)
        
        # Generate data
        x = np.linspace(-5, 5, 1000)
        
        # Calculate function values and derivatives
        functions = {
            'Sigmoid': sigmoid,
            'Tanh': tanh,
            'ReLU': relu,
            'Leaky ReLU': lambda x: leaky_relu(x),
            'Swish': lambda x: swish(x)
        }
        
        # Calculate derivatives numerically
        def numerical_derivative(f, x, h=1e-6):
            return (f(x + h) - f(x - h)) / (2 * h)
        
        # Visualization
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        for i, (name, func) in enumerate(functions.items()):
            row = i // 3
            col = i % 3
            
            # Function plot
            y = func(x)
            axes[row, col].plot(x, y, 'b-', linewidth=2, label=name)
            
            # Derivative plot
            dy_dx = numerical_derivative(func, x)
            axes[row, col].plot(x, dy_dx, 'r--', linewidth=2, label=f"{name}'")
            
            axes[row, col].set_title(f'{name} Function and Derivative')
            axes[row, col].set_xlabel('x')
            axes[row, col].set_ylabel('y')
            axes[row, col].grid(True)
            axes[row, col].legend()
            axes[row, col].set_xlim(-5, 5)
            axes[row, col].set_ylim(-2, 2)
        
        # Remove extra subplot
        axes[1, 2].remove()
        
        plt.tight_layout()
        plt.show()
        
        # Mathematical analysis
        print("""
        **Activation Function Analysis:**
        
        1. **Sigmoid:**
           - Range: (0, 1)
           - Derivative: œÉ'(x) = œÉ(x)(1-œÉ(x))
           - Issues: Vanishing gradient for large |x|
        
        2. **Tanh:**
           - Range: (-1, 1)
           - Derivative: tanh'(x) = 1 - tanh¬≤(x)
           - Better than sigmoid: zero-centered
        
        3. **ReLU:**
           - Range: [0, ‚àû)
           - Derivative: 1 if x > 0, 0 if x ‚â§ 0
           - Issues: Dying ReLU problem
        
        4. **Leaky ReLU:**
           - Range: (-‚àû, ‚àû)
           - Derivative: 1 if x > 0, Œ± if x ‚â§ 0
           - Solves dying ReLU problem
        
        5. **Swish:**
           - Range: (-‚àû, ‚àû)
           - Derivative: œÉ(Œ≤x) + Œ≤xœÉ(Œ≤x)(1-œÉ(Œ≤x))
           - Smooth, non-monotonic
        """)
        
        # Demonstrate vanishing gradient problem
        def demonstrate_vanishing_gradient():
            """Demonstrate vanishing gradient problem"""
            
            # Create deep network with sigmoid
            def sigmoid_network(x, weights):
                """Forward pass through network with sigmoid"""
                current = x
                for w in weights:
                    current = sigmoid(np.dot(current, w))
                return current
            
            # Test with different weight magnitudes
            x_input = np.array([1.0])
            weight_scenarios = [
                [np.array([[0.1]]), np.array([[0.1]]), np.array([[0.1]])],  # Small weights
                [np.array([[1.0]]), np.array([[1.0]]), np.array([[1.0]])],  # Medium weights
                [np.array([[5.0]]), np.array([[5.0]]), np.array([[5.0]])]   # Large weights
            ]
            
            print("\n**Vanishing Gradient Demonstration:**")
            print("Weight Magnitude\tOutput\t\tGradient Magnitude")
            print("-" * 60)
            
            for i, weights in enumerate(weight_scenarios):
                output = sigmoid_network(x_input, weights)
                
                # Calculate gradient magnitude (simplified)
                grad_magnitude = np.prod([sigmoid(np.dot(x_input, w)) * (1 - sigmoid(np.dot(x_input, w))) 
                                        for w in weights])
                
                weight_mag = np.mean([np.mean(np.abs(w)) for w in weights])
                print(f"{weight_mag:15.1f}\t{output[0]:10.6f}\t{grad_magnitude:15.6f}")
        
        demonstrate_vanishing_gradient()

# Demonstrate activation function theory
activation_theory = ActivationFunctionTheory()
activation_theory.analyze_activation_functions()
```

**T√†i li·ªáu tham kh·∫£o chuy√™n s√¢u:**
- **Universal Approximation**: [Approximation by Superpositions of a Sigmoidal Function](https://www.sciencedirect.com/science/article/abs/pii/0893608089900148)
- **Backpropagation**: [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0)
- **Activation Functions**: [Deep Learning with Rectified Linear Units](https://arxiv.org/abs/1803.08375)
- **Neural Network Theory**: [Neural Networks and Learning Machines](https://www.pearson.com/us/higher-education/program/Haykin-Neural-Networks-and-Learning-Machines-3rd-Edition/PGM263675.html)

#### V√≠ d·ª• minh h·ªça

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

def demonstrate_universal_approximation():
    """
    Minh h·ªça Universal Approximation Theorem
    T·∫°o neural network x·∫•p x·ªâ h√†m sin(x)
    """
    
    # T·∫°o d·ªØ li·ªáu
    x = torch.linspace(0, 2*np.pi, 1000).reshape(-1, 1)
    y_true = torch.sin(x)
    
    # Neural network v·ªõi 1 hidden layer
    class UniversalApproximator(nn.Module):
        def __init__(self, hidden_size=50):
            super().__init__()
            self.hidden = nn.Linear(1, hidden_size)
            self.output = nn.Linear(hidden_size, 1)
            self.activation = nn.Tanh()  # Tanh activation
            
        def forward(self, x):
            x = self.activation(self.hidden(x))
            x = self.output(x)
            return x
    
    # Kh·ªüi t·∫°o m√¥ h√¨nh
    model = UniversalApproximator(hidden_size=50)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    
    # Training
    losses = []
    for epoch in range(5000):
        optimizer.zero_grad()
        y_pred = model(x)
        loss = criterion(y_pred, y_true)
        loss.backward()
        optimizer.step()
        
        if epoch % 500 == 0:
            losses.append(loss.item())
            print(f"Epoch {epoch}, Loss: {loss.item():.6f}")
    
    # ƒê√°nh gi√° k·∫øt qu·∫£
    with torch.no_grad():
        y_pred = model(x)
        final_loss = criterion(y_pred, y_true)
        print(f"\nüéØ Final Loss: {final_loss.item():.6f}")
        
        # T√≠nh ƒë·ªô ch√≠nh x√°c
        mae = torch.mean(torch.abs(y_pred - y_true))
        print(f"üìä Mean Absolute Error: {mae.item():.6f}")
    
    # Visualization
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(x.numpy(), y_true.numpy(), 'b-', label='True: sin(x)', linewidth=2)
    plt.plot(x.numpy(), y_pred.numpy(), 'r--', label='NN Approximation', linewidth=2)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('Universal Approximation: sin(x)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(losses)
    plt.xlabel('Epoch (x500)')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return model, final_loss.item()

# V√≠ d·ª• s·ª≠ d·ª•ng
# model, final_loss = demonstrate_universal_approximation()
```

**Gi·∫£i th√≠ch k·∫øt qu·∫£:**
- **Loss**: ƒê·ªô l·ªói gi·ªØa d·ª± ƒëo√°n v√† gi√° tr·ªã th·∫≠t, c√†ng th·∫•p c√†ng t·ªët
- **MAE**: Mean Absolute Error - sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh
- **Hidden Size**: S·ªë n∆°-ron trong hidden layer, c√†ng nhi·ªÅu c√†ng c√≥ kh·∫£ nƒÉng x·∫•p x·ªâ ph·ª©c t·∫°p

### 1.2 Backpropagation - Lan truy·ªÅn ng∆∞·ª£c

> **Backpropagation** l√† thu·∫≠t to√°n c∆° b·∫£n ƒë·ªÉ t√≠nh gradient c·ªßa loss function theo c√°c tham s·ªë c·ªßa neural network.

#### Chain Rule v√† Gradient Flow

**Chain rule trong backpropagation**:
```
‚àÇL/‚àÇw·µ¢‚±º = ‚àÇL/‚àÇa‚±º √ó ‚àÇa‚±º/‚àÇz‚±º √ó ‚àÇz‚±º/‚àÇw·µ¢‚±º
```

**Gi·∫£i th√≠ch c√°c k√Ω hi·ªáu:**
- **‚àÇL/‚àÇw·µ¢‚±º**: Gradient c·ªßa loss theo weight w·µ¢‚±º
- **‚àÇL/‚àÇa‚±º**: Gradient c·ªßa loss theo activation a‚±º
- **‚àÇa‚±º/‚àÇz‚±º**: Gradient c·ªßa activation theo pre-activation z‚±º
- **‚àÇz‚±º/‚àÇw·µ¢‚±º**: Gradient c·ªßa pre-activation theo weight w·µ¢‚±º

**Trong ƒë√≥:**
- **a‚±º = œÉ(z‚±º)**: Activation output c·ªßa n∆°-ron j
- **z‚±º = Œ£·µ¢ w·µ¢‚±ºa·µ¢**: Pre-activation (t·ªïng c√≥ tr·ªçng s·ªë c·ªßa inputs)
- **œÉ**: Activation function (ReLU, Tanh, Sigmoid, etc.)

**Gradient flow t·ªïng qu√°t**:
```
‚àÇL/‚àÇŒ∏ = (‚àÇL/‚àÇ≈∑) √ó (‚àÇ≈∑/‚àÇŒ∏)
```

**Gi·∫£i th√≠ch:**
- **Œ∏**: T·∫•t c·∫£ tham s·ªë c·ªßa m·∫°ng
- **≈∑**: D·ª± ƒëo√°n c·ªßa m·∫°ng
- **‚àÇL/‚àÇ≈∑**: Gradient c·ªßa loss theo output

#### Implementation v·ªõi PyTorch

```python
import torch
import torch.nn as nn

class SimpleNN(nn.Module):
    """
    Neural network ƒë∆°n gi·∫£n ƒë·ªÉ minh h·ªça backpropagation
    
    Architecture:
    Input (10) ‚Üí Hidden (20) ‚Üí Output (1)
    """
    
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 20)      # Fully connected layer 1
        self.fc2 = nn.Linear(20, 1)       # Fully connected layer 2
        self.relu = nn.ReLU()             # Activation function
        
    def forward(self, x):
        """
        Forward pass: t√≠nh output t·ª´ input
        
        Parameters:
        x (torch.Tensor): Input tensor shape (batch_size, 10)
        
        Returns:
        torch.Tensor: Output tensor shape (batch_size, 1)
        """
        # Layer 1: input ‚Üí hidden
        x = self.fc1(x)        # Linear transformation: x @ W1 + b1
        x = self.relu(x)       # Non-linear activation: max(0, x)
        
        # Layer 2: hidden ‚Üí output
        x = self.fc2(x)        # Linear transformation: x @ W2 + b2
        
        return x

def demonstrate_backpropagation():
    """
    Minh h·ªça qu√° tr√¨nh backpropagation
    """
    
    # Kh·ªüi t·∫°o m√¥ h√¨nh
    model = SimpleNN()
    
    # T·∫°o d·ªØ li·ªáu m·∫´u
    batch_size = 32
    x = torch.randn(batch_size, 10)    # Input: 32 samples, 10 features
    y = torch.randn(batch_size, 1)     # Target: 32 samples, 1 output
    
    print("üîç MODEL ARCHITECTURE:")
    print(f"Input shape: {x.shape}")
    print(f"Target shape: {y.shape}")
    print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")
    
    # Forward pass
    print("\n‚û°Ô∏è FORWARD PASS:")
    y_pred = model(x)
    print(f"Prediction shape: {y_pred.shape}")
    print(f"Prediction range: [{y_pred.min().item():.4f}, {y_pred.max().item():.4f}]")
    
    # Loss calculation
    criterion = nn.MSELoss()
    loss = criterion(y_pred, y)
    print(f"Initial Loss: {loss.item():.6f}")
    
    # Backward pass (automatic differentiation)
    print("\n‚¨ÖÔ∏è BACKWARD PASS:")
    print("Computing gradients...")
    loss.backward()  # Automatic backpropagation
    
    # Ki·ªÉm tra gradients
    print("\nüìä GRADIENTS ANALYSIS:")
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            param_norm = param.norm().item()
            print(f"{name}:")
            print(f"  Parameter norm: {param_norm:.4f}")
            print(f"  Gradient norm: {grad_norm:.4f}")
            print(f"  Gradient/Parameter ratio: {grad_norm/param_norm:.4f}")
    
    # Gradient clipping (tr√°nh gradient explosion)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    print("\n‚úÇÔ∏è GRADIENT CLIPPING APPLIED (max_norm=1.0)")
    
    return model, loss

# V√≠ d·ª• s·ª≠ d·ª•ng
# model, loss = demonstrate_backpropagation()
```

**Gi·∫£i th√≠ch c√°c kh√°i ni·ªám:**
- **Forward Pass**: T√≠nh output t·ª´ input qua c√°c layer
- **Backward Pass**: T√≠nh gradient c·ªßa loss theo c√°c tham s·ªë
- **Gradient Norm**: ƒê·ªô l·ªõn c·ªßa gradient vector
- **Gradient Clipping**: K·ªπ thu·∫≠t tr√°nh gradient explosion

## ‚ö° 2. Optimization trong Deep Learning

### 2.1 Initialization Strategies - Chi·∫øn l∆∞·ª£c kh·ªüi t·∫°o

> **Weight Initialization** l√† y·∫øu t·ªë quan tr·ªçng ·∫£nh h∆∞·ªüng ƒë·∫øn hi·ªáu su·∫•t training c·ªßa neural network.

#### Xavier/Glorot Initialization

**C√¥ng th·ª©c Xavier/Glorot**:
```
W·µ¢‚±º ~ N(0, 2/(n_in + n_out))
```

**Gi·∫£i th√≠ch k√Ω hi·ªáu:**
- **W·µ¢‚±º**: Weight t·ª´ n∆°-ron i ƒë·∫øn n∆°-ron j
- **N(Œº, œÉ¬≤)**: Ph√¢n ph·ªëi chu·∫©n v·ªõi mean Œº v√† variance œÉ¬≤
- **n_in**: S·ªë n∆°-ron input c·ªßa layer
- **n_out**: S·ªë n∆°-ron output c·ªßa layer

**L√Ω do s·ª≠ d·ª•ng**:
- Gi·ªØ variance c·ªßa activations ·ªïn ƒë·ªãnh qua c√°c layer
- Tr√°nh vanishing/exploding gradients
- ƒê·∫∑c bi·ªát hi·ªáu qu·∫£ v·ªõi activation functions nh∆∞ Tanh, Sigmoid

#### He Initialization

**C√¥ng th·ª©c He Initialization**:
```
W·µ¢‚±º ~ N(0, 2/n_in)
```

**Khi n√†o s·ª≠ d·ª•ng**:
- ƒê·∫∑c bi·ªát hi·ªáu qu·∫£ v·ªõi ReLU activation function
- ReLU c√≥ xu h∆∞·ªõng "kill" m·ªôt n·ª≠a n∆°-ron (output = 0)
- He initialization b√π ƒë·∫Øp cho vi·ªác n√†y

**So s√°nh v·ªõi Xavier**:
- Xavier: œÉ¬≤ = 2/(n_in + n_out)
- He: œÉ¬≤ = 2/n_in
- He th∆∞·ªùng c√≥ variance cao h∆°n, ph√π h·ª£p v·ªõi ReLU

#### Implementation chi ti·∫øt

```python
import torch
import torch.nn as nn
import torch.nn.init as init
import matplotlib.pyplot as plt
import numpy as np

class BetterNN(nn.Module):
    """
    Neural network v·ªõi c√°c chi·∫øn l∆∞·ª£c initialization kh√°c nhau
    """
    
    def __init__(self, init_method='xavier'):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)    # Input layer ‚Üí Hidden layer 1
        self.fc2 = nn.Linear(256, 128)    # Hidden layer 1 ‚Üí Hidden layer 2
        self.fc3 = nn.Linear(128, 10)     # Hidden layer 2 ‚Üí Output layer
        
        # Apply initialization strategy
        self.apply_initialization(init_method)
        
    def apply_initialization(self, method):
        """
        √Åp d·ª•ng chi·∫øn l∆∞·ª£c initialization kh√°c nhau
        
        Parameters:
        method (str): 'xavier', 'he', 'normal', 'uniform'
        """
        
        if method == 'xavier':
            # Xavier/Glorot initialization
            init.xavier_uniform_(self.fc1.weight)
            init.xavier_uniform_(self.fc2.weight)
            init.xavier_uniform_(self.fc3.weight)
            print("‚úÖ Applied Xavier/Glorot initialization")
            
        elif method == 'he':
            # He initialization
            init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')
            init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')
            init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')
            print("‚úÖ Applied He initialization")
            
        elif method == 'normal':
            # Normal initialization
            init.normal_(self.fc1.weight, mean=0, std=0.1)
            init.normal_(self.fc2.weight, mean=0, std=0.1)
            init.normal_(self.fc3.weight, mean=0, std=0.1)
            print("‚úÖ Applied Normal initialization")
            
        elif method == 'uniform':
            # Uniform initialization
            init.uniform_(self.fc1.weight, a=-0.1, b=0.1)
            init.uniform_(self.fc2.weight, a=-0.1, b=0.1)
            init.uniform_(self.fc3.weight, a=-0.1, b=0.1)
            print("‚úÖ Applied Uniform initialization")
        
        # Initialize biases to zero (best practice)
        init.zeros_(self.fc1.bias)
        init.zeros_(self.fc2.bias)
        init.zeros_(self.fc3.bias)
        print("‚úÖ Initialized biases to zero")
    
    def forward(self, x):
        """Forward pass"""
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

def compare_initialization_methods():
    """
    So s√°nh hi·ªáu qu·∫£ c·ªßa c√°c ph∆∞∆°ng ph√°p initialization
    """
    
    methods = ['xavier', 'he', 'normal', 'uniform']
    results = {}
    
    print("üî¨ COMPARING INITIALIZATION METHODS")
    print("=" * 50)
    
    for method in methods:
        print(f"\nüìä Testing {method.upper()} initialization...")
        
        # T·∫°o m√¥ h√¨nh v·ªõi method c·ª• th·ªÉ
        model = BetterNN(init_method=method)
        
        # T·∫°o d·ªØ li·ªáu m·∫´u
        x = torch.randn(100, 784)
        
        # Forward pass
        with torch.no_grad():
            y = model(x)
            
            # T√≠nh statistics
            activations = []
            for layer in [model.fc1, model.fc2, model.fc3]:
                if hasattr(layer, 'weight'):
                    activations.append(layer.weight.data.numpy().flatten())
            
            # T√≠nh variance c·ªßa activations
            variances = [np.var(act) for act in activations]
            
            results[method] = {
                'output_range': [y.min().item(), y.max().item()],
                'layer_variances': variances,
                'total_params': sum(p.numel() for p in model.parameters())
            }
            
            print(f"  Output range: [{y.min().item():.4f}, {y.max().item():.4f}]")
            print(f"  Layer variances: {[f'{v:.4f}' for v in variances]}")
    
    # Visualization
    plt.figure(figsize=(15, 5))
    
    # Output range comparison
    plt.subplot(1, 3, 1)
    output_ranges = [results[m]['output_range'] for m in methods]
    ranges = [r[1] - r[0] for r in output_ranges]
    plt.bar(methods, ranges)
    plt.title('Output Range Comparison')
    plt.ylabel('Range (max - min)')
    plt.xticks(rotation=45)
    
    # Layer variance comparison
    plt.subplot(1, 3, 2)
    for i, method in enumerate(methods):
        variances = results[method]['layer_variances']
        plt.plot(range(1, len(variances)+1), variances, 
                marker='o', label=method.upper())
    plt.title('Layer Variance Comparison')
    plt.xlabel('Layer')
    plt.ylabel('Variance')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Parameter count
    plt.subplot(1, 3, 3)
    param_counts = [results[m]['total_params'] for m in methods]
    plt.bar(methods, param_counts)
    plt.title('Total Parameters')
    plt.ylabel('Number of Parameters')
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    return results

# V√≠ d·ª• s·ª≠ d·ª•ng
# results = compare_initialization_methods()
```

**Gi·∫£i th√≠ch k·∫øt qu·∫£:**
- **Output Range**: Kho·∫£ng gi√° tr·ªã output, c√†ng ·ªïn ƒë·ªãnh c√†ng t·ªët
- **Layer Variance**: Variance c·ªßa weights trong m·ªói layer
- **Parameter Count**: T·ªïng s·ªë tham s·ªë c·ªßa m√¥ h√¨nh

### 2.2 Batch Normalization - Chu·∫©n h√≥a theo batch

> **Batch Normalization** l√† k·ªπ thu·∫≠t chu·∫©n h√≥a d·ªØ li·ªáu trong qu√° tr√¨nh training ƒë·ªÉ ·ªïn ƒë·ªãnh h√≥a training v√† tƒÉng t·ªëc ƒë·ªô h·ªôi t·ª•.

#### L√Ω thuy·∫øt Batch Normalization

**Forward pass**:
```
Œº_B = (1/m)Œ£·µ¢ x·µ¢
œÉ¬≤_B = (1/m)Œ£·µ¢(x·µ¢ - Œº_B)¬≤
xÃÇ·µ¢ = (x·µ¢ - Œº_B) / ‚àö(œÉ¬≤_B + Œµ)
y·µ¢ = Œ≥xÃÇ·µ¢ + Œ≤
```

**Gi·∫£i th√≠ch c√°c k√Ω hi·ªáu:**
- **Œº_B**: Mean c·ªßa batch
- **œÉ¬≤_B**: Variance c·ªßa batch
- **m**: Batch size
- **x·µ¢**: Input th·ª© i trong batch
- **xÃÇ·µ¢**: Input ƒë√£ ƒë∆∞·ª£c normalize
- **Œµ**: Small constant (1e-8) ƒë·ªÉ tr√°nh division by zero
- **Œ≥, Œ≤**: Learnable parameters (scale v√† shift)
- **y·µ¢**: Output cu·ªëi c√πng

**L·ª£i √≠ch c·ªßa Batch Normalization**:
- Gi·∫£m internal covariate shift
- Cho ph√©p s·ª≠ d·ª•ng learning rate cao h∆°n
- Gi·∫£m dependency v√†o initialization
- Ho·∫°t ƒë·ªông nh∆∞ m·ªôt form of regularization

#### Implementation chi ti·∫øt

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np

class BatchNormNetwork(nn.Module):
    """
    Neural network v·ªõi v√† kh√¥ng c√≥ Batch Normalization
    """
    
    def __init__(self, use_batch_norm=True):
        super().__init__()
        self.use_batch_norm = use_batch_norm
        
        # Layers
        self.fc1 = nn.Linear(784, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 10)
        
        # Batch Normalization layers
        if use_batch_norm:
            self.bn1 = nn.BatchNorm1d(256)
            self.bn2 = nn.BatchNorm1d(128)
        
        # Dropout for regularization
        self.dropout = nn.Dropout(0.5)
        
    def forward(self, x):
        """Forward pass v·ªõi optional batch normalization"""
        
        # Layer 1
        x = self.fc1(x)
        if self.use_batch_norm:
            x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Layer 2
        x = self.fc2(x)
        if self.use_batch_norm:
            x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Output layer
        x = self.fc3(x)
        
        return x

def demonstrate_batch_normalization():
    """
    Minh h·ªça hi·ªáu qu·∫£ c·ªßa Batch Normalization
    """
    
    print("üî¨ BATCH NORMALIZATION DEMONSTRATION")
    print("=" * 50)
    
    # T·∫°o d·ªØ li·ªáu m·∫´u
    batch_size = 32
    x = torch.randn(batch_size, 784)
    
    # M√¥ h√¨nh kh√¥ng c√≥ Batch Norm
    model_no_bn = BatchNormNetwork(use_batch_norm=False)
    print("\nüìä Model WITHOUT Batch Normalization:")
    
    with torch.no_grad():
        y_no_bn = model_no_bn(x)
        print(f"  Output shape: {y_no_bn.shape}")
        print(f"  Output mean: {y_no_bn.mean().item():.4f}")
        print(f"  Output std: {y_no_bn.std().item():.4f}")
        print(f"  Output range: [{y_no_bn.min().item():.4f}, {y_no_bn.max().item():.4f}]")
    
    # M√¥ h√¨nh c√≥ Batch Norm
    model_with_bn = BatchNormNetwork(use_batch_norm=True)
    print("\nüìä Model WITH Batch Normalization:")
    
    with torch.no_grad():
        y_with_bn = model_with_bn(x)
        print(f"  Output shape: {y_with_bn.shape}")
        print(f"  Output mean: {y_with_bn.mean().item():.4f}")
        print(f"  Output std: {y_with_bn.std().item():.4f}")
        print(f"  Output range: [{y_with_bn.min().item():.4f}, {y_with_bn.max().item():.4f}]")
    
    # So s√°nh distribution
    plt.figure(figsize=(15, 5))
    
    # Histogram comparison
    plt.subplot(1, 3, 1)
    plt.hist(y_no_bn.numpy().flatten(), bins=50, alpha=0.7, label='No BN', density=True)
    plt.hist(y_with_bn.numpy().flatten(), bins=50, alpha=0.7, label='With BN', density=True)
    plt.xlabel('Output Values')
    plt.ylabel('Density')
    plt.title('Output Distribution Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Layer-wise activation comparison
    plt.subplot(1, 3, 2)
    layers = ['Input', 'Hidden 1', 'Hidden 2', 'Output']
    
    # Simulate activations through layers
    activations_no_bn = []
    activations_with_bn = []
    
    # Input
    activations_no_bn.append(x.numpy().flatten())
    activations_with_bn.append(x.numpy().flatten())
    
    # Hidden layers (simplified)
    with torch.no_grad():
        # No BN model
        h1_no_bn = F.relu(model_no_bn.fc1(x))
        h2_no_bn = F.relu(model_no_bn.fc2(h1_no_bn)
        activations_no_bn.extend([h1_no_bn.numpy().flatten(), h2_no_bn.numpy().flatten()])
        
        # With BN model
        h1_with_bn = F.relu(model_with_bn.bn1(model_with_bn.fc1(x)))
        h2_with_bn = F.relu(model_with_bn.bn2(model_with_bn.fc2(h1_with_bn)))
        activations_with_bn.extend([h1_with_bn.numpy().flatten(), h2_with_bn.numpy().flatten()])
    
    # Plot activation statistics
    means_no_bn = [np.mean(act) for act in activations_no_bn]
    means_with_bn = [np.mean(act) for act in activations_with_bn]
    
    x_pos = np.arange(len(layers))
    width = 0.35
    
    plt.bar(x_pos - width/2, means_no_bn, width, label='No BN', alpha=0.7)
    plt.bar(x_pos + width/2, means_with_bn, width, label='With BN', alpha=0.7)
    plt.xlabel('Layers')
    plt.ylabel('Mean Activation')
    plt.title('Mean Activation by Layer')
    plt.xticks(x_pos, layers)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Variance comparison
    plt.subplot(1, 3, 3)
    vars_no_bn = [np.var(act) for act in activations_no_bn]
    vars_with_bn = [np.var(act) for act in activations_with_bn]
    
    plt.bar(x_pos - width/2, vars_no_bn, width, label='No BN', alpha=0.7)
    plt.bar(x_pos + width/2, vars_with_bn, width, label='With BN', alpha=0.7)
    plt.xlabel('Layers')
    plt.ylabel('Variance')
    plt.title('Activation Variance by Layer')
    plt.xticks(x_pos, layers)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return model_no_bn, model_with_bn

# V√≠ d·ª• s·ª≠ d·ª•ng
# model_no_bn, model_with_bn = demonstrate_batch_normalization()
```

**Gi·∫£i th√≠ch k·∫øt qu·∫£:**
- **Output Distribution**: Ph√¢n ph·ªëi output c√≥ BN th∆∞·ªùng ·ªïn ƒë·ªãnh h∆°n
- **Mean Activation**: Mean c·ªßa activations qua c√°c layer
- **Activation Variance**: Variance c·ªßa activations, BN gi√∫p ·ªïn ƒë·ªãnh

## üìö T√†i li·ªáu tham kh·∫£o

### L√Ω thuy·∫øt Neural Networks
- [Deep Learning - Ian Goodfellow](https://www.deeplearningbook.org/) - S√°ch gi√°o khoa c∆° b·∫£n
- [Neural Networks and Deep Learning - Michael Nielsen](http://neuralnetworksanddeeplearning.com/) - H∆∞·ªõng d·∫´n tr·ª±c tuy·∫øn

### Optimization v√† Training
- [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a.html) - Xavier/Glorot paper
- [Delving Deep into Rectifiers](https://arxiv.org/abs/1502.01852) - He initialization paper
- [Batch Normalization: Accelerating Deep Network Training](https://arxiv.org/abs/1502.03167) - BatchNorm paper

### Implementation
- [PyTorch Tutorials](https://pytorch.org/tutorials/) - H∆∞·ªõng d·∫´n PyTorch ch√≠nh th·ª©c
- [PyTorch Documentation](https://pytorch.org/docs/stable/) - T√†i li·ªáu PyTorch

## üéØ B√†i t·∫≠p th·ª±c h√†nh

1. **Universal Approximation**: Implement neural network x·∫•p x·ªâ c√°c h√†m ph·ª©c t·∫°p
2. **Backpropagation**: T·ª± implement backpropagation t·ª´ ƒë·∫ßu
3. **Initialization**: So s√°nh hi·ªáu qu·∫£ c√°c ph∆∞∆°ng ph√°p initialization
4. **Batch Normalization**: Implement BatchNorm t·ª´ ƒë·∫ßu v√† so s√°nh v·ªõi PyTorch
5. **Architecture Design**: Thi·∫øt k·∫ø neural network cho c√°c b√†i to√°n c·ª• th·ªÉ

## üöÄ B∆∞·ªõc ti·∫øp theo

Sau khi ho√†n th√†nh Deep Learning c∆° b·∫£n, b·∫°n s·∫Ω:
- Hi·ªÉu s√¢u v·ªÅ l√Ω thuy·∫øt neural networks
- Bi·∫øt c√°ch t·ªëi ∆∞u h√≥a training process
- C√≥ th·ªÉ thi·∫øt k·∫ø ki·∫øn tr√∫c m·∫°ng ph√π h·ª£p
- S·∫µn s√†ng h·ªçc Computer Vision v√† NLP

---

*Ch√∫c b·∫°n tr·ªü th√†nh Deep Learning Engineer xu·∫•t s·∫Øc! üéâ*


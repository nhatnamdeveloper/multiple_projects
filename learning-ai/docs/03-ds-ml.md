# ü§ñ Data Science / Machine Learning - Khoa h·ªçc d·ªØ li·ªáu v√† h·ªçc m√°y

> **M·ª•c ti√™u**: Tr·ªü th√†nh chuy√™n gia Machine Learning, c√≥ kh·∫£ nƒÉng x√¢y d·ª±ng v√† tri·ªÉn khai c√°c m√¥ h√¨nh AI/ML trong th·ª±c t·∫ø

## üìö **1. B·∫£ng k√Ω hi·ªáu (Notation)**

### **Machine Learning:**
- **Dataset**: $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n$ (t·∫≠p d·ªØ li·ªáu training)
- **Feature vector**: $\mathbf{x}_i = [x_{i1}, x_{i2}, \ldots, x_{id}]^T \in \mathbb{R}^d$
- **Target**: $y_i \in \mathbb{R}$ (regression) ho·∫∑c $y_i \in \{0,1\}$ (classification)
- **Model**: $f_\theta: \mathbb{R}^d \rightarrow \mathbb{R}$ v·ªõi parameters $\theta$

### **Loss Functions:**
- **MSE**: $\mathcal{L}(\theta) = \frac{1}{n}\sum_{i=1}^n (y_i - f_\theta(\mathbf{x}_i))^2$
- **Cross-entropy**: $\mathcal{L}(\theta) = -\frac{1}{n}\sum_{i=1}^n [y_i \log(f_\theta(\mathbf{x}_i)) + (1-y_i)\log(1-f_\theta(\mathbf{x}_i))]$
- **Hinge loss**: $\mathcal{L}(\theta) = \frac{1}{n}\sum_{i=1}^n \max(0, 1 - y_i f_\theta(\mathbf{x}_i))$

### **Optimization:**
- **Gradient**: $\nabla_\theta \mathcal{L}(\theta) = [\frac{\partial \mathcal{L}}{\partial \theta_1}, \ldots, \frac{\partial \mathcal{L}}{\partial \theta_p}]^T$
- **Gradient descent**: $\theta_{t+1} = \theta_t - \alpha \nabla_\theta \mathcal{L}(\theta_t)$
- **Learning rate**: $\alpha$ (step size)

### **Evaluation Metrics:**
- **Accuracy**: $\text{Acc} = \frac{\text{Correct predictions}}{\text{Total predictions}}$
- **Precision**: $\text{Prec} = \frac{TP}{TP + FP}$
- **Recall**: $\text{Rec} = \frac{TP}{TP + FN}$
- **F1-score**: $\text{F1} = 2 \cdot \frac{\text{Prec} \cdot \text{Rec}}{\text{Prec} + \text{Rec}}$

### **Feature Engineering:**
- **Feature transformation**: $\phi: \mathbb{R}^d \rightarrow \mathbb{R}^{d'}$
- **Normalization**: $x' = \frac{x - \mu}{\sigma}$
- **Standardization**: $x' = \frac{x - \min(x)}{\max(x) - \min(x)}$

## üìñ **2. Glossary (ƒê·ªãnh nghƒ©a c·ªët l√µi)**

### **Machine Learning Types:**
- **Supervised Learning**: H·ªçc c√≥ gi√°m s√°t - h·ªçc t·ª´ labeled data
- **Unsupervised Learning**: H·ªçc kh√¥ng gi√°m s√°t - h·ªçc patterns t·ª´ unlabeled data
- **Semi-supervised Learning**: H·ªçc b√°n gi√°m s√°t - k·∫øt h·ª£p labeled v√† unlabeled data
- **Reinforcement Learning**: H·ªçc tƒÉng c∆∞·ªùng - h·ªçc t·ª´ rewards v√† penalties

### **Model Components:**
- **Feature**: ƒê·∫∑c tr∆∞ng - thu·ªôc t√≠nh c·ªßa d·ªØ li·ªáu
- **Label**: Nh√£n - gi√° tr·ªã m·ª•c ti√™u c·∫ßn d·ª± ƒëo√°n
- **Parameter**: Tham s·ªë - gi√° tr·ªã ƒë∆∞·ª£c h·ªçc trong qu√° tr√¨nh training
- **Hyperparameter**: Si√™u tham s·ªë - gi√° tr·ªã ƒë∆∞·ª£c set tr∆∞·ªõc training

### **Training Concepts:**
- **Overfitting**: Qu√° kh·ªõp - model h·ªçc qu√° chi ti·∫øt training data
- **Underfitting**: Thi·∫øu kh·ªõp - model kh√¥ng h·ªçc ƒë·ªß t·ª´ training data
- **Bias**: ƒê·ªô ch·ªách - systematic error trong predictions
- **Variance**: Ph∆∞∆°ng sai - sensitivity to fluctuations in training data

### **Evaluation Concepts:**
- **Cross-validation**: Ki·ªÉm ƒë·ªãnh ch√©o - ƒë√°nh gi√° model performance
- **Train/Test split**: Chia d·ªØ li·ªáu training/testing
- **Validation set**: T·∫≠p validation - d√πng ƒë·ªÉ tune hyperparameters
- **Generalization**: Kh·∫£ nƒÉng t·ªïng qu√°t h√≥a - performance tr√™n unseen data

## üìê **3. Th·∫ª thu·∫≠t to√°n - Linear Regression**

### **1. B√†i to√°n & d·ªØ li·ªáu:**
- **B√†i to√°n**: D·ª± ƒëo√°n gi√° tr·ªã li√™n t·ª•c t·ª´ features
- **D·ªØ li·ªáu**: $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n$ v·ªõi $y_i \in \mathbb{R}$
- **·ª®ng d·ª•ng**: Price prediction, demand forecasting, trend analysis

### **2. M√¥ h√¨nh & c√¥ng th·ª©c:**
**Linear Model:**
$$f_\theta(\mathbf{x}) = \mathbf{x}^T \theta + b = \sum_{j=1}^d x_j \theta_j + b$$

**Loss Function (MSE):**
$$\mathcal{L}(\theta) = \frac{1}{n}\sum_{i=1}^n (y_i - f_\theta(\mathbf{x}_i))^2$$

**Closed-form Solution:**
$$\theta^* = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$

### **3. Loss & m·ª•c ti√™u:**
- **M·ª•c ti√™u**: Minimize mean squared error
- **Loss**: $\mathcal{L}(\theta) = \frac{1}{n}\sum_{i=1}^n (y_i - f_\theta(\mathbf{x}_i))^2$

### **4. T·ªëi ∆∞u ho√° & c·∫≠p nh·∫≠t:**
- **Algorithm**: Gradient descent ho·∫∑c closed-form solution
- **C·∫≠p nh·∫≠t**: $\theta_{t+1} = \theta_t - \alpha \nabla_\theta \mathcal{L}(\theta_t)$

### **5. Hyperparams:**
- **Learning rate**: $\alpha$ (th∆∞·ªùng 0.01, 0.1)
- **Regularization**: $\lambda$ (L1/L2 regularization)
- **Max iterations**: S·ªë epoch t·ªëi ƒëa

### **6. ƒê·ªô ph·ª©c t·∫°p:**
- **Time**: $O(nd^2)$ cho closed-form, $O(nd)$ cho gradient descent
- **Space**: $O(d)$ cho storing parameters

### **7. Metrics ƒë√°nh gi√°:**
- **R¬≤**: Coefficient of determination
- **RMSE**: Root Mean Square Error
- **MAE**: Mean Absolute Error
- **MAPE**: Mean Absolute Percentage Error

### **8. ∆Øu / Nh∆∞·ª£c:**
**∆Øu ƒëi·ªÉm:**
- Interpretable v√† simple
- Fast training v√† inference
- Closed-form solution available
- Good baseline model

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Assumes linear relationship
- Sensitive to outliers
- Limited expressiveness
- May underfit complex data

### **9. B·∫´y & m·∫πo:**
- **B·∫´y**: Multicollinearity ‚Üí unstable estimates
- **B·∫´y**: Outliers ‚Üí biased estimates
- **M·∫πo**: Scale features tr∆∞·ªõc training
- **M·∫πo**: Add regularization ƒë·ªÉ prevent overfitting

### **10. Pseudocode:**
```python
def linear_regression(X, y, learning_rate=0.01, max_iter=1000):
    # Initialize parameters
    theta = np.zeros(X.shape[1])
    
    for iteration in range(max_iter):
        # Forward pass
        predictions = X @ theta
        
        # Compute gradients
        gradients = (2/n) * X.T @ (predictions - y)
        
        # Update parameters
        theta = theta - learning_rate * gradients
        
        # Check convergence
        if np.linalg.norm(gradients) < 1e-6:
            break
    
    return theta
```

### **11. Code m·∫´u:**
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

class LinearRegressionExample:
    """Linear Regression Implementation"""
    
    def __init__(self):
        self.model = LinearRegression()
        self.coefficients = None
        self.intercept = None
    
    def generate_sample_data(self, n_samples=100, noise=0.1):
        """Generate synthetic data for demonstration"""
        np.random.seed(42)
        
        # Generate features
        X = np.random.randn(n_samples, 2)
        
        # Generate target with linear relationship + noise
        true_coefficients = np.array([2.5, -1.8])
        true_intercept = 3.2
        y = X @ true_coefficients + true_intercept + noise * np.random.randn(n_samples)
        
        return X, y
    
    def fit_model(self, X, y):
        """Fit linear regression model"""
        self.model.fit(X, y)
        self.coefficients = self.model.coef_
        self.intercept = self.model.intercept_
        
        print(f"Fitted coefficients: {self.coefficients}")
        print(f"Fitted intercept: {self.intercept}")
    
    def evaluate_model(self, X, y):
        """Evaluate model performance"""
        predictions = self.model.predict(X)
        
        # Calculate metrics
        mse = mean_squared_error(y, predictions)
        rmse = np.sqrt(mse)
        r2 = r2_score(y, predictions)
        
        print(f"Mean Squared Error: {mse:.4f}")
        print(f"Root Mean Squared Error: {rmse:.4f}")
        print(f"R¬≤ Score: {r2:.4f}")
        
        return {
            'mse': mse,
            'rmse': rmse,
            'r2': r2,
            'predictions': predictions
        }
    
    def plot_results(self, X, y, predictions):
        """Plot actual vs predicted values"""
        plt.figure(figsize=(12, 4))
        
        # Plot 1: Actual vs Predicted
        plt.subplot(1, 2, 1)
        plt.scatter(y, predictions, alpha=0.6)
        plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
        plt.xlabel('Actual Values')
        plt.ylabel('Predicted Values')
        plt.title('Actual vs Predicted')
        
        # Plot 2: Residuals
        plt.subplot(1, 2, 2)
        residuals = y - predictions
        plt.scatter(predictions, residuals, alpha=0.6)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('Predicted Values')
        plt.ylabel('Residuals')
        plt.title('Residual Plot')
        
        plt.tight_layout()
        plt.show()
    
    def demonstrate_feature_importance(self, feature_names=None):
        """Demonstrate feature importance"""
        if feature_names is None:
            feature_names = [f'Feature_{i}' for i in range(len(self.coefficients))]
        
        importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Coefficient': self.coefficients,
            'Absolute_Coefficient': np.abs(self.coefficients)
        }).sort_values('Absolute_Coefficient', ascending=False)
        
        print("Feature Importance:")
        print(importance_df)
        
        # Plot feature importance
        plt.figure(figsize=(8, 6))
        plt.barh(importance_df['Feature'], importance_df['Coefficient'])
        plt.xlabel('Coefficient Value')
        plt.title('Feature Importance (Linear Regression)')
        plt.tight_layout()
        plt.show()
    
    def run_complete_example(self):
        """Run complete linear regression example"""
        print("=== Linear Regression Example ===\n")
        
        # Generate data
        X, y = self.generate_sample_data()
        print(f"Generated {X.shape[0]} samples with {X.shape[1]} features")
        
        # Split data
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Fit model
        print("\n--- Fitting Model ---")
        self.fit_model(X_train, y_train)
        
        # Evaluate on training set
        print("\n--- Training Set Evaluation ---")
        train_metrics = self.evaluate_model(X_train, y_train)
        
        # Evaluate on test set
        print("\n--- Test Set Evaluation ---")
        test_metrics = self.evaluate_model(X_test, y_test)
        
        # Plot results
        print("\n--- Visualization ---")
        self.plot_results(X_test, y_test, test_metrics['predictions'])
        
        # Feature importance
        print("\n--- Feature Importance ---")
        self.demonstrate_feature_importance()
        
        return {
            'train_metrics': train_metrics,
            'test_metrics': test_metrics
        }
```

### **12. Checklist ki·ªÉm tra nhanh:**
- [ ] Data c√≥ ƒë∆∞·ª£c scale properly?
- [ ] Features c√≥ linear relationship v·ªõi target?
- [ ] Outliers c√≥ ƒë∆∞·ª£c handle?
- [ ] Multicollinearity c√≥ ƒë∆∞·ª£c check?
- [ ] Model performance c√≥ acceptable?

---

# ü§ñ Data Science / Machine Learning - Khoa h·ªçc d·ªØ li·ªáu v√† h·ªçc m√°y

> **M·ª•c ti√™u**: Tr·ªü th√†nh chuy√™n gia Machine Learning, c√≥ kh·∫£ nƒÉng x√¢y d·ª±ng v√† tri·ªÉn khai c√°c m√¥ h√¨nh AI/ML trong th·ª±c t·∫ø

## üìã T·ªïng quan n·ªôi dung

```mermaid
graph TD
    A[üéØ Data Science & ML] --> B[üîß Feature Engineering]
    A --> C[üìä Supervised Learning]
    A --> D[üîç Unsupervised Learning]
    A --> E[‚öñÔ∏è Model Evaluation]
    A --> F[üöÄ Model Deployment]
    
    B --> B1[Temporal Features]
    B --> B2[Categorical Encoding]
    B --> B3[Feature Selection]
    B --> B4[Data Preprocessing]
    
    C --> C1[Linear Models]
    C --> C2[Tree-based Models]
    C --> C3[Neural Networks]
    C --> C4[Ensemble Methods]
    
    D --> D1[Clustering]
    D --> D2[Dimensionality Reduction]
    D --> D3[Association Rules]
    
    E --> E1[Cross-validation]
    E --> E2[Performance Metrics]
    E --> E3[Model Interpretability]
    
    F --> F1[Model Serialization]
    F --> F2[API Development]
    F --> F3[Monitoring & Maintenance]
```

![Data Science & ML Architecture](assets/ds-ml-architecture.svg)

![Data Science & ML Architecture PNG](assets/ds-ml-architecture.png)

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/ds-ml-architecture.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/ds-ml-architecture.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/ds-ml-architecture.png)**

## üîß 1. Feature Engineering v√† Preprocessing

> **"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering."** - Andrew Ng

**Feature Engineering** l√† ngh·ªá thu·∫≠t v√† khoa h·ªçc c·ªßa vi·ªác chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√¥ th√†nh c√°c **ƒë·∫∑c tr∆∞ng (features)** ph√π h·ª£p ƒë·ªÉ cung c·∫•p cho m√¥ h√¨nh machine learning. ƒê√¢y ƒë∆∞·ª£c coi l√† m·ªôt trong nh·ªØng b∆∞·ªõc quan tr·ªçng nh·∫•t quy·∫øt ƒë·ªãnh ƒë·∫øn hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh. Nguy√™n t·∫Øc c·ªët l√µi l√† **"Garbage In, Garbage Out"** - n·∫øu b·∫°n ƒë∆∞a v√†o m√¥ h√¨nh nh·ªØng feature k√©m ch·∫•t l∆∞·ª£ng, m√¥ h√¨nh c≈©ng s·∫Ω cho ra k·∫øt qu·∫£ k√©m ch·∫•t l∆∞·ª£ng.

M·ª•c ti√™u c·ªßa Feature Engineering l√†:
1.  **C·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh**: Cung c·∫•p cho m√¥ h√¨nh nh·ªØng t√≠n hi·ªáu (signals) r√µ r√†ng h∆°n.
2.  **Gi·∫£m ƒë·ªô ph·ª©c t·∫°p**: Gi√∫p m√¥ h√¨nh h·ªçc nhanh h∆°n v√† d·ªÖ di·ªÖn gi·∫£i h∆°n.
3.  **L√†m cho d·ªØ li·ªáu ph√π h·ª£p v·ªõi thu·∫≠t to√°n**: Nhi·ªÅu thu·∫≠t to√°n y√™u c·∫ßu input ·ªü m·ªôt ƒë·ªãnh d·∫°ng c·ª• th·ªÉ (v√≠ d·ª•: d·ªØ li·ªáu s·ªë, ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a).

### 1.1 K·ªπ thu·∫≠t t·∫°o Feature n√¢ng cao (Advanced Feature Engineering)

#### Temporal Features - ƒê·∫∑c tr∆∞ng th·ªùi gian
Khi l√†m vi·ªác v·ªõi d·ªØ li·ªáu c√≥ y·∫øu t·ªë th·ªùi gian (time-series), vi·ªác tr√≠ch xu·∫•t th√¥ng tin t·ª´ c·ªôt ng√†y th√°ng l√† c·ª±c k·ª≥ quan tr·ªçng. C√°c m√¥ h√¨nh tuy·∫øn t√≠nh ho·∫∑c c√¢y quy·∫øt ƒë·ªãnh kh√¥ng th·ªÉ t·ª± "hi·ªÉu" ƒë∆∞·ª£c t√≠nh chu k·ª≥ c·ªßa ng√†y trong tu·∫ßn hay th√°ng trong nƒÉm n·∫øu kh√¥ng c√≥ s·ª± tr·ª£ gi√∫p.

-   **T·∫°i sao c·∫ßn thi·∫øt?** Gi√∫p m√¥ h√¨nh nh·∫≠n bi·∫øt c√°c quy lu·∫≠t c√≥ t√≠nh chu k·ª≥ (v√≠ d·ª•: doanh s·ªë tƒÉng v√†o cu·ªëi tu·∫ßn, gi·∫£m v√†o ƒë·∫ßu tu·∫ßn) v√† c√°c xu h∆∞·ªõng d√†i h·∫°n.
-   **Cyclical Encoding**: K·ªπ thu·∫≠t n√†y ƒë·∫∑c bi·ªát h·ªØu √≠ch. Thay v√¨ m√£ h√≥a th√°ng 12 l√† `12` v√† th√°ng 1 l√† `1` (khi·∫øn m√¥ h√¨nh nghƒ© ch√∫ng ·ªü r·∫•t xa nhau), ta d√πng `sin` v√† `cos` ƒë·ªÉ bi·ªÉu di·ªÖn ch√∫ng tr√™n m·ªôt v√≤ng tr√≤n, th·ªÉ hi·ªán ƒë√∫ng b·∫£n ch·∫•t tu·∫ßn ho√†n.

```python
import pandas as pd
import numpy as np

def create_temporal_features(df, date_column):
    """
    T·∫°o c√°c ƒë·∫∑c tr∆∞ng th·ªùi gian to√†n di·ªán t·ª´ m·ªôt c·ªôt ng√†y th√°ng.
    """
    df = df.copy()
    # ƒê·∫£m b·∫£o c·ªôt ng√†y th√°ng ·ªü ƒë√∫ng ƒë·ªãnh d·∫°ng datetime
    df[date_column] = pd.to_datetime(df[date_column])
    
    # 1. C√°c feature c∆° b·∫£n
    df[f'{date_column}_year'] = df[date_column].dt.year
    df[f'{date_column}_month'] = df[date_column].dt.month
    df[f'{date_column}_day'] = df[date_column].dt.day
    df[f'{date_column}_dayofweek'] = df[date_column].dt.dayofweek # 0=Th·ª© 2, 6=Ch·ªß Nh·∫≠t
    df[f'{date_column}_dayofyear'] = df[date_column].dt.dayofyear
    df[f'{date_column}_weekofyear'] = df[date_column].dt.isocalendar().week.astype(int)
    df[f'{date_column}_quarter'] = df[date_column].dt.quarter
    
    # 2. M√£ h√≥a tu·∫ßn ho√†n (Cyclical Encoding)
    # Gi√∫p m√¥ h√¨nh hi·ªÉu r·∫±ng th√°ng 12 v√† th√°ng 1 l√† li·ªÅn k·ªÅ
    df[f'{date_column}_month_sin'] = np.sin(2 * np.pi * df[f'{date_column}_month'] / 12)
    df[f'{date_column}_month_cos'] = np.cos(2 * np.pi * df[f'{date_column}_month'] / 12)
    # T∆∞∆°ng t·ª± cho ng√†y trong tu·∫ßn
    df[f'{date_column}_dayofweek_sin'] = np.sin(2 * np.pi * df[f'{date_column}_dayofweek'] / 7)
    df[f'{date_column}_dayofweek_cos'] = np.cos(2 * np.pi * df[f'{date_column}_dayofweek'] / 7)
    
    # 3. C√°c feature d·ª±a tr√™n logic nghi·ªáp v·ª•
    df[f'{date_column}_is_weekend'] = (df[f'{date_column}_dayofweek'] >= 5).astype(int)
    
    return df

# V√≠ d·ª•
df_demo = pd.DataFrame({'sales_date': pd.date_range('2023-01-01', periods=5, freq='D')})
df_featured = create_temporal_features(df_demo, 'sales_date')
print(df_featured.head())
```

#### M√£ h√≥a d·ªØ li·ªáu ph√¢n lo·∫°i (Categorical Encoding)

C√°c m√¥ h√¨nh ML y√™u c·∫ßu input l√† s·ªë. Do ƒë√≥, ch√∫ng ta ph·∫£i chuy·ªÉn c√°c bi·∫øn ph√¢n lo·∫°i (nh∆∞ "Th√†nh ph·ªë", "Lo·∫°i s·∫£n ph·∫©m") th√†nh d·∫°ng s·ªë.

##### So s√°nh c√°c ph∆∞∆°ng ph√°p Encoding
| Ph∆∞∆°ng ph√°p | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm | Khi n√†o d√πng? |
| :--- | :--- | :--- | :--- |
| **One-Hot Encoding** | - Kh√¥ng t·∫°o ra th·ª© t·ª± gi·∫£. <br>- D·ªÖ di·ªÖn gi·∫£i. | - T·∫°o ra nhi·ªÅu c·ªôt m·ªõi (curse of dimensionality). <br>- G√¢y v·∫•n ƒë·ªÅ v·ªõi c√°c thu·∫≠t to√°n d·ª±a tr√™n c√¢y n·∫øu c√≥ qu√° nhi·ªÅu c·ªôt. | Khi s·ªë l∆∞·ª£ng categories √≠t (v√≠ d·ª•: < 15). |
| **Label Encoding** | - ƒê∆°n gi·∫£n, kh√¥ng l√†m tƒÉng s·ªë chi·ªÅu. | - T·∫°o ra m·ªôt th·ª© t·ª± gi·∫£ (v√≠ d·ª•: `H√† N·ªôi`=0, `HCM`=1, `ƒê√† N·∫µng`=2 ng·ª• √Ω `ƒêN > HCM > HN`). | Ch·ªâ d√πng cho c√°c bi·∫øn c√≥ th·ª© t·ª± t·ª± nhi√™n (ordinal variables), v√≠ d·ª•: `['Low', 'Medium', 'High']`. **Tr√°nh d√πng cho bi·∫øn kh√¥ng c√≥ th·ª© t·ª±.** |
| **Target Encoding** | - Kh√¥ng t·∫°o th√™m c·ªôt. <br>- M√£ h√≥a th√¥ng tin t·ª´ bi·∫øn m·ª•c ti√™u (target) v√†o feature. | - **R·∫•t d·ªÖ g√¢y data leakage v√† overfitting** n·∫øu kh√¥ng c·∫©n th·∫≠n. | D√πng cho bi·∫øn c√≥ s·ªë l∆∞·ª£ng category l·ªõn (high cardinality). Lu√¥n ph·∫£i k·∫øt h·ª£p v·ªõi cross-validation ƒë·ªÉ gi·∫£m leakage. |
| **Frequency Encoding** | - ƒê∆°n gi·∫£n. <br>- N·∫Øm b·∫Øt ƒë∆∞·ª£c t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa category. | - C√°c category c√≥ c√πng t·∫ßn su·∫•t s·∫Ω ƒë∆∞·ª£c m√£ h√≥a gi·ªëng nhau. | Khi t·∫ßn su·∫•t c·ªßa category l√† m·ªôt t√≠n hi·ªáu quan tr·ªçng. |

```python
import category_encoders as ce
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# D·ªØ li·ªáu m·∫´u
data = pd.DataFrame({
    'city': ['Hanoi', 'HCM', 'Danang', 'Hanoi', 'HCM'],
    'quality': ['Good', 'Great', 'Good', 'Bad', 'Great'],
    'target': [1, 1, 0, 0, 1]
})

# 1. One-Hot Encoding
ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
city_ohe = ohe.fit_transform(data[['city']])
print("One-Hot Encoded City:\n", city_ohe)

# 2. Label Encoding (CH·ªà D√ôNG CHO BI·∫æN C√ì TH·ª® T·ª∞)
# Gi·∫£ s·ª≠ 'quality' c√≥ th·ª© t·ª±: Bad < Good < Great
quality_map = {'Bad': 0, 'Good': 1, 'Great': 2}
data['quality_encoded'] = data['quality'].map(quality_map)
print("\nLabel Encoded Quality:\n", data[['quality', 'quality_encoded']])

# 3. Target Encoding (c·∫©n th·∫≠n v·ªõi data leakage)
target_encoder = ce.TargetEncoder(cols=['city'])
city_target_encoded = target_encoder.fit_transform(data['city'], data['target'])
print("\nTarget Encoded City:\n", city_target_encoded)
```

### 1.2 Chu·∫©n h√≥a v√† L·ª±a ch·ªçn Feature (Feature Scaling & Selection)

#### Chu·∫©n h√≥a Feature (Feature Scaling)
- **T·∫°i sao c·∫ßn?** Nhi·ªÅu thu·∫≠t to√°n ML (nh∆∞ Linear Regression, SVM, Neural Networks) r·∫•t nh·∫°y c·∫£m v·ªõi s·ª± kh√°c bi·ªát v·ªÅ thang ƒëo c·ªßa c√°c feature. V√≠ d·ª•, m·ªôt feature `tu·ªïi` (0-100) v√† m·ªôt feature `thu_nh·∫≠p` (0-1,000,000,000) s·∫Ω khi·∫øn m√¥ h√¨nh "∆∞u ti√™n" `thu_nh·∫≠p` h∆°n v√¨ gi√° tr·ªã c·ªßa n√≥ l·ªõn h∆°n nhi·ªÅu. Feature scaling ƒë∆∞a t·∫•t c·∫£ c√°c feature v·ªÅ c√πng m·ªôt thang ƒëo.
- **Khi n√†o c·∫ßn?** H·∫ßu h·∫øt c√°c thu·∫≠t to√°n, **tr·ª´ c√°c thu·∫≠t to√°n d·ª±a tr√™n c√¢y** (Decision Tree, Random Forest, Gradient Boosting) v√¨ ch√∫ng kh√¥ng quan t√¢m ƒë·∫øn ƒë·ªô l·ªõn c·ªßa feature, ch·ªâ quan t√¢m ƒë·∫øn ƒëi·ªÉm chia.

##### C√°c ph∆∞∆°ng ph√°p Scaling
1.  **StandardScaler (Z-score Normalization)**:
    -   **C√¥ng th·ª©c**: $x' = \frac{x - \mu}{\sigma}$ (tr·ª´ ƒëi trung b√¨nh v√† chia cho ƒë·ªô l·ªách chu·∫©n).
    -   **K·∫øt qu·∫£**: D·ªØ li·ªáu s·∫Ω c√≥ trung b√¨nh l√† 0 v√† ƒë·ªô l·ªách chu·∫©n l√† 1.
    -   **Khi n√†o d√πng?**: Ph√π h·ª£p v·ªõi h·∫ßu h·∫øt c√°c tr∆∞·ªùng h·ª£p, ƒë·∫∑c bi·ªát khi d·ªØ li·ªáu c√≥ ph√¢n ph·ªëi g·∫ßn chu·∫©n. N√≥ kh√¥ng gi·ªõi h·∫°n gi√° tr·ªã trong m·ªôt kho·∫£ng c·ª• th·ªÉ.

2.  **MinMaxScaler (Normalization)**:
    -   **C√¥ng th·ª©c**: $x' = \frac{x - \min(x)}{\max(x) - \min(x)}$
    -   **K·∫øt qu·∫£**: D·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c ƒë∆∞a v·ªÅ kho·∫£ng `[0, 1]`.
    -   **Khi n√†o d√πng?**: H·ªØu √≠ch cho c√°c thu·∫≠t to√°n y√™u c·∫ßu d·ªØ li·ªáu trong m·ªôt kho·∫£ng nh·∫•t ƒë·ªãnh (v√≠ d·ª•: Neural Networks v·ªõi activation Sigmoid/Tanh). Tuy nhi√™n, n√≥ r·∫•t nh·∫°y c·∫£m v·ªõi outliers.

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

data_to_scale = np.array([[100], [200], [500], [1000], [5000]], dtype=float)

# StandardScaler
scaler_std = StandardScaler()
scaled_std = scaler_std.fit_transform(data_to_scale)
print("StandardScaler (Z-score):\n", scaled_std.flatten())

# MinMaxScaler
scaler_minmax = MinMaxScaler()
scaled_minmax = scaler_minmax.fit_transform(data_to_scale)
print("\nMinMaxScaler:\n", scaled_minmax.flatten())
```

#### L·ª±a ch·ªçn Feature (Feature Selection)

> **M·ª•c ti√™u**: Ch·ªçn ra m·ªôt t·∫≠p h·ª£p con c√°c feature quan tr·ªçng nh·∫•t ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh, gi·∫£m th·ªùi gian hu·∫•n luy·ªán v√† tr√°nh overfitting.

##### C√°c lo·∫°i ph∆∞∆°ng ph√°p Feature Selection
1.  **Filter Methods**:
    -   **C√°ch ho·∫°t ƒë·ªông**: ƒê√°nh gi√° v√† x·∫øp h·∫°ng c√°c feature d·ª±a tr√™n c√°c b√†i ki·ªÉm tra th·ªëng k√™ (nh∆∞ t∆∞∆°ng quan, chi-square, mutual information) **tr∆∞·ªõc khi** hu·∫•n luy·ªán m√¥ h√¨nh.
    -   **∆Øu ƒëi·ªÉm**: Nhanh, kh√¥ng ph·ª• thu·ªôc v√†o m√¥ h√¨nh.
    -   **Nh∆∞·ª£c ƒëi·ªÉm**: C√≥ th·ªÉ b·ªè l·ª° c√°c m·ªëi quan h·ªá t∆∞∆°ng t√°c gi·ªØa c√°c feature (v√≠ d·ª•: feature A v√† B ri√™ng l·∫ª th√¨ y·∫øu, nh∆∞ng k·∫øt h·ª£p l·∫°i th√¨ r·∫•t m·∫°nh).
    -   **V√≠ d·ª•**: `SelectKBest` v·ªõi `f_classif` ho·∫∑c `mutual_info_classif`.

2.  **Wrapper Methods**:
    -   **C√°ch ho·∫°t ƒë·ªông**: S·ª≠ d·ª•ng m·ªôt m√¥ h√¨nh ML ƒë·ªÉ "b·ªçc" v√† ƒë√°nh gi√° c√°c t·∫≠p con feature kh√°c nhau. Coi vi·ªác ch·ªçn feature nh∆∞ m·ªôt b√†i to√°n t√¨m ki·∫øm.
    -   **∆Øu ƒëi·ªÉm**: Th∆∞·ªùng cho k·∫øt qu·∫£ t·ªët h∆°n Filter methods v√¨ x√©t ƒë·∫øn t∆∞∆°ng t√°c feature.
    -   **Nh∆∞·ª£c ƒëi·ªÉm**: R·∫•t t·ªën k√©m v·ªÅ m·∫∑t t√≠nh to√°n.
    -   **V√≠ d·ª•**: Recursive Feature Elimination (RFE).

3.  **Embedded Methods**:
    -   **C√°ch ho·∫°t ƒë·ªông**: Qu√° tr√¨nh ch·ªçn feature ƒë∆∞·ª£c "nh√∫ng" ngay trong qu√° tr√¨nh hu·∫•n luy·ªán m√¥ h√¨nh.
    -   **∆Øu ƒëi·ªÉm**: Hi·ªáu qu·∫£ h∆°n Wrapper methods, n·∫Øm b·∫Øt ƒë∆∞·ª£c t∆∞∆°ng t√°c feature.
    -   **Nh∆∞·ª£c ƒëi·ªÉm**: Ph·ª• thu·ªôc v√†o m√¥ h√¨nh c·ª• th·ªÉ.
    -   **V√≠ d·ª•**: **Lasso (L1) Regression** t·ª± ƒë·ªông "zero out" c√°c feature kh√¥ng quan tr·ªçng. **Random Forest** cung c·∫•p `feature_importances_`.

```python
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Gi·∫£ s·ª≠ X, y l√† d·ªØ li·ªáu c·ªßa b·∫°n

# 1. Filter Method
kbest = SelectKBest(score_func=f_classif, k=5)
X_kbest = kbest.fit_transform(X, y)
selected_indices_filter = kbest.get_support(indices=True)
print(f"Filter - 5 features t·ªët nh·∫•t: {selected_indices_filter}")

# 2. Wrapper Method
estimator = LogisticRegression(max_iter=1000)
rfe = RFE(estimator, n_features_to_select=5, step=1)
rfe.fit(X, y)
selected_indices_wrapper = rfe.get_support(indices=True)
print(f"Wrapper (RFE) - 5 features t·ªët nh·∫•t: {selected_indices_wrapper}")

# 3. Embedded Method
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)
importances = rf.feature_importances_
# S·∫Øp x·∫øp v√† l·∫•y 5 features quan tr·ªçng nh·∫•t
selected_indices_embedded = np.argsort(importances)[::-1][:5]
print(f"Embedded (RF) - 5 features t·ªët nh·∫•t: {selected_indices_embedded}")
```
## üìä 2. Supervised Learning - H·ªçc c√≥ gi√°m s√°t

> **T∆∞ t∆∞·ªüng c·ªët l√µi**: H·ªçc c√≥ gi√°m s√°t (Supervised Learning) gi·ªëng nh∆∞ vi·ªác m·ªôt h·ªçc sinh h·ªçc b√†i v·ªõi m·ªôt ng∆∞·ªùi th·∫ßy c√≥ ƒë√°p √°n. Ch√∫ng ta cung c·∫•p cho m√¥ h√¨nh m·ªôt t·∫≠p d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c **g√°n nh√£n (labeled)**, bao g·ªìm c·∫£ "c√¢u h·ªèi" (input features, **X**) v√† "c√¢u tr·∫£ l·ªùi ƒë√∫ng" (output target, **y**). M·ª•c ti√™u c·ªßa m√¥ h√¨nh l√† h·ªçc ra m·ªôt **h√†m √°nh x·∫° `f`** sao cho `f(X)` c√≥ th·ªÉ d·ª± ƒëo√°n `y` m·ªôt c√°ch ch√≠nh x√°c nh·∫•t c√≥ th·ªÉ, ngay c·∫£ v·ªõi nh·ªØng "c√¢u h·ªèi" m·ªõi m√† n√≥ ch∆∞a t·ª´ng th·∫•y.

Supervised Learning bao g·ªìm 2 lo·∫°i b√†i to√°n ch√≠nh:
1.  **Regression (H·ªìi quy)**: D·ª± ƒëo√°n m·ªôt gi√° tr·ªã li√™n t·ª•c.
    -   *V√≠ d·ª•*: D·ª± ƒëo√°n gi√° nh√†, nhi·ªát ƒë·ªô ng√†y mai.
2.  **Classification (Ph√¢n lo·∫°i)**: D·ª± ƒëo√°n m·ªôt nh√£n ph√¢n lo·∫°i (category).
    -   *V√≠ d·ª•*: Email l√† spam hay kh√¥ng spam, m·ªôt b·ª©c ·∫£nh ch·ª©a ch√≥ hay m√®o.

### 2.1 Linear Models - M√¥ h√¨nh tuy·∫øn t√≠nh

> **Linear Models** l√† nh√≥m m√¥ h√¨nh ƒë∆°n gi·∫£n nh·∫•t, gi·∫£ ƒë·ªãnh r·∫±ng m·ªëi quan h·ªá gi·ªØa c√°c input feature v√† output target l√† m·ªôt ƒë∆∞·ªùng th·∫≥ng (ho·∫∑c m·ªôt si√™u ph·∫≥ng trong kh√¥ng gian nhi·ªÅu chi·ªÅu). Ch√∫ng r·∫•t nhanh, d·ªÖ di·ªÖn gi·∫£i v√† l√† m·ªôt baseline tuy·ªát v·ªùi cho b·∫•t k·ª≥ b√†i to√°n n√†o.

#### Linear Regression v√† Regularization

Trong Linear Regression, ch√∫ng ta c·ªë g·∫Øng t√¨m c√°c h·ªá s·ªë (coefficients) ƒë·ªÉ t·ªëi thi·ªÉu h√≥a t·ªïng b√¨nh ph∆∞∆°ng l·ªói. Tuy nhi√™n, n·∫øu m√¥ h√¨nh qu√° ph·ª©c t·∫°p ho·∫∑c c√≥ qu√° nhi·ªÅu feature, n√≥ c√≥ th·ªÉ b·ªã **overfitting**: h·ªçc thu·ªôc l√≤ng d·ªØ li·ªáu training nh∆∞ng ho·∫°t ƒë·ªông k√©m tr√™n d·ªØ li·ªáu m·ªõi. **Regularization** l√† m·ªôt k·ªπ thu·∫≠t ƒë·ªÉ ch·ªëng l·∫°i overfitting b·∫±ng c√°ch "ph·∫°t" c√°c m√¥ h√¨nh c√≥ h·ªá s·ªë qu√° l·ªõn, bu·ªôc ch√∫ng ph·∫£i ƒë∆°n gi·∫£n h∆°n.

##### ƒê√°nh ƒë·ªïi Bias-Variance (Bias-Variance Tradeoff)
-   **Bias (ƒê·ªô ch·ªách)**: Sai s·ªë do c√°c gi·∫£ ƒë·ªãnh ƒë∆°n gi·∫£n h√≥a c·ªßa m√¥ h√¨nh. M√¥ h√¨nh c√≥ bias cao c√≥ th·ªÉ **underfit** (kh√¥ng n·∫Øm b·∫Øt ƒë∆∞·ª£c quy lu·∫≠t c·ªßa d·ªØ li·ªáu).
-   **Variance (Ph∆∞∆°ng sai)**: M·ª©c ƒë·ªô thay ƒë·ªïi c·ªßa d·ª± ƒëo√°n n·∫øu ta hu·∫•n luy·ªán m√¥ h√¨nh tr√™n c√°c t·∫≠p d·ªØ li·ªáu training kh√°c nhau. M√¥ h√¨nh c√≥ variance cao th∆∞·ªùng **overfit** (qu√° nh·∫°y c·∫£m v·ªõi nhi·ªÖu trong d·ªØ li·ªáu training).
-   **M·ª•c ti√™u**: T√¨m ƒëi·ªÉm c√¢n b·∫±ng gi·ªØa Bias v√† Variance. Regularization gi√∫p gi·∫£m Variance kosztem (t·∫°i chi ph√≠ c·ªßa) vi·ªác tƒÉng m·ªôt ch√∫t Bias.

##### C√°c lo·∫°i Regularization
1.  **Ridge Regression (L2 Regularization)**:
    -   **C√°ch ho·∫°t ƒë·ªông**: Th√™m v√†o h√†m m·∫•t m√°t m·ªôt th√†nh ph·∫ßn b·∫±ng t·ªïng b√¨nh ph∆∞∆°ng c·ªßa c√°c h·ªá s·ªë (`alpha * Œ£(coefficient¬≤)`).
    -   **T√°c d·ª•ng**: "Co" c√°c h·ªá s·ªë v·ªÅ g·∫ßn 0, nh∆∞ng kh√¥ng bao gi·ªù b·∫±ng 0. N√≥ l√†m cho m√¥ h√¨nh √≠t nh·∫°y c·∫£m h∆°n v·ªõi c√°c feature ri√™ng l·∫ª, gi√∫p gi·∫£m variance v√† ch·ªëng overfitting.
    -   **Khi d√πng**: Khi b·∫°n c√≥ nhi·ªÅu feature v√† tin r·∫±ng h·∫ßu h·∫øt ch√∫ng ƒë·ªÅu c√≥ √≠ch.

2.  **Lasso Regression (L1 Regularization)**:
    -   **C√°ch ho·∫°t ƒë·ªông**: Th√™m v√†o h√†m m·∫•t m√°t m·ªôt th√†nh ph·∫ßn b·∫±ng t·ªïng gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa c√°c h·ªá s·ªë (`alpha * Œ£|coefficient|`).
    -   **T√°c d·ª•ng**: C√≥ kh·∫£ nƒÉng ƒë∆∞a m·ªôt s·ªë h·ªá s·ªë v·ªÅ **ch√≠nh x√°c b·∫±ng 0**. ƒêi·ªÅu n√†y ƒë·ªìng nghƒ©a v·ªõi vi·ªác n√≥ t·ª± ƒë·ªông **l·ª±a ch·ªçn feature (feature selection)**, lo·∫°i b·ªè c√°c feature kh√¥ng quan tr·ªçng.
    -   **Khi d√πng**: Khi b·∫°n nghi ng·ªù r·∫±ng ch·ªâ m·ªôt v√†i feature l√† th·ª±c s·ª± quan tr·ªçng.

3.  **Elastic Net**:
    -   **C√°ch ho·∫°t ƒë·ªông**: K·∫øt h·ª£p c·∫£ hai lo·∫°i L1 v√† L2.
    -   **T√°c d·ª•ng**: T·∫≠n d·ª•ng ∆∞u ƒëi·ªÉm c·ªßa c·∫£ hai: v·ª´a c√≥ th·ªÉ lo·∫°i b·ªè feature kh√¥ng c·∫ßn thi·∫øt (nh∆∞ Lasso), v·ª´a x·ª≠ l√Ω t·ªët khi c√°c feature c√≥ t∆∞∆°ng quan cao v·ªõi nhau (nh∆∞ Ridge).

```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd

def compare_linear_models(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge (L2)': Ridge(alpha=1.0),
        'Lasso (L1)': Lasso(alpha=0.1),
        'Elastic Net': ElasticNet(alpha=0.1, l1_ratio=0.5)
    }
    
    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        print(f"\n--- {name} ---")
        if hasattr(model, 'coef_'):
            # ƒê·∫øm s·ªë feature b·ªã lo·∫°i b·ªè (h·ªá s·ªë b·∫±ng 0)
            zero_coeffs = np.sum(np.abs(model.coef_) < 1e-6)
            print(f"H·ªá s·ªë: {model.coef_[:3]}...") # In 3 h·ªá s·ªë ƒë·∫ßu
            print(f"S·ªë feature b·ªã lo·∫°i b·ªè: {zero_coeffs}")

# V√≠ d·ª• s·ª≠ d·ª•ng (y√™u c·∫ßu c√≥ X v√† y t·ª´ d·ªØ li·ªáu th·ª±c t·∫ø)
# compare_linear_models(X, y)
```

### 2.2 Tree-based Models - M√¥ h√¨nh d·ª±a tr√™n c√¢y

> **Tree-based Models** ph√¢n chia kh√¥ng gian feature th√†nh c√°c v√πng nh·ªè h∆°n b·∫±ng m·ªôt chu·ªói c√°c quy t·∫Øc "if-then-else" ƒë∆°n gi·∫£n, gi·ªëng nh∆∞ m·ªôt c√¢y quy·∫øt ƒë·ªãnh. Ch√∫ng m·∫°nh m·∫Ω, d·ªÖ di·ªÖn gi·∫£i v√† kh√¥ng y√™u c·∫ßu feature scaling.

#### Decision Trees (C√¢y quy·∫øt ƒë·ªãnh)
-   **C√°ch ho·∫°t ƒë·ªông**: T·∫°i m·ªói n√∫t (node), c√¢y t√¨m ra m·ªôt feature v√† m·ªôt ng∆∞·ª°ng chia (split point) ƒë·ªÉ ph√¢n t√°ch d·ªØ li·ªáu sao cho c√°c nh√≥m con tr·ªü n√™n "thu·∫ßn khi·∫øt" nh·∫•t c√≥ th·ªÉ.
-   **ƒê·ªô "thu·∫ßn khi·∫øt" (Impurity)**: Th∆∞·ªùng ƒë∆∞·ª£c ƒëo b·∫±ng **Gini Impurity** ho·∫∑c **Entropy**. M·ª•c ti√™u l√† gi·∫£m ƒë·ªô "b·∫•t ƒë·ªãnh" sau m·ªói l·∫ßn chia.
-   **V·∫•n ƒë·ªÅ**: M·ªôt c√¢y quy·∫øt ƒë·ªãnh ƒë∆°n l·∫ª r·∫•t d·ªÖ b·ªã **overfitting**. N√≥ c√≥ th·ªÉ t·∫°o ra m·ªôt c√¢y r·∫•t s√¢u v√† ph·ª©c t·∫°p ƒë·ªÉ ph√¢n lo·∫°i ho√†n h·∫£o d·ªØ li·ªáu training.

#### Random Forest (R·ª´ng ng·∫´u nhi√™n)
-   **T∆∞ t∆∞·ªüng**: "Tr√≠ tu·ªá t·∫≠p th·ªÉ". Thay v√¨ ch·ªâ d·ª±a v√†o m·ªôt c√¢y quy·∫øt ƒë·ªãnh, Random Forest x√¢y d·ª±ng m·ªôt "khu r·ª´ng" g·ªìm nhi·ªÅu c√¢y quy·∫øt ƒë·ªãnh kh√°c nhau v√† l·∫•y k·∫øt qu·∫£ trung b√¨nh (h·ªìi quy) ho·∫∑c b·ªè phi·∫øu ƒëa s·ªë (ph√¢n lo·∫°i).
-   **T·∫°i sao hi·ªáu qu·∫£?**: N√≥ s·ª≠ d·ª•ng hai k·ªπ thu·∫≠t ch√≠nh ƒë·ªÉ gi·∫£m overfitting v√† variance:
    1.  **Bagging (Bootstrap Aggregating)**: M·ªói c√¢y ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n m·ªôt m·∫´u ng·∫´u nhi√™n *c√≥ l·∫∑p l·∫°i* (bootstrap sample) t·ª´ d·ªØ li·ªáu g·ªëc. ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o c√°c c√¢y trong r·ª´ng l√† kh√°c nhau.
    2.  **Feature Randomness**: T·∫°i m·ªói l·∫ßn chia node, m·ªói c√¢y ch·ªâ ƒë∆∞·ª£c ph√©p xem x√©t m·ªôt t·∫≠p con ng·∫´u nhi√™n c√°c feature. ƒêi·ªÅu n√†y ngƒÉn c√°c c√¢y tr·ªü n√™n qu√° gi·ªëng nhau n·∫øu c√≥ m·ªôt v√†i feature r·∫•t m·∫°nh.
-   **K·∫øt qu·∫£**: T·∫°o ra m·ªôt m√¥ h√¨nh m·∫°nh m·∫Ω, √≠t b·ªã overfitting h∆°n nhi·ªÅu so v·ªõi m·ªôt c√¢y quy·∫øt ƒë·ªãnh ƒë∆°n l·∫ª.

#### Gradient Boosting
-   **T∆∞ t∆∞·ªüng**: "H·ªçc t·ª´ l·ªói sai". Gradient Boosting c≈©ng x√¢y d·ª±ng nhi·ªÅu c√¢y, nh∆∞ng theo m·ªôt c√°ch **tu·∫ßn t·ª± (sequentially)**.
-   **C√°ch ho·∫°t ƒë·ªông**:
    1.  B·∫Øt ƒë·∫ßu v·ªõi m·ªôt m√¥ h√¨nh r·∫•t ƒë∆°n gi·∫£n (v√≠ d·ª•: d·ª± ƒëo√°n gi√° tr·ªã trung b√¨nh c·ªßa t·∫•t c·∫£ target).
    2.  X√¢y d·ª±ng m·ªôt c√¢y quy·∫øt ƒë·ªãnh m·ªõi ƒë·ªÉ **d·ª± ƒëo√°n ph·∫ßn l·ªói (residuals)** c·ªßa m√¥ h√¨nh tr∆∞·ªõc ƒë√≥.
    3.  Th√™m d·ª± ƒëo√°n c·ªßa c√¢y m·ªõi n√†y v√†o m√¥ h√¨nh t·ªïng th·ªÉ (v·ªõi m·ªôt learning rate nh·ªè).
    4.  L·∫∑p l·∫°i qu√° tr√¨nh, m·ªói c√¢y m·ªõi t·∫≠p trung v√†o vi·ªác s·ª≠a nh·ªØng l·ªói m√† c√°c c√¢y tr∆∞·ªõc ƒë√≥ v·∫´n c√≤n m·∫Øc ph·∫£i.
-   **K·∫øt qu·∫£**: Th∆∞·ªùng cho hi·ªáu su·∫•t r·∫•t cao, l√† m·ªôt trong nh·ªØng thu·∫≠t to√°n h√†ng ƒë·∫ßu trong c√°c cu·ªôc thi Kaggle. Tuy nhi√™n, n√≥ nh·∫°y c·∫£m h∆°n v·ªõi hyperparameter v√† c√≥ th·ªÉ b·ªã overfitting n·∫øu s·ªë l∆∞·ª£ng c√¢y qu√° l·ªõn.

```python
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import pandas as pd
import numpy as np

def analyze_tree_models(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    models = {
        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),
        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)
    }
    
    for name, model in models.items():
        print(f"\n--- Training {name} ---")
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        
        print(f"Test R¬≤: {r2:.4f}")
        print(f"Test RMSE: {rmse:.4f}")

        if hasattr(model, 'feature_importances_'):
            importance_df = pd.DataFrame({
                'Feature': X.columns,
                'Importance': model.feature_importances_
            }).sort_values('Importance', ascending=False)
            print("Top 5 Features:\n", importance_df.head())

# V√≠ d·ª• s·ª≠ d·ª•ng
# analyze_tree_models(X, y)
```

**Gi·∫£i th√≠ch c√°c m√¥ h√¨nh d·ª±a tr√™n c√¢y:**
- **Random Forest**: M·ªôt t·∫≠p h·ª£p c√°c c√¢y quy·∫øt ƒë·ªãnh ƒë·ªôc l·∫≠p, gi·∫£m variance v√† overfitting th√¥ng qua bagging v√† feature randomness.
- **Gradient Boosting**: M·ªôt chu·ªói c√°c c√¢y quy·∫øt ƒë·ªãnh, trong ƒë√≥ m·ªói c√¢y h·ªçc c√°ch s·ª≠a l·ªói c·ªßa c√¢y tr∆∞·ªõc ƒë√≥, t·∫°o ra m·ªôt m√¥ h√¨nh m·∫°nh m·∫Ω t·ª´ nhi·ªÅu m√¥ h√¨nh y·∫øu.
- **Feature Importance**: C·∫£ hai m√¥ h√¨nh ƒë·ªÅu cung c·∫•p m·ªôt th∆∞·ªõc ƒëo v·ªÅ t·∫ßm quan tr·ªçng c·ªßa m·ªói feature, gi√∫p di·ªÖn gi·∫£i m√¥ h√¨nh.

## ‚öñÔ∏è 3. Model Evaluation - ƒê√°nh gi√° m√¥ h√¨nh

### 3.1 Cross-validation Strategies - Chi·∫øn l∆∞·ª£c cross-validation

> **Cross-validation** l√† k·ªπ thu·∫≠t ƒë√°nh gi√° m√¥ h√¨nh b·∫±ng c√°ch chia d·ªØ li·ªáu th√†nh nhi·ªÅu folds v√† train/test nhi·ªÅu l·∫ßn.

#### Time Series Cross-validation

```python
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
import pandas as pd

def time_series_cv_evaluation(model, X, y, n_splits=5):
    """
    ƒê√°nh gi√° m√¥ h√¨nh v·ªõi time series cross-validation
    """
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    cv_scores = []
    
    print("‚è∞ TIME SERIES CROSS-VALIDATION")
    print("=" * 50)
    
    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
        
        # Train m√¥ h√¨nh
        model.fit(X_train, y_train)
        
        # D·ª± ƒëo√°n
        y_pred = model.predict(X_test)
        
        # T√≠nh metrics
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        
        cv_scores.append({
            'Fold': fold,
            'Train Size': len(train_idx),
            'Test Size': len(test_idx),
            'RMSE': rmse,
            'MAE': mae
        })
        
        print(f"Fold {fold}: Train={len(train_idx)}, Test={len(test_idx)}, RMSE={rmse:.4f}, MAE={mae:.4f}")
    
    cv_df = pd.DataFrame(cv_scores)
    
    print(f"\nüìä CROSS-VALIDATION SUMMARY")
    print("=" * 40)
    print(f"Mean RMSE: {cv_df['RMSE'].mean():.4f} ¬± {cv_df['RMSE'].std():.4f}")
    print(f"Mean MAE: {cv_df['MAE'].mean():.4f} ¬± {cv_df['MAE'].std():.4f}")
    
    return cv_df

# V√≠ d·ª• s·ª≠ d·ª•ng
# ƒê·ªÉ ch·∫°y v√≠ d·ª• n√†y, b·∫°n c·∫ßn c√≥ m·ªôt model ƒë√£ hu·∫•n luy·ªán v√† d·ªØ li·ªáu X, y.
# from sklearn.linear_model import LinearRegression
# model = LinearRegression()
# # T·∫°o d·ªØ li·ªáu gi·∫£ v·ªõi c·ªôt th·ªùi gian ƒë·ªÉ d√πng TimeSeriesSplit
# X_dummy = pd.DataFrame(np.random.rand(100, 3), columns=['f1', 'f2', 'f3'])
# y_dummy = pd.Series(np.random.rand(100))
# cv_results = time_series_cv_evaluation(model, X_dummy, y_dummy, n_splits=5)
```

**Gi·∫£i th√≠ch Time Series CV:**
-   **TimeSeriesSplit**: M·ªôt chi·∫øn l∆∞·ª£c chia d·ªØ li·ªáu theo tr√¨nh t·ª± th·ªùi gian, ƒë·∫£m b·∫£o r·∫±ng d·ªØ li·ªáu hu·∫•n luy·ªán lu√¥n x·∫£y ra *tr∆∞·ªõc* d·ªØ li·ªáu ki·ªÉm tra.
-   **Forward Chaining**: M·ªói fold s·ª≠ d·ª•ng d·ªØ li·ªáu qu√° kh·ª© ƒë·ªÉ hu·∫•n luy·ªán v√† d·ªØ li·ªáu t∆∞∆°ng lai ƒë·ªÉ ki·ªÉm tra.
-   **No Data Leakage**: NgƒÉn ch·∫∑n r√≤ r·ªâ th√¥ng tin t·ª´ t∆∞∆°ng lai v√†o qu√° kh·ª©, ƒëi·ªÅu c·ª±c k·ª≥ quan tr·ªçng trong d·ª± b√°o chu·ªói th·ªùi gian.

---

## üîç 4. Unsupervised Learning - H·ªçc kh√¥ng gi√°m s√°t

> **T∆∞ t∆∞·ªüng c·ªët l√µi**: Trong h·ªçc kh√¥ng gi√°m s√°t, m√¥ h√¨nh ƒë∆∞·ª£c cung c·∫•p **d·ªØ li·ªáu kh√¥ng c√≥ nh√£n (unlabeled data)** v√† nhi·ªám v·ª• c·ªßa n√≥ l√† t√¨m ki·∫øm c√°c **c·∫•u tr√∫c ti·ªÅm ·∫©n**, c√°c **m√¥ h√¨nh (patterns)** ho·∫∑c c√°c **m·ªëi quan h·ªá** b√™n trong d·ªØ li·ªáu ƒë√≥. Gi·ªëng nh∆∞ b·∫°n ƒëang nh√¨n v√†o m·ªôt ƒë√°m ƒë√¥ng v√† c·ªë g·∫Øng t√¨m ra c√°c nh√≥m ng∆∞·ªùi c√≥ v·∫ª gi·ªëng nhau m√† kh√¥ng ai n√≥i cho b·∫°n bi·∫øt h·ªç l√† ai hay thu·ªôc nh√≥m n√†o.

C√°c t√°c v·ª• ch√≠nh c·ªßa h·ªçc kh√¥ng gi√°m s√°t bao g·ªìm:
1.  **Clustering (Ph√¢n c·ª•m)**: Nh√≥m c√°c ƒëi·ªÉm d·ªØ li·ªáu t∆∞∆°ng t·ª± nhau th√†nh c√°c c·ª•m.
2.  **Dimensionality Reduction (Gi·∫£m chi·ªÅu d·ªØ li·ªáu)**: Bi·∫øn ƒë·ªïi d·ªØ li·ªáu t·ª´ kh√¥ng gian nhi·ªÅu chi·ªÅu sang kh√¥ng gian √≠t chi·ªÅu h∆°n m√† v·∫´n gi·ªØ ƒë∆∞·ª£c c√†ng nhi·ªÅu th√¥ng tin c√†ng t·ªët.
3.  **Anomaly Detection (Ph√°t hi·ªán b·∫•t th∆∞·ªùng)**: T√¨m ki·∫øm c√°c ƒëi·ªÉm d·ªØ li·ªáu kh√°c bi·ªát ƒë√°ng k·ªÉ so v·ªõi ph·∫ßn l·ªõn d·ªØ li·ªáu.

### 4.1 Ph√¢n c·ª•m (Clustering)

#### K-Means Clustering

-   **T∆∞ t∆∞·ªüng**: M·ªôt trong nh·ªØng thu·∫≠t to√°n ph√¢n c·ª•m ƒë∆°n gi·∫£n v√† ph·ªï bi·∫øn nh·∫•t. N√≥ c·ªë g·∫Øng chia d·ªØ li·ªáu th√†nh $K$ c·ª•m, trong ƒë√≥ m·ªói ƒëi·ªÉm d·ªØ li·ªáu thu·ªôc v·ªÅ c·ª•m c√≥ t√¢m (centroid) g·∫ßn n√≥ nh·∫•t.
-   **C√°ch ho·∫°t ƒë·ªông**:
    1.  **Kh·ªüi t·∫°o**: Ch·ªçn ng·∫´u nhi√™n $K$ ƒëi·ªÉm l√†m t√¢m c·ª•m ban ƒë·∫ßu.
    2.  **G√°n**: M·ªói ƒëi·ªÉm d·ªØ li·ªáu ƒë∆∞·ª£c g√°n v√†o c·ª•m c√≥ t√¢m g·∫ßn nh·∫•t.
    3.  **C·∫≠p nh·∫≠t**: T√¢m c·ª•m m·ªõi ƒë∆∞·ª£c t√≠nh to√°n b·∫±ng c√°ch l·∫•y trung b√¨nh t·∫•t c·∫£ c√°c ƒëi·ªÉm d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c g√°n v√†o c·ª•m ƒë√≥.
    4.  **L·∫∑p l·∫°i**: C√°c b∆∞·ªõc 2 v√† 3 ƒë∆∞·ª£c l·∫∑p l·∫°i cho ƒë·∫øn khi c√°c t√¢m c·ª•m kh√¥ng thay ƒë·ªïi ƒë√°ng k·ªÉ ho·∫∑c ƒë·∫°t ƒë·∫øn s·ªë l·∫ßn l·∫∑p t·ªëi ƒëa.
-   **∆Øu ƒëi·ªÉm**: ƒê∆°n gi·∫£n, nhanh, d·ªÖ hi·ªÉu v√† d·ªÖ th·ª±c hi·ªán.
-   **Nh∆∞·ª£c ƒëi·ªÉm**:
    -   C·∫ßn x√°c ƒë·ªãnh tr∆∞·ªõc s·ªë l∆∞·ª£ng c·ª•m $K$.
    -   Nh·∫°y c·∫£m v·ªõi vi·ªác kh·ªüi t·∫°o t√¢m c·ª•m ban ƒë·∫ßu.
    -   Ch·ªâ t√¨m ƒë∆∞·ª£c c√°c c·ª•m c√≥ h√¨nh d·∫°ng c·∫ßu (spherical clusters).
    -   Nh·∫°y c·∫£m v·ªõi outliers.
-   **C√°ch ch·ªçn $K$**:
    -   **Elbow Method**: V·∫Ω ƒë·ªì th·ªã t·ªïng b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch t·ª´ m·ªói ƒëi·ªÉm ƒë·∫øn t√¢m c·ª•m c·ªßa n√≥ (SSE) theo s·ªë l∆∞·ª£ng c·ª•m $K$. Ch·ªçn $K$ t·∫°i "ƒëi·ªÉm khu·ª∑u tay" n∆°i ƒë·ªô d·ªëc c·ªßa ƒë∆∞·ªùng cong gi·∫£m ƒë√°ng k·ªÉ.
    -   **Silhouette Score**: ƒêo l∆∞·ªùng m·ª©c ƒë·ªô t∆∞∆°ng t·ª± c·ªßa m·ªôt ƒë·ªëi t∆∞·ª£ng v·ªõi c·ª•m c·ªßa ch√≠nh n√≥ so v·ªõi c√°c c·ª•m kh√°c. ƒêi·ªÉm cao h∆°n (g·∫ßn 1) cho th·∫•y c·ª•m r√µ r√†ng.

```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
import numpy as np

# T·∫°o d·ªØ li·ªáu gi·∫£
X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)

# Hu·∫•n luy·ªán K-Means
kmeans = KMeans(n_clusters=4, random_state=0, n_init=10) # n_init ƒë·ªÉ ch·∫°y nhi·ªÅu l·∫ßn kh·ªüi t·∫°o
kmeans.fit(X)
y_kmeans = kmeans.predict(X)

# Tr·ª±c quan h√≥a k·∫øt qu·∫£
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.7, marker='X', label='T√¢m c·ª•m')
plt.title("K-Means Clustering")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.show()
```

#### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

-   **T∆∞ t∆∞·ªüng**: Thay v√¨ x√°c ƒë·ªãnh c·ª•m b·∫±ng t√¢m c·ª•m, DBSCAN ƒë·ªãnh nghƒ©a c·ª•m d·ª±a tr√™n **m·∫≠t ƒë·ªô** c√°c ƒëi·ªÉm d·ªØ li·ªáu. C√°c ƒëi·ªÉm ƒë∆∞·ª£c nh√≥m l·∫°i n·∫øu ch√∫ng ƒë·ªß g·∫ßn nhau v√† n·∫±m trong m·ªôt v√πng c√≥ m·∫≠t ƒë·ªô ƒë·ªß l·ªõn.
-   **C√°c kh√°i ni·ªám**:
    -   **Core Point (ƒêi·ªÉm l√µi)**: M·ªôt ƒëi·ªÉm c√≥ √≠t nh·∫•t `min_samples` (ng∆∞·ª°ng m·∫≠t ƒë·ªô) ƒëi·ªÉm kh√°c n·∫±m trong b√°n k√≠nh `eps` c·ªßa n√≥.
    -   **Border Point (ƒêi·ªÉm bi√™n)**: M·ªôt ƒëi·ªÉm n·∫±m trong b√°n k√≠nh `eps` c·ªßa m·ªôt ƒëi·ªÉm l√µi nh∆∞ng kh√¥ng ph·∫£i l√† ƒëi·ªÉm l√µi.
    -   **Noise Point (ƒêi·ªÉm nhi·ªÖu)**: M·ªôt ƒëi·ªÉm kh√¥ng ph·∫£i l√† ƒëi·ªÉm l√µi v√† c≈©ng kh√¥ng ph·∫£i l√† ƒëi·ªÉm bi√™n.
-   **C√°ch ho·∫°t ƒë·ªông**:
    1.  Ch·ªçn m·ªôt ƒëi·ªÉm ng·∫´u nhi√™n ch∆∞a ƒë∆∞·ª£c gh√© thƒÉm.
    2.  N·∫øu ƒëi·ªÉm n√†y l√† Core Point, b·∫Øt ƒë·∫ßu m·ªôt c·ª•m m·ªõi v√† t√¨m t·∫•t c·∫£ c√°c ƒëi·ªÉm c√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c t·ª´ n√≥.
    3.  N·∫øu kh√¥ng, ƒë√°nh d·∫•u n√≥ l√† Noise.
    4.  L·∫∑p l·∫°i cho ƒë·∫øn khi t·∫•t c·∫£ c√°c ƒëi·ªÉm ƒë∆∞·ª£c gh√© thƒÉm.
-   **∆Øu ƒëi·ªÉm**:
    -   C√≥ th·ªÉ t√¨m c√°c c·ª•m c√≥ h√¨nh d·∫°ng t√πy √Ω.
    -   Kh√¥ng y√™u c·∫ßu x√°c ƒë·ªãnh tr∆∞·ªõc s·ªë l∆∞·ª£ng c·ª•m $K$.
    -   C√≥ th·ªÉ ph√°t hi·ªán v√† x·ª≠ l√Ω t·ªët c√°c ƒëi·ªÉm nhi·ªÖu (outliers).
-   **Nh∆∞·ª£c ƒëi·ªÉm**:
    -   Kh√≥ x√°c ƒë·ªãnh c√°c tham s·ªë `eps` v√† `min_samples`, ƒë·∫∑c bi·ªát v·ªõi d·ªØ li·ªáu c√≥ m·∫≠t ƒë·ªô kh√°c nhau.
    -   G·∫∑p kh√≥ khƒÉn v·ªõi c√°c c·ª•m c√≥ m·∫≠t ƒë·ªô thay ƒë·ªïi ho·∫∑c khi c√°c c·ª•m c√≥ m·∫≠t ƒë·ªô t∆∞∆°ng t·ª± nhau.

```python
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt
import numpy as np

# T·∫°o d·ªØ li·ªáu gi·∫£ v·ªõi h√¨nh d·∫°ng ph·ª©c t·∫°p
X, y_true = make_moons(n_samples=200, noise=0.05, random_state=0)

# Hu·∫•n luy·ªán DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=5) # eps: b√°n k√≠nh, min_samples: ng∆∞·ª°ng m·∫≠t ƒë·ªô
dbscan.fit(X)
y_dbscan = dbscan.labels_ # -1 cho c√°c ƒëi·ªÉm nhi·ªÖu

# Tr·ª±c quan h√≥a k·∫øt qu·∫£
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, s=50, cmap='viridis')
plt.title("DBSCAN Clustering")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```

### 4.2 Gi·∫£m chi·ªÅu d·ªØ li·ªáu (Dimensionality Reduction)

-   **T·∫°i sao c·∫ßn?** Khi c√≥ qu√° nhi·ªÅu feature (s·ªë chi·ªÅu cao), d·ªØ li·ªáu tr·ªü n√™n r·∫•t th∆∞a th·ªõt, kh√≥ tr·ª±c quan h√≥a v√† hu·∫•n luy·ªán m√¥ h√¨nh (g·ªçi l√† **"l·ªùi nguy·ªÅn c·ªßa s·ªë chi·ªÅu" - Curse of Dimensionality**). Gi·∫£m chi·ªÅu gi√∫p gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y.

#### Ph√¢n t√≠ch th√†nh ph·∫ßn ch√≠nh (Principal Component Analysis - PCA)

-   **T∆∞ t∆∞·ªüng**: PCA t√¨m c√°c h∆∞·ªõng (Principal Components) trong d·ªØ li·ªáu m√† ·ªü ƒë√≥ ph∆∞∆°ng sai (variance) l√† l·ªõn nh·∫•t. N√≥ chi·∫øu d·ªØ li·ªáu l√™n c√°c h∆∞·ªõng n√†y, t·∫°o ra m·ªôt bi·ªÉu di·ªÖn m·ªõi c√≥ s·ªë chi·ªÅu th·∫•p h∆°n m√† v·∫´n gi·ªØ ƒë∆∞·ª£c c√†ng nhi·ªÅu th√¥ng tin c√†ng t·ªët.
-   **K·∫øt n·ªëi l√Ω thuy·∫øt**: PCA d·ª±a tr√™n c√°c kh√°i ni·ªám t·ª´ **ƒê·∫°i s·ªë tuy·∫øn t√≠nh** (ƒë·∫∑c bi·ªát l√† **Eigenvalues** v√† **Eigenvectors** - xem docs/01-foundations.md).
    -   **Eigenvector**: Ch√≠nh l√† c√°c h∆∞·ªõng (Principal Components) m√† PCA t√¨m th·∫•y.
    -   **Eigenvalue**: Cho bi·∫øt l∆∞·ª£ng ph∆∞∆°ng sai (th√¥ng tin) m√† m·ªói Principal Component n·∫Øm gi·ªØ.
-   **C√°c b∆∞·ªõc ch√≠nh**:
    1.  **Center the data**: Tr·ª´ ƒëi gi√° tr·ªã trung b√¨nh t·ª´ m·ªói feature.
    2.  **T√≠nh ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai (Covariance Matrix)**: M√¥ t·∫£ m·ªëi quan h·ªá gi·ªØa c√°c c·∫∑p feature.
    3.  **T√≠nh Eigenvalues v√† Eigenvectors** c·ªßa ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai.
    4.  **S·∫Øp x·∫øp**: S·∫Øp x·∫øp c√°c Eigenvector theo th·ª© t·ª± gi·∫£m d·∫ßn c·ªßa Eigenvalue (Eigenvector c√≥ Eigenvalue l·ªõn nh·∫•t l√† Principal Component 1, v.v.).
    5.  **Ch·ªçn s·ªë chi·ªÅu**: Ch·ªçn $k$ Eigenvector h√†ng ƒë·∫ßu (t∆∞∆°ng ·ª©ng v·ªõi $k$ Eigenvalue l·ªõn nh·∫•t) ƒë·ªÉ t·∫°o th√†nh m·ªôt ma tr·∫≠n chi·∫øu (projection matrix).
    6.  **Chi·∫øu d·ªØ li·ªáu**: Nh√¢n d·ªØ li·ªáu g·ªëc v·ªõi ma tr·∫≠n chi·∫øu ƒë·ªÉ c√≥ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d·ªØ li·ªáu trong kh√¥ng gian $k$ chi·ªÅu m·ªõi.
-   **·ª®ng d·ª•ng**:
    -   **Tr·ª±c quan h√≥a**: Gi·∫£m d·ªØ li·ªáu v·ªÅ 2 ho·∫∑c 3 chi·ªÅu ƒë·ªÉ c√≥ th·ªÉ v·∫Ω bi·ªÉu ƒë·ªì.
    -   **Kh·ª≠ nhi·ªÖu (Noise Reduction)**: C√°c Principal Component c√≥ Eigenvalue nh·ªè th∆∞·ªùng ch·ª©a nhi·ªÖu.
    -   **TƒÉng t·ªëc m√¥ h√¨nh**: Gi·∫£m s·ªë chi·ªÅu input cho c√°c m√¥ h√¨nh ML.

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
import numpy as np

# T·∫£i b·ªô d·ªØ li·ªáu Iris
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# Hu·∫•n luy·ªán PCA ƒë·ªÉ gi·∫£m v·ªÅ 2 chi·ªÅu
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print(f"D·ªØ li·ªáu g·ªëc c√≥ {X.shape[1]} chi·ªÅu.")
print(f"D·ªØ li·ªáu sau PCA c√≥ {X_pca.shape[1]} chi·ªÅu.")
print(f"T·ª∑ l·ªá ph∆∞∆°ng sai ƒë∆∞·ª£c gi·∫£i th√≠ch b·ªüi m·ªói th√†nh ph·∫ßn ch√≠nh: {pca.explained_variance_ratio_}")
print(f"T·ªïng t·ª∑ l·ªá ph∆∞∆°ng sai ƒë∆∞·ª£c gi·∫£i th√≠ch b·ªüi 2 th√†nh ph·∫ßn ch√≠nh: {pca.explained_variance_ratio_.sum():.2f}")

# Tr·ª±c quan h√≥a d·ªØ li·ªáu sau PCA
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=50, alpha=0.8)
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA c·ªßa b·ªô d·ªØ li·ªáu Iris")
plt.colorbar(label="Lo√†i hoa")
plt.show()

# Di·ªÖn gi·∫£i c√°c th√†nh ph·∫ßn ch√≠nh
print("\nC√°c th√†nh ph·∫ßn ch√≠nh (Principal Components) l√† s·ª± k·∫øt h·ª£p tuy·∫øn t√≠nh c·ªßa c√°c feature g·ªëc:")
for i, pc in enumerate(pca.components_):
    print(f"  PC{i+1}: " + " + ".join([f"{val:.2f} * {name}" for val, name in zip(pc, feature_names)]))
```

## üìö T√†i li·ªáu tham kh·∫£o

### Unsupervised Learning
- [K-Means Clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means) - Scikit-learn documentation
- [DBSCAN Clustering](https://scikit-learn.org/stable/modules/clustering.html#dbscan) - Scikit-learn documentation
- [PCA (Principal Component Analysis)](https://scikit-learn.org/stable/modules/decomposition.html#pca) - Scikit-learn documentation
- [An Introduction to Statistical Learning](https://www.statlearning.com/) - James, Witten, Hastie, Tibshirani

## üéØ B√†i t·∫≠p th·ª±c h√†nh

1.  **Ph√¢n c·ª•m kh√°ch h√†ng**: √Åp d·ª•ng K-Means v√† DBSCAN ƒë·ªÉ ph√¢n c·ª•m kh√°ch h√†ng d·ª±a tr√™n h√†nh vi mua s·∫Øm (v√≠ d·ª•: b·ªô d·ªØ li·ªáu mua h√†ng tr·ª±c tuy·∫øn c·ªßa UCI). So s√°nh k·∫øt qu·∫£ v√† ph√¢n t√≠ch ∆∞u nh∆∞·ª£c ƒëi·ªÉm c·ªßa t·ª´ng thu·∫≠t to√°n.
2.  **Gi·∫£m chi·ªÅu d·ªØ li·ªáu v√† Tr·ª±c quan h√≥a**: S·ª≠ d·ª•ng PCA ƒë·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu c·ªßa m·ªôt b·ªô d·ªØ li·ªáu c√≥ nhi·ªÅu feature (v√≠ d·ª•: b·ªô d·ªØ li·ªáu Wine) xu·ªëng 2 ho·∫∑c 3 chi·ªÅu ƒë·ªÉ tr·ª±c quan h√≥a. Ph√¢n t√≠ch xem c√°c th√†nh ph·∫ßn ch√≠nh ƒë·∫°i di·ªán cho th√¥ng tin g√¨ c·ªßa d·ªØ li·ªáu g·ªëc.
3.  **X√°c ƒë·ªãnh s·ªë c·ª•m t·ªëi ∆∞u**: √Åp d·ª•ng Elbow Method v√† Silhouette Score ƒë·ªÉ t√¨m s·ªë l∆∞·ª£ng c·ª•m t·ªëi ∆∞u cho m·ªôt b·ªô d·ªØ li·ªáu (v√≠ d·ª•: kh√°ch h√†ng, ho·∫∑c d·ªØ li·ªáu h√¨nh h·ªçc gi·∫£).

## üöÄ B∆∞·ªõc ti·∫øp theo

Sau khi ho√†n th√†nh Unsupervised Learning, b·∫°n s·∫Ω:
-   Hi·ªÉu r√µ c√°c ph∆∞∆°ng ph√°p ph√¢n c·ª•m v√† gi·∫£m chi·ªÅu d·ªØ li·ªáu.
-   C√≥ kh·∫£ nƒÉng t√¨m ki·∫øm c·∫•u tr√∫c ti·ªÅm ·∫©n trong d·ªØ li·ªáu kh√¥ng nh√£n.
-   S·∫µn s√†ng √°p d·ª•ng c√°c k·ªπ thu·∫≠t n√†y ƒë·ªÉ ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu v√† kh√°m ph√° insight.

## üìö T√†i li·ªáu tham kh·∫£o

### Feature Engineering
- [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/) - Alice Zheng
- [Python Feature Engineering Cookbook](https://www.packtpub.com/product/python-feature-engineering-cookbook/9781789806311) - Soledad Galli

### Machine Learning
- [Introduction to Machine Learning with Python](https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/) - Andreas M√ºller
- [Hands-On Machine Learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) - Aur√©lien G√©ron

### Model Evaluation
- [Model Evaluation Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) - Scikit-learn Documentation
- [Cross-Validation Strategies](https://scikit-learn.org/stable/modules/cross_validation.html) - Scikit-learn Documentation

## üéØ B√†i t·∫≠p th·ª±c h√†nh

1. **Feature Engineering**: T·∫°o temporal features cho dataset th·ªùi gian
2. **Model Comparison**: So s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh kh√°c nhau
3. **Cross-validation**: Implement time series CV cho d·ªØ li·ªáu th·ªùi gian
4. **Feature Selection**: √Åp d·ª•ng c√°c ph∆∞∆°ng ph√°p feature selection
5. **Model Deployment**: Tri·ªÉn khai m√¥ h√¨nh ML v√†o production

## üöÄ B∆∞·ªõc ti·∫øp theo

Sau khi ho√†n th√†nh Machine Learning, b·∫°n s·∫Ω:
- Hi·ªÉu s√¢u v·ªÅ feature engineering v√† preprocessing
- C√≥ th·ªÉ so s√°nh v√† ch·ªçn m√¥ h√¨nh ML ph√π h·ª£p
- Bi·∫øt c√°ch ƒë√°nh gi√° m√¥ h√¨nh m·ªôt c√°ch ch√≠nh x√°c
- S·∫µn s√†ng h·ªçc Deep Learning v√† MLOps

---

*Ch√∫c b·∫°n tr·ªü th√†nh ML Engineer xu·∫•t s·∫Øc! üéâ*

## üß© Ch∆∞∆°ng tr√¨nh 50/50 (L√Ω thuy·∫øt : Th·ª±c h√†nh)

- M·ª•c ti√™u: 50% l√Ω thuy·∫øt (thu·∫≠t to√°n, c√¥ng th·ª©c, ƒë·ªô ph·ª©c t·∫°p, bias-variance), 50% th·ª±c h√†nh (pipeline, hu·∫•n luy·ªán, ƒë√°nh gi√°, tri·ªÉn khai)

| M√¥-ƒëun | L√Ω thuy·∫øt (50%) | Th·ª±c h√†nh (50%) |
|---|---|---|
| Feature Engineering | T·∫°i sao/ khi n√†o, leakage, ch·ªçn ƒë·∫∑c tr∆∞ng | Pipeline FE + ƒë√°nh gi√° ·∫£nh h∆∞·ªüng |
| Supervised Learning | Loss/regularization, bias-variance | Train/validate, search hyper-params |
| Unsupervised | Kho·∫£ng c√°ch, m·ª•c ti√™u, ƒë√°nh gi√° | Clustering + DR, silhouette/UMAP |
| Evaluation | Cross-val, metrics, ROC/PR | So s√°nh m√¥ h√¨nh, error analysis |
| Deployment | Serialization, drift, monitoring | API FastAPI + checks/alerts |

Rubric (100ƒë/module): L√Ω thuy·∫øt 30 | Code 30 | K·∫øt qu·∫£ 30 | B√°o c√°o 10

---


# ü§ñ Data Science / Machine Learning - Khoa h·ªçc d·ªØ li·ªáu v√† h·ªçc m√°y

> **M·ª•c ti√™u**: Tr·ªü th√†nh chuy√™n gia Machine Learning, c√≥ kh·∫£ nƒÉng x√¢y d·ª±ng v√† tri·ªÉn khai c√°c m√¥ h√¨nh AI/ML trong th·ª±c t·∫ø

## üìã T·ªïng quan n·ªôi dung

```mermaid
graph TD
    A[üéØ Data Science & ML] --> B[üîß Feature Engineering]
    A --> C[üìä Supervised Learning]
    A --> D[üîç Unsupervised Learning]
    A --> E[‚öñÔ∏è Model Evaluation]
    A --> F[üöÄ Model Deployment]
    
    B --> B1[Temporal Features]
    B --> B2[Categorical Encoding]
    B --> B3[Feature Selection]
    B --> B4[Data Preprocessing]
    
    C --> C1[Linear Models]
    C --> C2[Tree-based Models]
    C --> C3[Neural Networks]
    C --> C4[Ensemble Methods]
    
    D --> D1[Clustering]
    D --> D2[Dimensionality Reduction]
    D --> D3[Association Rules]
    
    E --> E1[Cross-validation]
    E --> E2[Performance Metrics]
    E --> E3[Model Interpretability]
    
    F --> F1[Model Serialization]
    F --> F2[API Development]
    F --> F3[Monitoring & Maintenance]
```

![Data Science & ML Architecture](assets/ds-ml-architecture.svg)

![Data Science & ML Architecture PNG](assets/ds-ml-architecture.png)

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/ds-ml-architecture.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/ds-ml-architecture.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/ds-ml-architecture.png)**

## üîß 1. Feature Engineering v√† Preprocessing

### 1.1 Advanced Feature Engineering

> **Feature Engineering** l√† qu√° tr√¨nh t·∫°o ra c√°c ƒë·∫∑c tr∆∞ng m·ªõi t·ª´ d·ªØ li·ªáu g·ªëc ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh machine learning.

#### Temporal Features - ƒê·∫∑c tr∆∞ng th·ªùi gian

**L√Ω thuy·∫øt c∆° b·∫£n:**
- **Time Series Decomposition**: Trend + Seasonality + Residual
- **Cyclical Encoding**: Sinusoidal transformation ƒë·ªÉ preserve circular relationships
- **Fourier Transform**: Decompose time series th√†nh frequency components
- **Autocorrelation**: Measure temporal dependencies

**Mathematical Foundations:**

**1. Cyclical Encoding Theory:**
```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal
from scipy.stats import pearsonr

class CyclicalEncodingTheory:
    """Theoretical framework cho cyclical encoding"""
    
    @staticmethod
    def explain_cyclical_encoding():
        """Explain why cyclical encoding is necessary"""
        print("""
        **V·∫•n ƒë·ªÅ v·ªõi Linear Encoding:**
        - Th√°ng 1 = 1, Th√°ng 12 = 12
        - Kho·∫£ng c√°ch: |12-1| = 11 (r·∫•t xa)
        - Th·ª±c t·∫ø: Th√°ng 12 v√† 1 li·ªÅn k·ªÅ nhau
        
        **Gi·∫£i ph√°p: Cyclical Encoding:**
        - Th√°ng 1: (sin(2œÄ√ó1/12), cos(2œÄ√ó1/12)) = (0.5, 0.866)
        - Th√°ng 12: (sin(2œÄ√ó12/12), cos(2œÄ√ó12/12)) = (0, 1)
        - Kho·∫£ng c√°ch Euclidean: ‚àö[(0.5-0)¬≤ + (0.866-1)¬≤] = 0.5 (g·∫ßn nhau)
        """)
    
    @staticmethod
    def demonstrate_cyclical_properties():
        """Demonstrate mathematical properties c·ªßa cyclical encoding"""
        months = np.arange(1, 13)
        
        # Linear encoding
        linear_encoding = months
        
        # Cyclical encoding
        cyclical_sin = np.sin(2 * np.pi * months / 12)
        cyclical_cos = np.cos(2 * np.pi * months / 12)
        
        # Calculate distances
        def euclidean_distance(x1, y1, x2, y2):
            return np.sqrt((x1-x2)**2 + (y1-y2)**2)
        
        # Distance between consecutive months
        linear_distances = []
        cyclical_distances = []
        
        for i in range(len(months)-1):
            # Linear distance
            linear_dist = abs(linear_encoding[i+1] - linear_encoding[i])
            linear_distances.append(linear_dist)
            
            # Cyclical distance
            cyclical_dist = euclidean_distance(
                cyclical_sin[i], cyclical_cos[i],
                cyclical_sin[i+1], cyclical_cos[i+1]
            )
            cyclical_distances.append(cyclical_dist)
        
        # Special case: December to January
        linear_dist_dec_jan = abs(12 - 1)
        cyclical_dist_dec_jan = euclidean_distance(
            cyclical_sin[-1], cyclical_cos[-1],  # December
            cyclical_sin[0], cyclical_cos[0]     # January
        )
        
        print("**Distance Analysis:**")
        print(f"Linear encoding - consecutive months: {np.mean(linear_distances):.2f}")
        print(f"Cyclical encoding - consecutive months: {np.mean(cyclical_distances):.2f}")
        print(f"Linear encoding - Dec to Jan: {linear_dist_dec_jan}")
        print(f"Cyclical encoding - Dec to Jan: {cyclical_dist_dec_jan:.3f}")
        
        return {
            'months': months,
            'linear': linear_encoding,
            'cyclical_sin': cyclical_sin,
            'cyclical_cos': cyclical_cos,
            'linear_distances': linear_distances,
            'cyclical_distances': cyclical_distances
        }
    
    @staticmethod
    def fourier_analysis_example():
        """Demonstrate Fourier analysis cho time series"""
        # Generate synthetic time series with seasonality
        t = np.linspace(0, 100, 1000)
        signal_clean = (np.sin(2 * np.pi * t / 12) +  # Annual seasonality
                       np.sin(2 * np.pi * t / 4) +    # Quarterly seasonality
                       np.sin(2 * np.pi * t / 1))     # Monthly seasonality
        
        # Add noise
        signal_noisy = signal_clean + 0.1 * np.random.randn(len(t))
        
        # Perform FFT
        fft_result = np.fft.fft(signal_noisy)
        frequencies = np.fft.fftfreq(len(t), t[1] - t[0])
        
        # Find dominant frequencies
        power_spectrum = np.abs(fft_result)**2
        dominant_freq_idx = np.argsort(power_spectrum)[-5:]  # Top 5 frequencies
        
        print("**Fourier Analysis Results:**")
        print("Dominant frequencies (cycles per time unit):")
        for idx in reversed(dominant_freq_idx):
            if frequencies[idx] > 0:  # Only positive frequencies
                period = 1 / frequencies[idx]
                print(f"  Frequency: {frequencies[idx]:.3f}, Period: {period:.1f}")
        
        return {
            'time': t,
            'signal_clean': signal_clean,
            'signal_noisy': signal_noisy,
            'frequencies': frequencies,
            'power_spectrum': power_spectrum
        }
    
    @staticmethod
    def autocorrelation_analysis():
        """Demonstrate autocorrelation analysis"""
        # Generate time series with autocorrelation
        np.random.seed(42)
        n = 1000
        
        # AR(1) process: X_t = 0.8 * X_{t-1} + Œµ_t
        ar_process = np.zeros(n)
        for t in range(1, n):
            ar_process[t] = 0.8 * ar_process[t-1] + np.random.normal(0, 1)
        
        # Calculate autocorrelation
        def autocorr(x, max_lag=20):
            """Calculate autocorrelation function"""
            acf = []
            for lag in range(max_lag + 1):
                if lag == 0:
                    acf.append(1.0)
                else:
                    # Pearson correlation between X_t and X_{t-lag}
                    correlation = pearsonr(x[lag:], x[:-lag])[0]
                    acf.append(correlation)
            return acf
        
        acf_values = autocorr(ar_process)
        lags = range(len(acf_values))
        
        print("**Autocorrelation Analysis:**")
        print(f"AR(1) process with œÜ = 0.8")
        print(f"Expected ACF(1) ‚âà 0.8")
        print(f"Actual ACF(1) = {acf_values[1]:.3f}")
        
        return {
            'ar_process': ar_process,
            'lags': lags,
            'acf_values': acf_values
        }

# Demonstrate theoretical concepts
theory = CyclicalEncodingTheory()
theory.explain_cyclical_encoding()

# Run demonstrations
cyclical_props = theory.demonstrate_cyclical_properties()
fourier_results = theory.fourier_analysis_example()
autocorr_results = theory.autocorrelation_analysis()

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# 1. Cyclical encoding visualization
axes[0,0].scatter(cyclical_props['cyclical_sin'], cyclical_props['cyclical_cos'], 
                  c=cyclical_props['months'], cmap='viridis')
axes[0,0].set_title('Cyclical Encoding: Months in 2D Space')
axes[0,0].set_xlabel('sin(2œÄ√ómonth/12)')
axes[0,0].set_ylabel('cos(2œÄ√ómonth/12)')
for i, month in enumerate(cyclical_props['months']):
    axes[0,0].annotate(month, (cyclical_props['cyclical_sin'][i], cyclical_props['cyclical_cos'][i]))

# 2. Fourier analysis
axes[0,1].plot(fourier_results['time'], fourier_results['signal_noisy'], alpha=0.7, label='Noisy Signal')
axes[0,1].plot(fourier_results['time'], fourier_results['signal_clean'], 'r-', label='Clean Signal')
axes[0,1].set_title('Time Series with Multiple Seasonalities')
axes[0,1].set_xlabel('Time')
axes[0,1].set_ylabel('Value')
axes[0,1].legend()

# 3. Power spectrum
axes[1,0].plot(fourier_results['frequencies'][:len(fourier_results['frequencies'])//2], 
                fourier_results['power_spectrum'][:len(fourier_results['power_spectrum'])//2])
axes[1,0].set_title('Power Spectrum')
axes[1,0].set_xlabel('Frequency')
axes[1,0].set_ylabel('Power')

# 4. Autocorrelation function
axes[1,1].stem(autocorr_results['lags'], autocorr_results['acf_values'])
axes[1,1].set_title('Autocorrelation Function (AR(1) process)')
axes[1,1].set_xlabel('Lag')
axes[1,1].set_ylabel('ACF')

plt.tight_layout()
plt.show()
```

**2. Advanced Temporal Feature Engineering:**
```python
class AdvancedTemporalFeatures:
    """Advanced temporal feature engineering techniques"""
    
    def __init__(self):
        self.feature_history = []
    
    def create_lag_features(self, df: pd.DataFrame, value_column: str, 
                           date_column: str, lags: List[int]) -> pd.DataFrame:
        """Create lag features v·ªõi proper time alignment"""
        df = df.copy()
        df = df.sort_values(date_column).reset_index(drop=True)
        
        for lag in lags:
            df[f'{value_column}_lag_{lag}'] = df[value_column].shift(lag)
        
        return df
    
    def create_rolling_features(self, df: pd.DataFrame, value_column: str, 
                               windows: List[int], functions: List[str]) -> pd.DataFrame:
        """Create rolling window features"""
        df = df.copy()
        
        for window in windows:
            for func in functions:
                if func == 'mean':
                    df[f'{value_column}_rolling_mean_{window}'] = df[value_column].rolling(window).mean()
                elif func == 'std':
                    df[f'{value_column}_rolling_std_{window}'] = df[value_column].rolling(window).std()
                elif func == 'min':
                    df[f'{value_column}_rolling_min_{window}'] = df[value_column].rolling(window).min()
                elif func == 'max':
                    df[f'{value_column}_rolling_max_{window}'] = df[value_column].rolling(window).max()
                elif func == 'median':
                    df[f'{value_column}_rolling_median_{window}'] = df[value_column].rolling(window).median()
        
        return df
    
    def create_expanding_features(self, df: pd.DataFrame, value_column: str) -> pd.DataFrame:
        """Create expanding window features (cumulative)"""
        df = df.copy()
        
        df[f'{value_column}_expanding_mean'] = df[value_column].expanding().mean()
        df[f'{value_column}_expanding_std'] = df[value_column].expanding().std()
        df[f'{value_column}_expanding_min'] = df[value_column].expanding().min()
        df[f'{value_column}_expanding_max'] = df[value_column].expanding().max()
        
        return df
    
    def create_difference_features(self, df: pd.DataFrame, value_column: str, 
                                 differences: List[int]) -> pd.DataFrame:
        """Create difference features (stationarity)"""
        df = df.copy()
        
        for diff in differences:
            df[f'{value_column}_diff_{diff}'] = df[value_column].diff(diff)
        
        return df
    
    def create_seasonal_decomposition_features(self, df: pd.DataFrame, 
                                             value_column: str, 
                                             period: int) -> pd.DataFrame:
        """Create seasonal decomposition features"""
        from statsmodels.tsa.seasonal import seasonal_decompose
        
        df = df.copy()
        
        # Ensure data is sorted by time
        df = df.sort_values('date').reset_index(drop=True)
        
        # Perform seasonal decomposition
        decomposition = seasonal_decompose(df[value_column], period=period, extrapolate_trend='freq')
        
        # Add decomposition components
        df[f'{value_column}_trend'] = decomposition.trend
        df[f'{value_column}_seasonal'] = decomposition.seasonal
        df[f'{value_column}_residual'] = decomposition.resid
        
        return df
    
    def create_fourier_features(self, df: pd.DataFrame, date_column: str, 
                               periods: List[float]) -> pd.DataFrame:
        """Create Fourier features cho multiple seasonalities"""
        df = df.copy()
        df[date_column] = pd.to_datetime(df[date_column])
        
        # Convert to numeric for Fourier analysis
        time_numeric = (df[date_column] - df[date_column].min()).dt.total_seconds()
        
        for period in periods:
            # Create multiple frequency components
            for harmonic in range(1, 4):  # First 3 harmonics
                freq = 2 * np.pi * harmonic / period
                df[f'fourier_sin_{period}_{harmonic}'] = np.sin(freq * time_numeric)
                df[f'fourier_cos_{period}_{harmonic}'] = np.cos(freq * time_numeric)
        
        return df

# Example usage
temporal_features = AdvancedTemporalFeatures()

# Create sample time series data
dates = pd.date_range('2024-01-01', periods=1000, freq='D')
values = (np.sin(2 * np.pi * np.arange(1000) / 365) +  # Annual seasonality
          np.sin(2 * np.pi * np.arange(1000) / 30) +   # Monthly seasonality
          np.random.normal(0, 0.1, 1000))              # Noise

df = pd.DataFrame({
    'date': dates,
    'value': values
})

# Apply advanced temporal features
df_with_features = temporal_features.create_lag_features(df, 'value', 'date', [1, 7, 30])
df_with_features = temporal_features.create_rolling_features(df_with_features, 'value', [7, 30], ['mean', 'std'])
df_with_features = temporal_features.create_expanding_features(df_with_features, 'value')
df_with_features = temporal_features.create_difference_features(df_with_features, 'value', [1, 7])
df_with_features = temporal_features.create_seasonal_decomposition_features(df_with_features, 'value', 365)
df_with_features = temporal_features.create_fourier_features(df_with_features, 'date', [365, 30, 7])

print(f"Original columns: {len(df.columns)}")
print(f"After feature engineering: {len(df_with_features.columns)}")
print(f"New features created: {len(df_with_features.columns) - len(df.columns)}")
```

**3. Mathematical Validation v√† Feature Selection:**
```python
class TemporalFeatureValidation:
    """Validate temporal features using statistical methods"""
    
    @staticmethod
    def check_stationarity(series: pd.Series, significance_level: float = 0.05) -> Dict[str, Any]:
        """Check stationarity using Augmented Dickey-Fuller test"""
        from statsmodels.tsa.stattools import adfuller
        
        result = adfuller(series.dropna())
        
        is_stationary = result[1] < significance_level
        
        return {
            'is_stationary': is_stationary,
            'adf_statistic': result[0],
            'p_value': result[1],
            'critical_values': result[4],
            'significance_level': significance_level
        }
    
    @staticmethod
    def feature_importance_analysis(df: pd.DataFrame, target_column: str) -> pd.DataFrame:
        """Analyze feature importance using correlation v√† mutual information"""
        from sklearn.feature_selection import mutual_info_regression
        from sklearn.preprocessing import StandardScaler
        
        # Prepare data
        feature_cols = [col for col in df.columns if col != target_column and not col.startswith('date')]
        X = df[feature_cols].fillna(0)
        y = df[target_column].fillna(0)
        
        # Standardize features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Calculate correlations
        correlations = []
        for col in feature_cols:
            corr = df[col].corr(df[target_column])
            correlations.append(abs(corr))
        
        # Calculate mutual information
        mi_scores = mutual_info_regression(X_scaled, y, random_state=42)
        
        # Create feature importance dataframe
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'correlation_abs': correlations,
            'mutual_information': mi_scores,
            'importance_score': (np.array(correlations) + mi_scores) / 2
        }).sort_values('importance_score', ascending=False)
        
        return feature_importance
    
    @staticmethod
    def temporal_feature_correlation_analysis(df: pd.DataFrame, 
                                            feature_prefix: str) -> pd.DataFrame:
        """Analyze correlation between temporal features"""
        # Find all features with the given prefix
        temporal_features = [col for col in df.columns if col.startswith(feature_prefix)]
        
        if len(temporal_features) < 2:
            return pd.DataFrame()
        
        # Calculate correlation matrix
        correlation_matrix = df[temporal_features].corr()
        
        # Find highly correlated features
        high_corr_pairs = []
        for i in range(len(temporal_features)):
            for j in range(i+1, len(temporal_features)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.8:  # High correlation threshold
                    high_corr_pairs.append({
                        'feature1': temporal_features[i],
                        'feature2': temporal_features[j],
                        'correlation': corr_value
                    })
        
        return pd.DataFrame(high_corr_pairs).sort_values('correlation', key=abs, ascending=False)

# Validate temporal features
validation = TemporalFeatureValidation()

# Check stationarity of original series
stationarity_result = validation.check_stationarity(df['value'])
print(f"Original series stationary: {stationarity_result['is_stationary']}")
print(f"ADF p-value: {stationarity_result['p_value']:.6f}")

# Check stationarity of differenced series
diff_series = df['value'].diff(1).dropna()
diff_stationarity = validation.check_stationarity(diff_series)
print(f"Differenced series stationary: {diff_stationarity['is_stationary']}")
print(f"ADF p-value: {diff_stationarity['p_value']:.6f}")

# Analyze feature importance
feature_importance = validation.feature_importance_analysis(df_with_features, 'value')
print("\nTop 10 most important temporal features:")
print(feature_importance.head(10))

# Analyze temporal feature correlations
temporal_correlations = validation.temporal_feature_correlation_analysis(df_with_features, 'value_rolling')
if not temporal_correlations.empty:
    print("\nHighly correlated rolling features:")
    print(temporal_correlations)
```

**T√†i li·ªáu tham kh·∫£o chuy√™n s√¢u:**
- **Time Series Analysis**: [Box & Jenkins - Time Series Analysis](https://www.wiley.com/en-us/Time+Series+Analysis:+Forecasting+and+Control,+5th+Edition-p-9781118675021)
- **Fourier Analysis**: [Fourier Analysis and Its Applications](https://www.springer.com/gp/book/9780387946009)
- **Autocorrelation**: [Time Series Analysis: Theory and Methods](https://link.springer.com/book/10.1007/978-1-4419-0320-4)
- **Feature Engineering**: [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

def create_temporal_features(df, date_column):
    """
    T·∫°o ƒë·∫∑c tr∆∞ng th·ªùi gian to√†n di·ªán
    
    Parameters:
    df (pd.DataFrame): DataFrame c·∫ßn x·ª≠ l√Ω
    date_column (str): T√™n c·ªôt ch·ª©a ng√†y th√°ng
    
    Returns:
    pd.DataFrame: DataFrame v·ªõi c√°c ƒë·∫∑c tr∆∞ng th·ªùi gian m·ªõi
    """
    df = df.copy()
    df[date_column] = pd.to_datetime(df[date_column])
    
    # 1. Basic temporal features - ƒê·∫∑c tr∆∞ng th·ªùi gian c∆° b·∫£n
    df[f'{date_column}_year'] = df[date_column].dt.year          # NƒÉm
    df[f'{date_column}_month'] = df[date_column].dt.month        # Th√°ng (1-12)
    df[f'{date_column}_day'] = df[date_column].dt.day            # Ng√†y trong th√°ng
    df[f'{date_column}_dayofweek'] = df[date_column].dt.dayofweek # Ng√†y trong tu·∫ßn (0=Monday)
    df[f'{date_column}_quarter'] = df[date_column].dt.quarter     # Qu√Ω (1-4)
    df[f'{date_column}_is_month_end'] = df[date_column].dt.is_month_end.astype(int)    # Cu·ªëi th√°ng
    df[f'{date_column}_is_month_start'] = df[date_column].dt.is_month_start.astype(int) # ƒê·∫ßu th√°ng
    
    # 2. Cyclical encoding - M√£ h√≥a tu·∫ßn ho√†n cho ƒë·∫∑c tr∆∞ng ƒë·ªãnh k·ª≥
    # Gi√∫p m√¥ h√¨nh hi·ªÉu r·∫±ng th√°ng 12 v√† th√°ng 1 g·∫ßn nhau
    df[f'{date_column}_month_sin'] = np.sin(2 * np.pi * df[f'{date_column}_month'] / 12)
    df[f'{date_column}_month_cos'] = np.cos(2 * np.pi * df[f'{date_column}_month'] / 12)
    
    # Gi√∫p m√¥ h√¨nh hi·ªÉu r·∫±ng th·ª© 7 v√† ch·ªß nh·∫≠t g·∫ßn nhau
    df[f'{date_column}_dayofweek_sin'] = np.sin(2 * np.pi * df[f'{date_column}_dayofweek'] / 7)
    df[f'{date_column}_dayofweek_cos'] = np.cos(2 * np.pi * df[f'{date_column}_dayofweek'] / 7)
    
    # 3. Time since epoch - Th·ªùi gian t·ª´ epoch (1970-01-01)
    df[f'{date_column}_epoch'] = (df[date_column] - pd.Timestamp('1970-01-01')).dt.total_seconds()
    
    # 4. Business logic features - ƒê·∫∑c tr∆∞ng logic nghi·ªáp v·ª•
    df[f'{date_column}_is_weekend'] = df[f'{date_column}_dayofweek'].isin([5, 6]).astype(int)      # Cu·ªëi tu·∫ßn
    df[f'{date_column}_is_business_day'] = df[f'{date_column}_dayofweek'].isin([0, 1, 2, 3, 4]).astype(int) # Ng√†y l√†m vi·ªác
    
    return df

# V√≠ d·ª• s·ª≠ d·ª•ng
df = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=100, freq='D'),
    'value': np.random.randn(100)
})
df = create_temporal_features(df, 'date')

print("üìÖ Temporal Features Created:")
print(f"Original columns: {list(df.columns[:2])}")
print(f"New temporal features: {list(df.columns[2:])}")
```

**Gi·∫£i th√≠ch c√°c kh√°i ni·ªám:**
- **Cyclical Encoding**: Chuy·ªÉn ƒë·ªïi ƒë·∫∑c tr∆∞ng tu·∫ßn ho√†n th√†nh sin/cos ƒë·ªÉ m√¥ h√¨nh hi·ªÉu t√≠nh li√™n t·ª•c
- **Epoch Time**: S·ªë gi√¢y t·ª´ 1970-01-01, gi√∫p m√¥ h√¨nh hi·ªÉu kho·∫£ng c√°ch th·ªùi gian
- **Business Logic**: T·∫°o ƒë·∫∑c tr∆∞ng d·ª±a tr√™n ki·∫øn th·ª©c nghi·ªáp v·ª• (v√≠ d·ª•: ng√†y cu·ªëi tu·∫ßn)

#### Categorical Encoding - M√£ h√≥a d·ªØ li·ªáu ph√¢n lo·∫°i

**T·∫°i sao c·∫ßn categorical encoding?**
- M√¥ h√¨nh ML ch·ªâ x·ª≠ l√Ω ƒë∆∞·ª£c d·ªØ li·ªáu s·ªë
- C√°c ph∆∞∆°ng ph√°p encoding kh√°c nhau ph√π h·ª£p v·ªõi t·ª´ng lo·∫°i d·ªØ li·ªáu
- Target encoding gi√∫p capture th√¥ng tin v·ªÅ target variable

```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
import category_encoders as ce

class AdvancedCategoricalEncoder:
    """
    B·ªô m√£ h√≥a d·ªØ li·ªáu ph√¢n lo·∫°i n√¢ng cao
    
    H·ªó tr·ª£ nhi·ªÅu ph∆∞∆°ng ph√°p encoding kh√°c nhau:
    - Label Encoding: Cho d·ªØ li·ªáu c√≥ th·ª© t·ª±
    - One-Hot Encoding: Cho d·ªØ li·ªáu kh√¥ng c√≥ th·ª© t·ª±
    - Target Encoding: Cho d·ªØ li·ªáu c√≥ target variable
    - Count Encoding: Thay th·∫ø b·∫±ng t·∫ßn su·∫•t xu·∫•t hi·ªán
    - Hash Encoding: Cho d·ªØ li·ªáu c√≥ nhi·ªÅu categories
    """
    
    def __init__(self):
        self.label_encoders = {}      # L∆∞u tr·ªØ label encoders
        self.onehot_encoders = {}     # L∆∞u tr·ªØ one-hot encoders
        self.target_encoders = {}     # L∆∞u tr·ªØ target encoders
        self.count_encoders = {}      # L∆∞u tr·ªØ count encoders
        self.hash_encoders = {}       # L∆∞u tr·ªØ hash encoders
    
    def label_encode(self, df, categorical_columns):
        """
        Label encoding cho categories c√≥ th·ª© t·ª±
        
        Parameters:
        df (pd.DataFrame): DataFrame c·∫ßn encode
        categorical_columns (list): Danh s√°ch c·ªôt c·∫ßn encode
        
        Returns:
        pd.DataFrame: DataFrame ƒë√£ ƒë∆∞·ª£c encode
        """
        df_encoded = df.copy()
        
        for col in categorical_columns:
            if col not in self.label_encoders:
                # Fit encoder m·ªõi
                self.label_encoders[col] = LabelEncoder()
                df_encoded[col] = self.label_encoders[col].fit_transform(df[col])
            else:
                # Transform v·ªõi encoder ƒë√£ c√≥
                df_encoded[col] = self.label_encoders[col].transform(df[col])
        
        return df_encoded
    
    def onehot_encode(self, df, categorical_columns, sparse=False):
        """
        One-hot encoding cho categories kh√¥ng c√≥ th·ª© t·ª±
        
        Parameters:
        df (pd.DataFrame): DataFrame c·∫ßn encode
        categorical_columns (list): Danh s√°ch c·ªôt c·∫ßn encode
        sparse (bool): C√≥ s·ª≠ d·ª•ng sparse matrix kh√¥ng
        
        Returns:
        pd.DataFrame: DataFrame ƒë√£ ƒë∆∞·ª£c encode
        """
        df_encoded = df.copy()
        
        for col in categorical_columns:
            if col not in self.onehot_encoders:
                # Fit encoder m·ªõi
                self.onehot_encoders[col] = OneHotEncoder(sparse=sparse, drop='first')
                encoded = self.onehot_encoders[col].fit_transform(df[[col]])
                
                # T·∫°o t√™n c·ªôt m·ªõi
                if sparse:
                    feature_names = [f"{col}_{cat}" for cat in self.onehot_encoders[col].categories_[0][1:]]
                    encoded_df = pd.DataFrame(encoded.toarray(), columns=feature_names, index=df.index)
                else:
                    feature_names = [f"{col}_{cat}" for cat in self.onehot_encoders[col].categories_[0][1:]]
                    encoded_df = pd.DataFrame(encoded, columns=feature_names, index=df.index)
                
                # Th√™m c·ªôt m·ªõi v√† x√≥a c·ªôt c≈©
                df_encoded = pd.concat([df_encoded, encoded_df], axis=1)
                df_encoded.drop(col, axis=1, inplace=True)
        
        return df_encoded
    
    def target_encode(self, df, categorical_columns, target_column, cv_folds=5):
        """
        Target encoding v·ªõi cross-validation ƒë·ªÉ tr√°nh data leakage
        
        Parameters:
        df (pd.DataFrame): DataFrame c·∫ßn encode
        categorical_columns (list): Danh s√°ch c·ªôt c·∫ßn encode
        target_column (str): T√™n c·ªôt target
        cv_folds (int): S·ªë folds cho cross-validation
        
        Returns:
        pd.DataFrame: DataFrame ƒë√£ ƒë∆∞·ª£c encode
        """
        df_encoded = df.copy()
        
        for col in categorical_columns:
            if col not in self.target_encoders:
                # S·ª≠ d·ª•ng TargetEncoder v·ªõi cross-validation
                self.target_encoders[col] = ce.TargetEncoder(cols=[col], cv=cv_folds)
                df_encoded = self.target_encoders[col].fit_transform(df_encoded, df[target_column])
            else:
                # Transform v·ªõi encoder ƒë√£ c√≥
                df_encoded = self.target_encoders[col].transform(df_encoded)
        
        return df_encoded
    
    def count_encode(self, df, categorical_columns):
        """
        Count encoding - thay th·∫ø category b·∫±ng t·∫ßn su·∫•t xu·∫•t hi·ªán
        
        Parameters:
        df (pd.DataFrame): DataFrame c·∫ßn encode
        categorical_columns (list): Danh s√°ch c·ªôt c·∫ßn encode
        
        Returns:
        pd.DataFrame: DataFrame ƒë√£ ƒë∆∞·ª£c encode
        """
        df_encoded = df.copy()
        
        for col in categorical_columns:
            if col not in self.count_encoders:
                # T√≠nh t·∫ßn su·∫•t xu·∫•t hi·ªán
                value_counts = df[col].value_counts()
                self.count_encoders[col] = value_counts
                df_encoded[f'{col}_count'] = df[col].map(value_counts)
            else:
                # S·ª≠ d·ª•ng mapping ƒë√£ c√≥
                df_encoded[f'{col}_count'] = df[col].map(self.count_encoders[col])
        
        return df_encoded

# V√≠ d·ª• s·ª≠ d·ª•ng
encoder = AdvancedCategoricalEncoder()

# D·ªØ li·ªáu m·∫´u
sample_df = pd.DataFrame({
    'category': ['A', 'B', 'A', 'C', 'B'],
    'ordinal': ['Low', 'Medium', 'Low', 'High', 'Medium'],
    'target': [0, 1, 0, 1, 1]
})

print("üìä Original Data:")
print(sample_df)
print("\n" + "="*50)

# Label encoding cho ordinal data
df_labeled = encoder.label_encode(sample_df, ['ordinal'])
print("üè∑Ô∏è Label Encoded (Ordinal):")
print(df_labeled[['ordinal', 'target']])

# One-hot encoding cho nominal data
df_onehot = encoder.onehot_encode(sample_df, ['category'])
print("\nüî• One-Hot Encoded (Nominal):")
print(df_onehot)

# Target encoding
df_target = encoder.target_encode(sample_df, ['category'], 'target')
print("\nüéØ Target Encoded:")
print(df_target)

# Count encoding
df_count = encoder.count_encode(sample_df, ['category'])
print("\nüî¢ Count Encoded:")
print(df_count)
```

**Gi·∫£i th√≠ch c√°c ph∆∞∆°ng ph√°p encoding:**
- **Label Encoding**: G√°n s·ªë cho m·ªói category (0, 1, 2...), ph√π h·ª£p cho d·ªØ li·ªáu c√≥ th·ª© t·ª±
- **One-Hot Encoding**: T·∫°o c·ªôt ri√™ng cho m·ªói category (0/1), ph√π h·ª£p cho d·ªØ li·ªáu kh√¥ng c√≥ th·ª© t·ª±
- **Target Encoding**: Thay th·∫ø category b·∫±ng gi√° tr·ªã trung b√¨nh c·ªßa target, c√≥ th·ªÉ g√¢y data leakage
- **Count Encoding**: Thay th·∫ø category b·∫±ng t·∫ßn su·∫•t xu·∫•t hi·ªán, gi√∫p capture frequency information

### 1.2 Feature Selection - L·ª±a ch·ªçn ƒë·∫∑c tr∆∞ng

> **Feature Selection** l√† qu√° tr√¨nh ch·ªçn ra nh·ªØng ƒë·∫∑c tr∆∞ng quan tr·ªçng nh·∫•t ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh v√† gi·∫£m overfitting.

#### Statistical Methods - Ph∆∞∆°ng ph√°p th·ªëng k√™

```python
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
import numpy as np

def statistical_feature_selection(X, y, k=10, method='f_classif'):
    """
    L·ª±a ch·ªçn ƒë·∫∑c tr∆∞ng b·∫±ng ph∆∞∆°ng ph√°p th·ªëng k√™
    
    Parameters:
    X (pd.DataFrame): Features
    y (pd.Series): Target variable
    k (int): S·ªë l∆∞·ª£ng features c·∫ßn ch·ªçn
    method (str): Ph∆∞∆°ng ph√°p l·ª±a ch·ªçn ('f_classif', 'mutual_info_classif')
    
    Returns:
    tuple: (selected_features, feature_scores)
    """
    
    if method == 'f_classif':
        # F-test cho classification
        selector = SelectKBest(score_func=f_classif, k=k)
        selector.fit(X, y)
        
        # L·∫•y scores v√† p-values
        scores = selector.scores_
        p_values = selector.pvalues_
        
        # T·∫°o DataFrame v·ªõi scores
        feature_scores = pd.DataFrame({
            'Feature': X.columns,
            'F_Score': scores,
            'P_Value': p_values
        }).sort_values('F_Score', ascending=False)
        
    elif method == 'mutual_info_classif':
        # Mutual Information cho classification
        selector = SelectKBest(score_func=mutual_info_classif, k=k)
        selector.fit(X, y)
        
        # L·∫•y scores
        scores = selector.scores_
        
        # T·∫°o DataFrame v·ªõi scores
        feature_scores = pd.DataFrame({
            'Feature': X.columns,
            'MI_Score': scores
        }).sort_values('MI_Score', ascending=False)
    
    # L·∫•y features ƒë∆∞·ª£c ch·ªçn
    selected_features = feature_scores.head(k)['Feature'].tolist()
    
    return selected_features, feature_scores

def model_based_feature_selection(X, y, threshold='median'):
    """
    L·ª±a ch·ªçn ƒë·∫∑c tr∆∞ng d·ª±a tr√™n m√¥ h√¨nh (Random Forest)
    
    Parameters:
    X (pd.DataFrame): Features
    y (pd.Series): Target variable
    threshold (str/float): Ng∆∞·ª°ng ƒë·ªÉ l·ª±a ch·ªçn features
    
    Returns:
    tuple: (selected_features, feature_importance)
    """
    
    # S·ª≠ d·ª•ng Random Forest ƒë·ªÉ ƒë√°nh gi√° feature importance
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X, y)
    
    # L·∫•y feature importance
    feature_importance = pd.DataFrame({
        'Feature': X.columns,
        'Importance': rf.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    # L·ª±a ch·ªçn features d·ª±a tr√™n threshold
    if threshold == 'median':
        threshold_value = feature_importance['Importance'].median()
    elif threshold == 'mean':
        threshold_value = feature_importance['Importance'].mean()
    else:
        threshold_value = threshold
    
    selected_features = feature_importance[feature_importance['Importance'] > threshold_value]['Feature'].tolist()
    
    return selected_features, feature_importance

# V√≠ d·ª• s·ª≠ d·ª•ng
# Gi·∫£ s·ª≠ ch√∫ng ta c√≥ d·ªØ li·ªáu X v√† y
# selected_features, scores = statistical_feature_selection(X, y, k=5, method='f_classif')
# model_features, importance = model_based_feature_selection(X, y, threshold='median')
```

**Gi·∫£i th√≠ch c√°c ph∆∞∆°ng ph√°p feature selection:**
- **F-test**: ƒêo l∆∞·ªùng s·ª± kh√°c bi·ªát gi·ªØa c√°c nh√≥m, p-value th·∫•p = feature quan tr·ªçng
- **Mutual Information**: ƒêo l∆∞·ªùng m·ª©c ƒë·ªô ph·ª• thu·ªôc gi·ªØa feature v√† target
- **Model-based**: S·ª≠ d·ª•ng m√¥ h√¨nh ML ƒë·ªÉ ƒë√°nh gi√° t·∫ßm quan tr·ªçng c·ªßa features

## üìä 2. Supervised Learning - H·ªçc c√≥ gi√°m s√°t

### 2.1 Linear Models - M√¥ h√¨nh tuy·∫øn t√≠nh

> **Linear Models** l√† c√°c m√¥ h√¨nh c∆° b·∫£n gi·∫£ ƒë·ªãnh m·ªëi quan h·ªá tuy·∫øn t√≠nh gi·ªØa features v√† target.

#### Linear Regression v·ªõi Regularization

```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
import matplotlib.pyplot as plt

def compare_linear_models(X, y, test_size=0.2, random_state=42):
    """
    So s√°nh c√°c m√¥ h√¨nh tuy·∫øn t√≠nh kh√°c nhau
    
    Parameters:
    X (pd.DataFrame): Features
    y (pd.Series): Target variable
    test_size (float): T·ª∑ l·ªá d·ªØ li·ªáu test
    random_state (int): Random seed
    
    Returns:
    dict: K·∫øt qu·∫£ so s√°nh c√°c m√¥ h√¨nh
    """
    
    # Chia d·ªØ li·ªáu train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ƒê·ªãnh nghƒ©a c√°c m√¥ h√¨nh
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge (L2)': Ridge(alpha=1.0),
        'Lasso (L1)': Lasso(alpha=1.0),
        'Elastic Net': ElasticNet(alpha=1.0, l1_ratio=0.5)
    }
    
    results = {}
    
    for name, model in models.items():
        # Train m√¥ h√¨nh
        model.fit(X_train_scaled, y_train)
        
        # D·ª± ƒëo√°n
        y_pred_train = model.predict(X_train_scaled)
        y_pred_test = model.predict(X_test_scaled)
        
        # T√≠nh metrics
        from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
        
        train_r2 = r2_score(y_train, y_pred_train)
        test_r2 = r2_score(y_test, y_pred_test)
        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
        train_mae = mean_absolute_error(y_train, y_pred_train)
        test_mae = mean_absolute_error(y_test, y_pred_test)
        
        # Cross-validation score
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')
        
        results[name] = {
            'Train R¬≤': train_r2,
            'Test R¬≤': test_r2,
            'Train RMSE': train_rmse,
            'Test RMSE': test_rmse,
            'Train MAE': train_mae,
            'Test MAE': test_mae,
            'CV R¬≤ Mean': cv_scores.mean(),
            'CV R¬≤ Std': cv_scores.std()
        }
    
    # T·∫°o b·∫£ng so s√°nh
    comparison_df = pd.DataFrame(results).T
    comparison_df = comparison_df.round(4)
    
    print("üìä LINEAR MODELS COMPARISON")
    print("=" * 60)
    print(comparison_df)
    
    # So s√°nh coefficients
    print("\nüîç COEFFICIENT COMPARISON")
    print("=" * 40)
    
    for name, model in models.items():
        if hasattr(model, 'coef_'):
            coef_df = pd.DataFrame({
                'Feature': X.columns,
                'Coefficient': model.coef_
            }).sort_values('Coefficient', key=abs, ascending=False)
            
            print(f"\n{name}:")
            print(coef_df.head())
    
    return results, models

# V√≠ d·ª• s·ª≠ d·ª•ng
# results, models = compare_linear_models(X, y)
```

**Gi·∫£i th√≠ch c√°c lo·∫°i regularization:**
- **Ridge (L2)**: Th√™m penalty cho t·ªïng b√¨nh ph∆∞∆°ng coefficients, gi√∫p gi·∫£m overfitting
- **Lasso (L1)**: Th√™m penalty cho t·ªïng tuy·ªát ƒë·ªëi coefficients, c√≥ th·ªÉ zero out m·ªôt s·ªë coefficients
- **Elastic Net**: K·∫øt h·ª£p c·∫£ L1 v√† L2 regularization

### 2.2 Tree-based Models - M√¥ h√¨nh d·ª±a tr√™n c√¢y

> **Tree-based Models** l√† c√°c m√¥ h√¨nh s·ª≠ d·ª•ng c√¢y quy·∫øt ƒë·ªãnh ƒë·ªÉ ph√¢n lo·∫°i ho·∫∑c h·ªìi quy.

#### Random Forest v√† Gradient Boosting

```python
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

def analyze_tree_models(X, y, test_size=0.2, random_state=42):
    """
    Ph√¢n t√≠ch v√† so s√°nh c√°c m√¥ h√¨nh d·ª±a tr√™n c√¢y
    
    Parameters:
    X (pd.DataFrame): Features
    y (pd.Series): Target variable
    test_size (float): T·ª∑ l·ªá d·ªØ li·ªáu test
    random_state (int): Random seed
    
    Returns:
    dict: K·∫øt qu·∫£ ph√¢n t√≠ch
    """
    
    # Chia d·ªØ li·ªáu
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    # ƒê·ªãnh nghƒ©a c√°c m√¥ h√¨nh
    models = {
        'Random Forest': RandomForestRegressor(
            n_estimators=100, 
            max_depth=10, 
            random_state=random_state
        ),
        'Gradient Boosting': GradientBoostingRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=random_state
        )
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\nüå≥ Training {name}...")
        
        # Train m√¥ h√¨nh
        model.fit(X_train, y_train)
        
        # D·ª± ƒëo√°n
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # T√≠nh metrics
        train_r2 = r2_score(y_train, y_pred_train)
        test_r2 = r2_score(y_test, y_pred_test)
        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
        
        # Feature importance
        if hasattr(model, 'feature_importances_'):
            importance_df = pd.DataFrame({
                'Feature': X.columns,
                'Importance': model.feature_importances_
            }).sort_values('Importance', ascending=False)
        
        results[name] = {
            'Train R¬≤': train_r2,
            'Test R¬≤': test_r2,
            'Train RMSE': train_rmse,
            'Test RMSE': test_rmse,
            'Feature Importance': importance_df
        }
        
        print(f"‚úÖ {name} completed!")
    
    # In k·∫øt qu·∫£
    print("\nüìä TREE MODELS COMPARISON")
    print("=" * 50)
    
    comparison_df = pd.DataFrame({
        'Model': list(results.keys()),
        'Train R¬≤': [results[name]['Train R¬≤'] for name in results.keys()],
        'Test R¬≤': [results[name]['Test R¬≤'] for name in results.keys()],
        'Train RMSE': [results[name]['Train RMSE'] for name in results.keys()],
        'Test RMSE': [results[name]['Test RMSE'] for name in results.keys()]
    }).round(4)
    
    print(comparison_df)
    
    # Feature importance comparison
    print("\nüîç TOP 10 FEATURES BY IMPORTANCE")
    print("=" * 40)
    
    for name, result in results.items():
        print(f"\n{name}:")
        print(result['Feature Importance'].head(10))
    
    return results, models

# V√≠ d·ª• s·ª≠ d·ª•ng
# tree_results, tree_models = analyze_tree_models(X, y)
```

**Gi·∫£i th√≠ch c√°c m√¥ h√¨nh d·ª±a tr√™n c√¢y:**
- **Random Forest**: Ensemble c·ªßa nhi·ªÅu c√¢y quy·∫øt ƒë·ªãnh, m·ªói c√¢y train tr√™n subset kh√°c nhau
- **Gradient Boosting**: Sequential training, m·ªói c√¢y m·ªõi s·ª≠a l·ªói c·ªßa c√°c c√¢y tr∆∞·ªõc
- **Feature Importance**: ƒêo l∆∞·ªùng m·ª©c ƒë·ªô quan tr·ªçng c·ªßa m·ªói feature trong vi·ªác d·ª± ƒëo√°n

## ‚öñÔ∏è 3. Model Evaluation - ƒê√°nh gi√° m√¥ h√¨nh

### 3.1 Cross-validation Strategies - Chi·∫øn l∆∞·ª£c cross-validation

> **Cross-validation** l√† k·ªπ thu·∫≠t ƒë√°nh gi√° m√¥ h√¨nh b·∫±ng c√°ch chia d·ªØ li·ªáu th√†nh nhi·ªÅu folds v√† train/test nhi·ªÅu l·∫ßn.

#### Time Series Cross-validation

```python
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error
import numpy as np

def time_series_cv_evaluation(model, X, y, n_splits=5):
    """
    ƒê√°nh gi√° m√¥ h√¨nh v·ªõi time series cross-validation
    
    Parameters:
    model: M√¥ h√¨nh c·∫ßn ƒë√°nh gi√°
    X (pd.DataFrame): Features
    y (pd.Series): Target variable
    n_splits (int): S·ªë l∆∞·ª£ng splits
    
    Returns:
    dict: K·∫øt qu·∫£ ƒë√°nh gi√°
    """
    
    # Time series split
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    cv_scores = []
    train_sizes = []
    
    print("‚è∞ TIME SERIES CROSS-VALIDATION")
    print("=" * 50)
    
    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
        
        # Train m√¥ h√¨nh
        model.fit(X_train, y_train)
        
        # D·ª± ƒëo√°n
        y_pred = model.predict(X_test)
        
        # T√≠nh metrics
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        
        cv_scores.append({
            'Fold': fold,
            'Train Size': len(train_idx),
            'Test Size': len(test_idx),
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae
        })
        
        train_sizes.append(len(train_idx))
        
        print(f"Fold {fold}: Train={len(train_idx)}, Test={len(test_idx)}, RMSE={rmse:.4f}")
    
    # T√≥m t·∫Øt k·∫øt qu·∫£
    cv_df = pd.DataFrame(cv_scores)
    
    print(f"\nüìä CROSS-VALIDATION SUMMARY")
    print("=" * 40)
    print(f"Mean RMSE: {cv_df['RMSE'].mean():.4f} ¬± {cv_df['RMSE'].std():.4f}")
    print(f"Mean MAE: {cv_df['MAE'].mean():.4f} ¬± {cv_df['MAE'].std():.4f}")
    print(f"Min RMSE: {cv_df['RMSE'].min():.4f}")
    print(f"Max RMSE: {cv_df['RMSE'].max():.4f}")
    
    return cv_df

# V√≠ d·ª• s·ª≠ d·ª•ng
# cv_results = time_series_cv_evaluation(model, X, y, n_splits=5)
```

**Gi·∫£i th√≠ch Time Series CV:**
- **TimeSeriesSplit**: Chia d·ªØ li·ªáu theo th·ª© t·ª± th·ªùi gian, kh√¥ng random
- **Forward Chaining**: M·ªói fold s·ª≠ d·ª•ng d·ªØ li·ªáu qu√° kh·ª© ƒë·ªÉ train, t∆∞∆°ng lai ƒë·ªÉ test
- **No Data Leakage**: ƒê·∫£m b·∫£o kh√¥ng c√≥ th√¥ng tin t·ª´ t∆∞∆°ng lai trong training

## üìö T√†i li·ªáu tham kh·∫£o

### Feature Engineering
- [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/) - Alice Zheng
- [Python Feature Engineering Cookbook](https://www.packtpub.com/product/python-feature-engineering-cookbook/9781789806311) - Soledad Galli

### Machine Learning
- [Introduction to Machine Learning with Python](https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/) - Andreas M√ºller
- [Hands-On Machine Learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) - Aur√©lien G√©ron

### Model Evaluation
- [Model Evaluation Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) - Scikit-learn Documentation
- [Cross-Validation Strategies](https://scikit-learn.org/stable/modules/cross_validation.html) - Scikit-learn Documentation

## üéØ B√†i t·∫≠p th·ª±c h√†nh

1. **Feature Engineering**: T·∫°o temporal features cho dataset th·ªùi gian
2. **Model Comparison**: So s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh kh√°c nhau
3. **Cross-validation**: Implement time series CV cho d·ªØ li·ªáu th·ªùi gian
4. **Feature Selection**: √Åp d·ª•ng c√°c ph∆∞∆°ng ph√°p feature selection
5. **Model Deployment**: Tri·ªÉn khai m√¥ h√¨nh ML v√†o production

## üöÄ B∆∞·ªõc ti·∫øp theo

Sau khi ho√†n th√†nh Machine Learning, b·∫°n s·∫Ω:
- Hi·ªÉu s√¢u v·ªÅ feature engineering v√† preprocessing
- C√≥ th·ªÉ so s√°nh v√† ch·ªçn m√¥ h√¨nh ML ph√π h·ª£p
- Bi·∫øt c√°ch ƒë√°nh gi√° m√¥ h√¨nh m·ªôt c√°ch ch√≠nh x√°c
- S·∫µn s√†ng h·ªçc Deep Learning v√† MLOps

---

*Ch√∫c b·∫°n tr·ªü th√†nh ML Engineer xu·∫•t s·∫Øc! üéâ*

## üß© Ch∆∞∆°ng tr√¨nh 50/50 (L√Ω thuy·∫øt : Th·ª±c h√†nh)

- M·ª•c ti√™u: 50% l√Ω thuy·∫øt (thu·∫≠t to√°n, c√¥ng th·ª©c, ƒë·ªô ph·ª©c t·∫°p, bias-variance), 50% th·ª±c h√†nh (pipeline, hu·∫•n luy·ªán, ƒë√°nh gi√°, tri·ªÉn khai)

| M√¥-ƒëun | L√Ω thuy·∫øt (50%) | Th·ª±c h√†nh (50%) |
|---|---|---|
| Feature Engineering | T·∫°i sao/ khi n√†o, leakage, ch·ªçn ƒë·∫∑c tr∆∞ng | Pipeline FE + ƒë√°nh gi√° ·∫£nh h∆∞·ªüng |
| Supervised Learning | Loss/regularization, bias-variance | Train/validate, search hyper-params |
| Unsupervised | Kho·∫£ng c√°ch, m·ª•c ti√™u, ƒë√°nh gi√° | Clustering + DR, silhouette/UMAP |
| Evaluation | Cross-val, metrics, ROC/PR | So s√°nh m√¥ h√¨nh, error analysis |
| Deployment | Serialization, drift, monitoring | API FastAPI + checks/alerts |

Rubric (100ƒë/module): L√Ω thuy·∫øt 30 | Code 30 | K·∫øt qu·∫£ 30 | B√°o c√°o 10

---


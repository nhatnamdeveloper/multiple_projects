# ğŸ•¸ï¸ Graph Neural Networks (GNNs) - Máº¡ng nÆ¡-ron trÃªn Ä‘á»“ thá»‹

> **Má»¥c tiÃªu**: Hiá»ƒu cÃ¡c khÃ¡i niá»‡m cÆ¡ báº£n vÃ  kiáº¿n trÃºc cá»‘t lÃµi cá»§a Máº¡ng nÆ¡-ron Ä‘á»“ thá»‹ (GNNs), má»™t lÄ©nh vá»±c Ä‘ang phÃ¡t triá»ƒn nhanh chÃ³ng cho viá»‡c há»c trÃªn dá»¯ liá»‡u cÃ³ cáº¥u trÃºc quan há»‡.

## ğŸ“‹ Tá»•ng quan ná»™i dung

```mermaid
graph TD
    A[ğŸ•¸ï¸ Graph Neural Networks] --> B[ğŸ§  Ná»n táº£ng Ä‘á»“ thá»‹]
    A --> C[ğŸ”§ CÆ¡ cháº¿ Message Passing]
    A --> D[ğŸ›ï¸ CÃ¡c kiáº¿n trÃºc GNN]
    A --> E[ğŸ¯ CÃ¡c tÃ¡c vá»¥ trÃªn Ä‘á»“ thá»‹]
    
    B --> B1[Nodes, Edges, Adjacency Matrix]
    B --> B2[Node Features, Edge Features]
    B --> B3[Graph Isomorphism Problem]
    
    C --> C1[Aggregate Function]
    C --> C2[Update Function]
    C --> C3[Permutation Invariance/Equivariance]
    
    D --> D1[Graph Convolutional Networks (GCN)]
    D --> D2[GraphSAGE]
    D --> D3[Graph Attention Networks (GAT)]
    
    E --> E1[Node Classification]
    E --> E2[Link Prediction]
    E --> E3[Graph Classification]
```

## ğŸ“š 1. Báº£ng kÃ½ hiá»‡u (Notation)

- **Graph (\$\mathcal{G}\$)**: Má»™t Ä‘á»“ thá»‹ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi táº­p há»£p cÃ¡c Ä‘á»‰nh vÃ  cáº¡nh, \$\mathcal{G} = (\mathcal{V}, \mathcal{E})
- **Node/Vertex ($v \in \mathcal{V}$)**: Má»™t Ä‘á»‰nh trong Ä‘á»“ thá»‹.
- **Edge ($e_{ij} \in \mathcal{E}$)**: Má»™t cáº¡nh ná»‘i giá»¯a Ä‘á»‰nh $i$ vÃ  Ä‘á»‰nh $j$.
- **Adjacency Matrix (\$\mathbf{A}\$)**: Ma tráº­n ká», \$\mathbf{A}_{ij} = 1\$ náº¿u cÃ³ cáº¡nh ná»‘i giá»¯a $i$ vÃ  $j$, ngÆ°á»£c láº¡i báº±ng 0.
- **Node Feature Matrix (\$\mathbf{X}\$)**: Ma tráº­n Ä‘áº·c trÆ°ng cá»§a cÃ¡c Ä‘á»‰nh, má»—i hÃ ng lÃ  má»™t vector Ä‘áº·c trÆ°ng cá»§a má»™t Ä‘á»‰nh.
- **Node Embedding/Hidden State (\$\mathbf{h}_v^{(k)}\$)**: Biá»ƒu diá»…n vector cá»§a Ä‘á»‰nh $v$ táº¡i layer thá»© $k$.

## ğŸ“– 2. Glossary (Äá»‹nh nghÄ©a cá»‘t lÃµi)

-   **Graph**: Má»™t cáº¥u trÃºc dá»¯ liá»‡u bao gá»“m cÃ¡c **Ä‘á»‰nh (nodes)** vÃ  cÃ¡c **cáº¡nh (edges)** ná»‘i giá»¯a chÃºng. DÃ¹ng Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a cÃ¡c má»‘i quan há»‡.
-   **Node Classification**: BÃ i toÃ¡n dá»± Ä‘oÃ¡n nhÃ£n cho tá»«ng Ä‘á»‰nh trong Ä‘á»“ thá»‹. *VÃ­ dá»¥: PhÃ¢n loáº¡i má»™t ngÆ°á»i dÃ¹ng trong máº¡ng xÃ£ há»™i lÃ  bot hay ngÆ°á»i tháº­t.*
-   **Link Prediction**: BÃ i toÃ¡n dá»± Ä‘oÃ¡n xem liá»‡u cÃ³ má»™t cáº¡nh tá»“n táº¡i giá»¯a hai Ä‘á»‰nh hay khÃ´ng. *VÃ­ dá»¥: Gá»£i Ã½ káº¿t báº¡n trong máº¡ng xÃ£ há»™i.*
-   **Graph Classification**: BÃ i toÃ¡n dá»± Ä‘oÃ¡n nhÃ£n cho toÃ n bá»™ Ä‘á»“ thá»‹. *VÃ­ dá»¥: PhÃ¢n loáº¡i má»™t phÃ¢n tá»­ hÃ³a há»c lÃ  Ä‘á»™c háº¡i hay khÃ´ng.*
-   **Permutation Invariance**: TÃ­nh cháº¥t cá»§a má»™t hÃ m mÃ  output khÃ´ng thay Ä‘á»•i khi thá»© tá»± cá»§a cÃ¡c input bá»‹ hoÃ¡n vá»‹. CÃ¡c hÃ m `sum`, `mean`, `max` cÃ³ tÃ­nh cháº¥t nÃ y, ráº¥t quan trá»ng cho cÃ¡c hÃ m `Aggregate` trong GNN.

---

## ğŸ§  3. Ná»n táº£ng: Táº¡i sao cáº§n GNNs?

CÃ¡c máº¡ng nÆ¡-ron truyá»n thá»‘ng nhÆ° CNN hay RNN Ä‘Æ°á»£c thiáº¿t káº¿ cho dá»¯ liá»‡u cÃ³ cáº¥u trÃºc dáº¡ng lÆ°á»›i (grid-like) nhÆ° áº£nh (2D grid) hoáº·c vÄƒn báº£n (1D sequence). Tuy nhiÃªn, ráº¥t nhiá»u dá»¯ liá»‡u trong tháº¿ giá»›i thá»±c khÃ´ng cÃ³ cáº¥u trÃºc nÃ y, mÃ  tá»“n táº¡i dÆ°á»›i dáº¡ng Ä‘á»“ thá»‹ vá»›i cÃ¡c má»‘i quan há»‡ phá»©c táº¡p:
-   **Máº¡ng xÃ£ há»™i**: NgÆ°á»i dÃ¹ng lÃ  cÃ¡c Ä‘á»‰nh, má»‘i quan há»‡ báº¡n bÃ¨ lÃ  cÃ¡c cáº¡nh.
-   **HÃ³a há»c**: CÃ¡c nguyÃªn tá»­ lÃ  Ä‘á»‰nh, liÃªn káº¿t hÃ³a há»c lÃ  cáº¡nh.
-   **Há»‡ thá»‘ng gá»£i Ã½**: NgÆ°á»i dÃ¹ng vÃ  sáº£n pháº©m lÃ  cÃ¡c Ä‘á»‰nh, hÃ nh vi mua hÃ ng/Ä‘Ã¡nh giÃ¡ lÃ  cÃ¡c cáº¡nh.

GNNs ra Ä‘á»i Ä‘á»ƒ há»c trá»±c tiáº¿p trÃªn cáº¥u trÃºc Ä‘á»“ thá»‹ nÃ y, cho phÃ©p mÃ´ hÃ¬nh hÃ³a cÃ¡c má»‘i quan há»‡ vÃ  tÆ°Æ¡ng tÃ¡c má»™t cÃ¡ch tá»± nhiÃªn.

## ğŸ”§ 4. CÆ¡ cháº¿ cá»‘t lÃµi: Message Passing (Lan truyá»n thÃ´ng Ä‘iá»‡p)

Háº§u háº¿t cÃ¡c kiáº¿n trÃºc GNN hiá»‡n Ä‘áº¡i Ä‘á»u tuÃ¢n theo má»™t khuÃ´n khá»• chung gá»i lÃ  **Message Passing** hoáº·c **Neighborhood Aggregation**. Ã tÆ°á»Ÿng ráº¥t trá»±c quan: "Má»™t Ä‘á»‰nh Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi nhá»¯ng hÃ ng xÃ³m cá»§a nÃ³."

Má»™t layer GNN thá»±c hiá»‡n viá»‡c cáº­p nháº­t biá»ƒu diá»…n (embedding) cá»§a má»—i Ä‘á»‰nh thÃ´ng qua 3 bÆ°á»›c:

1.  **Gather (Thu tháº­p)**: Vá»›i má»—i Ä‘á»‰nh, nÃ³ "nhÃ¬n" sang cÃ¡c Ä‘á»‰nh hÃ ng xÃ³m vÃ  thu tháº­p cÃ¡c vector Ä‘áº·c trÆ°ng cá»§a chÃºng.
2.  **Aggregate (Tá»•ng há»£p)**: Äá»‰nh Ä‘Ã³ tá»•ng há»£p táº¥t cáº£ thÃ´ng tin tá»« hÃ ng xÃ³m thÃ nh má»™t "thÃ´ng Ä‘iá»‡p" duy nháº¥t. PhÃ©p tá»•ng há»£p nÃ y pháº£i cÃ³ tÃ­nh **hoÃ¡n vá»‹ báº¥t biáº¿n (permutation invariant)**, vÃ¬ cÃ¡c hÃ ng xÃ³m khÃ´ng cÃ³ thá»© tá»± cá»‘ Ä‘á»‹nh. CÃ¡c hÃ m phá»• biáº¿n lÃ  `sum`, `mean`, hoáº·c `max`.
3.  **Update (Cáº­p nháº­t)**: Äá»‰nh Ä‘Ã³ sá»­ dá»¥ng thÃ´ng Ä‘iá»‡p tá»•ng há»£p tá»« hÃ ng xÃ³m vÃ  vector Ä‘áº·c trÆ°ng hiá»‡n táº¡i cá»§a chÃ­nh nÃ³ Ä‘á»ƒ tÃ­nh toÃ¡n ra vector Ä‘áº·c trÆ°ng má»›i cho layer tiáº¿p theo. BÆ°á»›c nÃ y thÆ°á»ng bao gá»“m má»™t phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh vÃ  má»™t hÃ m kÃ­ch hoáº¡t phi tuyáº¿n.

Khi xáº¿p chá»“ng nhiá»u layer GNN lÃªn nhau, má»™t Ä‘á»‰nh cÃ³ thá»ƒ tá»•ng há»£p thÃ´ng tin tá»« cÃ¡c hÃ ng xÃ³m ngÃ y cÃ ng xa hÆ¡n (hÃ ng xÃ³m cá»§a hÃ ng xÃ³m, v.v.), cho phÃ©p nÃ³ há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng cáº¥u trÃºc phá»©c táº¡p hÆ¡n.

---

## âš™ï¸ 5. Tháº» thuáº­t toÃ¡n - GCN (Graph Convolutional Network)

### 1. BÃ i toÃ¡n & dá»¯ liá»‡u
- **BÃ i toÃ¡n**: Há»c biá»ƒu diá»…n (embedding) cho cÃ¡c Ä‘á»‰nh trong Ä‘á»“ thá»‹ Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥ nhÆ° phÃ¢n loáº¡i Ä‘á»‰nh (Node Classification), phÃ¢n loáº¡i Ä‘á»“ thá»‹ (Graph Classification).
- **Dá»¯ liá»‡u**: Äá»“ thá»‹ $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, vá»›i $\mathbf{X}$ lÃ  ma tráº­n Ä‘áº·c trÆ°ng Ä‘á»‰nh, $\mathbf{A}$ lÃ  ma tráº­n ká».
- **á»¨ng dá»¥ng**: PhÃ¢n loáº¡i bÃ i bÃ¡o khoa há»c, phÃ¢n loáº¡i ngÆ°á»i dÃ¹ng trong máº¡ng xÃ£ há»™i, phÃ¢n tÃ­ch máº¡ng lÆ°á»›i.

### 2. MÃ´ hÃ¬nh & cÃ´ng thá»©c
- **Ã tÆ°á»Ÿng cá»‘t lÃµi**: Tá»•ng há»£p thÃ´ng tin tá»« cÃ¡c Ä‘á»‰nh hÃ ng xÃ³m vÃ  thÃ´ng tin cá»§a chÃ­nh Ä‘á»‰nh Ä‘Ã³, sau Ä‘Ã³ biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh vÃ  Ã¡p dá»¥ng hÃ m kÃ­ch hoáº¡t phi tuyáº¿n.
- **CÃ´ng thá»©c má»™t layer GCN**:
  $$ \mathbf{H}^{(l+1)} = \sigma \left( \tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{H}^{(l)} \mathbf{W}^{(l)} \right) $$
  Trong Ä‘Ã³:
  -   $\mathbf{H}^{(l)}$: Ma tráº­n biá»ƒu diá»…n Ä‘á»‰nh (embeddings) cá»§a layer $l$. $\mathbf{H}^{(0)} = \mathbf{X}$.
  -   $\tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}$: Ma tráº­n ká» vá»›i cÃ¡c vÃ²ng láº·p tá»± ná»‘i (self-loops).
  -   $\tilde{\mathbf{D}}$: Ma tráº­n báº­c (degree matrix) cá»§a $\tilde{\mathbf{A}}$.
  -   $\mathbf{W}^{(l)}$: Ma tráº­n trá»ng sá»‘ há»c Ä‘Æ°á»£c cá»§a layer $l$.
  -   $\sigma$: HÃ m kÃ­ch hoáº¡t phi tuyáº¿n (vÃ­ dá»¥: ReLU).

### 3. Loss & má»¥c tiÃªu
- **Má»¥c tiÃªu**: Tá»‘i thiá»ƒu hÃ³a hÃ m máº¥t mÃ¡t trÃªn cÃ¡c nhÃ£n Ä‘Ã£ biáº¿t (vÃ­ dá»¥: Cross-entropy cho phÃ¢n loáº¡i Ä‘á»‰nh).
- **Loss**: Phá»¥ thuá»™c vÃ o tÃ¡c vá»¥. Äá»‘i vá»›i Node Classification, thÆ°á»ng lÃ  cross-entropy chá»‰ tÃ­nh trÃªn cÃ¡c Ä‘á»‰nh Ä‘Ã£ Ä‘Æ°á»£c gÃ¡n nhÃ£n.

### 4. Tá»‘i Æ°u hoÃ¡ & cáº­p nháº­t
- **Algorithm**: Lan truyá»n ngÆ°á»£c (Backpropagation) Ä‘á»ƒ cáº­p nháº­t cÃ¡c ma tráº­n trá»ng sá»‘ $\mathbf{W}^{(l)}$.
- **Optimizer**: ThÆ°á»ng lÃ  Adam hoáº·c SGD.

### 5. Hyperparams
- **Sá»‘ layer GCN**: ThÆ°á»ng Ã­t (2-3 layer) do váº¥n Ä‘á» over-smoothing.
- **Learning Rate**: 0.01-0.001.
- **Hidden dimension**: KÃ­ch thÆ°á»›c vector biá»ƒu diá»…n áº©n cho má»—i Ä‘á»‰nh.

### 6. Äá»™ phá»©c táº¡p
- **Time**: $O(|\mathcal{E}| D L)$ vá»›i $|\mathcal{E}|$ lÃ  sá»‘ cáº¡nh, $D$ lÃ  sá»‘ chiá»u embedding, $L$ lÃ  sá»‘ layer. CÃ³ thá»ƒ tá»‘n kÃ©m vá»›i Ä‘á»“ thá»‹ lá»›n.
- **Space**: $O(|\mathcal{V}| D + |\mathcal{E}|)$ vá»›i $|\mathcal{V}|$ lÃ  sá»‘ Ä‘á»‰nh.

### 7. Metrics Ä‘Ã¡nh giÃ¡
- **Node Classification**: Accuracy, F1-score.
- **Graph Classification**: Accuracy.

### 8. Æ¯u / NhÆ°á»£c Ä‘iá»ƒm
**Æ¯u Ä‘iá»ƒm**:
-   Há»c biá»ƒu diá»…n máº¡nh máº½ cho dá»¯ liá»‡u Ä‘á»“ thá»‹.
-   Táº­n dá»¥ng Ä‘Æ°á»£c cáº¥u trÃºc cá»¥c bá»™ cá»§a Ä‘á»“ thá»‹.
-   Dá»… hiá»ƒu vÃ  triá»ƒn khai (so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn spectral graph theory).

**NhÆ°á»£c Ä‘iá»ƒm**:
-   **Transductive**: ThÆ°á»ng chá»‰ hoáº¡t Ä‘á»™ng tá»‘t trÃªn cÃ¡c Ä‘á»“ thá»‹ Ä‘Ã£ biáº¿t trong quÃ¡ trÃ¬nh training (khÃ´ng kháº£ nÄƒng quy náº¡p trÃªn cÃ¡c Ä‘á»‰nh má»›i hoáº·c Ä‘á»“ thá»‹ má»›i).
-   **Over-smoothing**: Khi stacking quÃ¡ nhiá»u layer GCN, cÃ¡c biá»ƒu diá»…n Ä‘á»‰nh cÃ³ xu hÆ°á»›ng trá»Ÿ nÃªn giá»‘ng nhau.
-   **ChÆ°a giáº£i quyáº¿t tá»‘t bÃ i toÃ¡n Ä‘á»“ thá»‹ lá»›n**: YÃªu cáº§u toÃ n bá»™ ma tráº­n ká», tá»‘n bá»™ nhá»›.

### 9. Báº«y & máº¹o
- **Báº«y**: Over-smoothing khi dÃ¹ng quÃ¡ nhiá»u layer.
- **Báº«y**: KhÃ³ khÄƒn vá»›i Ä‘á»“ thá»‹ lá»›n do yÃªu cáº§u ma tráº­n ká».
- **Máº¹o**: Sá»­ dá»¥ng Dropout Ä‘á»ƒ chá»‘ng overfitting.
- **Máº¹o**: Sá»­ dá»¥ng Early Stopping.
- **Máº¹o**: Feature Engineering cho Ä‘á»‰nh vÃ  cáº¡nh.

### 10. Pseudocode (má»™t layer GCN):
```python
# H(l) lÃ  ma tráº­n Ä‘áº·c trÆ°ng cá»§a cÃ¡c Ä‘á»‰nh á»Ÿ layer l
# A_hat = D_tilde^(-1/2) * A_tilde * D_tilde^(-1/2) (ma tráº­n ká» Ä‘Æ°á»£c chuáº©n hÃ³a)
# W(l) lÃ  ma tráº­n trá»ng sá»‘ há»c Ä‘Æ°á»£c

H_next = ReLU(A_hat @ H_current @ W)
# (Trong PyTorch Geometric, A_hat Ä‘Æ°á»£c xá»­ lÃ½ hiá»‡u quáº£ hÆ¡n)
```

### 11. Code máº«u (GCN Layer vá»›i PyTorch Geometric)
```python
import torch
from torch_geometric.nn import GCNConv

class GCN(torch.nn.Module):
    def __init__(self, num_node_features, hidden_channels, num_classes):
        super().__init__()
        # GCNConv lÃ  má»™t lá»›p GCN
        self.conv1 = GCNConv(num_node_features, hidden_channels)
        self.relu = torch.nn.ReLU(inplace=True)
        self.conv2 = GCNConv(hidden_channels, num_classes)

    def forward(self, x, edge_index):
        # x: ma tráº­n Ä‘áº·c trÆ°ng cá»§a cÃ¡c Ä‘á»‰nh (num_nodes, num_node_features)
        # edge_index: ma tráº­n cáº¡nh (2, num_edges)
        
        x = self.conv1(x, edge_index)
        x = self.relu(x) # Ãp dá»¥ng hÃ m kÃ­ch hoáº¡t ReLU
        x = self.conv2(x, edge_index)
        return x

# VÃ­ dá»¥ khá»Ÿi táº¡o vÃ  sá»­ dá»¥ng
# from torch_geometric.datasets import Planetoid
# from torch_geometric.data import Data
#
# # Táº£i má»™t dataset Ä‘á»“ thá»‹ (vÃ­ dá»¥: Cora)
# dataset = Planetoid(root='/tmp/Cora', name='Cora')
# data = dataset[0] # Láº¥y Ä‘á»“ thá»‹ Ä‘áº§u tiÃªn
#
# # Khá»Ÿi táº¡o mÃ´ hÃ¬nh GCN
# model = GCN(num_node_features=dataset.num_node_features, 
#             hidden_channels=16, 
#             num_classes=dataset.num_classes)
#
# # Forward pass
# # output = model(data.x, data.edge_index)
# # print(output.shape) # (num_nodes, num_classes)
```

### 12. Checklist kiá»ƒm tra nhanh:
- [ ] Dá»¯ liá»‡u Ä‘á»“ thá»‹ cÃ³ Ä‘Æ°á»£c biá»ƒu diá»…n Ä‘Ãºng (feature Ä‘á»‰nh, ma tráº­n ká»)?
- [ ] Sá»‘ layer GCN cÃ³ phÃ¹ há»£p (trÃ¡nh over-smoothing)?
- [ ] HÃ m máº¥t mÃ¡t vÃ  optimizer cÃ³ Ä‘Æ°á»£c chá»n Ä‘Ãºng cho tÃ¡c vá»¥ khÃ´ng?
- [ ] CÃ³ thá»ƒ trá»±c quan hÃ³a embeddings Ä‘á»ƒ hiá»ƒu nhá»¯ng gÃ¬ GNN há»c Ä‘Æ°á»£c khÃ´ng?

---

## ğŸ›ï¸ 6. CÃ¡c kiáº¿n trÃºc GNN phá»• biáº¿n

### 6.1 Graph Convolutional Networks (GCN)

-   **TÆ° tÆ°á»Ÿng**: GCN lÃ  má»™t trong nhá»¯ng kiáº¿n trÃºc GNN tiÃªn phong, má»Ÿ rá»™ng Ã½ tÆ°á»Ÿng cá»§a phÃ©p tÃ­ch cháº­p (convolution) tá»« dá»¯ liá»‡u dáº¡ng lÆ°á»›i (áº£nh) sang dá»¯ liá»‡u Ä‘á»“ thá»‹. NÃ³ Ä‘Æ¡n giáº£n hÃ³a cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn miá»n táº§n sá»‘ (spectral graph theory) thÃ nh má»™t cÃ¡ch tiáº¿p cáº­n dá»±a trÃªn lan truyá»n thÃ´ng Ä‘iá»‡p trong miá»n khÃ´ng gian (spatial domain).
-   **CÃ¡ch hoáº¡t Ä‘á»™ng**: Má»—i layer GCN tÃ­nh toÃ¡n biá»ƒu diá»…n má»›i cho má»™t Ä‘á»‰nh báº±ng cÃ¡ch tá»•ng há»£p thÃ´ng tin tá»« chÃ­nh nÃ³ vÃ  cÃ¡c Ä‘á»‰nh hÃ ng xÃ³m. PhÃ©p tá»•ng há»£p nÃ y thÆ°á»ng lÃ  má»™t trung bÃ¬nh cÃ³ trá»ng sá»‘, nÆ¡i cÃ¡c Ä‘á»‰nh cÃ³ báº­c cao hÆ¡n Ä‘Æ°á»£c chuáº©n hÃ³a Ä‘á»ƒ trÃ¡nh áº£nh hÆ°á»Ÿng quÃ¡ má»©c.
-   **CÃ´ng thá»©c má»™t layer GCN (Spectral perspective simplified to Spatial)**:
    $$ \mathbf{H}^{(l+1)} = \sigma \left( \tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{H}^{(l)} \mathbf{W}^{(l)} \right) $$
    Trong Ä‘Ã³:
    -   $\mathbf{H}^{(l)}$: Ma tráº­n biá»ƒu diá»…n Ä‘á»‰nh (embeddings) cá»§a layer $l$. $\mathbf{H}^{(0)} = \mathbf{X}$ (ma tráº­n Ä‘áº·c trÆ°ng ban Ä‘áº§u).
    -   $\tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}$: Ma tráº­n ká» $\mathbf{A}$ Ä‘Æ°á»£c thÃªm cÃ¡c vÃ²ng láº·p tá»± ná»‘i (self-loops) $\mathbf{I}$ Ä‘á»ƒ Ä‘á»‰nh $v$ cÅ©ng tá»•ng há»£p thÃ´ng tin tá»« chÃ­nh nÃ³.
    -   $\tilde{\mathbf{D}}$: Ma tráº­n báº­c (degree matrix) cá»§a $\tilde{\mathbf{A}}$. Viá»‡c chuáº©n hÃ³a $\tilde{\mathbf{D}}^{-\frac{1}{2}}$ giÃºp á»•n Ä‘á»‹nh quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  trÃ¡nh cÃ¡c giÃ¡ trá»‹ embedding bá»‹ thá»•i phá»“ng.
    -   $\mathbf{W}^{(l)}$: Ma tráº­n trá»ng sá»‘ há»c Ä‘Æ°á»£c cá»§a layer $l$.
    -   $\sigma$: HÃ m kÃ­ch hoáº¡t phi tuyáº¿n (vÃ­ dá»¥: ReLU).
-   **Æ¯u Ä‘iá»ƒm**:
    -   ÄÆ¡n giáº£n, dá»… hiá»ƒu vÃ  dá»… triá»ƒn khai.
    -   Hiá»‡u quáº£ cho nhiá»u tÃ¡c vá»¥ trÃªn Ä‘á»“ thá»‹ bÃ¡n giÃ¡m sÃ¡t (semi-supervised).
-   **NhÆ°á»£c Ä‘iá»ƒm**:
    -   **Transductive**: Chá»§ yáº¿u hoáº¡t Ä‘á»™ng trÃªn cÃ¡c Ä‘á»“ thá»‹ Ä‘Ã£ biáº¿t trong quÃ¡ trÃ¬nh training.
    -   **Over-smoothing**: Khi xáº¿p chá»“ng nhiá»u layer, biá»ƒu diá»…n cá»§a cÃ¡c Ä‘á»‰nh cÃ³ xu hÆ°á»›ng trá»Ÿ nÃªn ráº¥t giá»‘ng nhau, lÃ m máº¥t Ä‘i kháº£ nÄƒng phÃ¢n biá»‡t.
    -   **KhÃ´ng má»Ÿ rá»™ng (Not scalable)**: YÃªu cáº§u toÃ n bá»™ ma tráº­n ká», tá»‘n bá»™ nhá»› vÃ  tÃ­nh toÃ¡n cho Ä‘á»“ thá»‹ lá»›n.

### 6.2 GraphSAGE (Graph SAmple and aggreGatE)

-   **TÆ° tÆ°á»Ÿng**: GraphSAGE giáº£i quyáº¿t má»™t háº¡n cháº¿ lá»›n cá»§a GCN lÃ  kháº£ nÄƒng má»Ÿ rá»™ng (scalability) vÃ  kháº£ nÄƒng quy náº¡p (inductive capability). Thay vÃ¬ yÃªu cáº§u toÃ n bá»™ Ä‘á»“ thá»‹ vÃ  hoáº¡t Ä‘á»™ng theo kiá»ƒu transductive, GraphSAGE Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ há»c cÃ¡c hÃ m tá»•ng há»£p cÃ³ thá»ƒ Ã¡p dá»¥ng cho cÃ¡c Ä‘á»‰nh má»›i hoáº·c Ä‘á»“ thá»‹ má»›i chÆ°a tá»«ng tháº¥y trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.
-   **CÃ¡ch hoáº¡t Ä‘á»™ng**: GraphSAGE há»c má»™t hÃ m Ä‘á»ƒ táº¡o ra cÃ¡c biá»ƒu diá»…n Ä‘á»‰nh báº±ng cÃ¡ch láº¥y máº«u (sample) vÃ  tá»•ng há»£p (aggregate) cÃ¡c Ä‘áº·c trÆ°ng tá»« cÃ¡c hÃ ng xÃ³m cá»§a má»—i Ä‘á»‰nh. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n láº·p Ä‘i láº·p láº¡i qua nhiá»u layer.
-   **Quy trÃ¬nh chÃ­nh**:
    1.  **Láº¥y máº«u hÃ ng xÃ³m (Neighbor Sampling)**: Äá»‘i vá»›i má»—i Ä‘á»‰nh, GraphSAGE láº¥y máº«u má»™t táº­p con cá»‘ Ä‘á»‹nh cÃ¡c hÃ ng xÃ³m, thay vÃ¬ sá»­ dá»¥ng táº¥t cáº£ hÃ ng xÃ³m. Äiá»u nÃ y giÃºp kiá»ƒm soÃ¡t bá»™ nhá»› vÃ  thá»i gian tÃ­nh toÃ¡n.
    2.  **Tá»•ng há»£p thÃ´ng tin (Information Aggregation)**: ThÃ´ng tin tá»« cÃ¡c hÃ ng xÃ³m Ä‘Ã£ Ä‘Æ°á»£c láº¥y máº«u sau Ä‘Ã³ Ä‘Æ°á»£c tá»•ng há»£p báº±ng má»™t hÃ m tá»•ng há»£p (aggregator function) thÃ nh má»™t vector duy nháº¥t. HÃ m tá»•ng há»£p pháº£i cÃ³ tÃ­nh cháº¥t hoÃ¡n vá»‹ báº¥t biáº¿n (permutation invariant) vÃ¬ thá»© tá»± hÃ ng xÃ³m khÃ´ng quan trá»ng.
-   **CÃ¡c hÃ m Aggregator phá»• biáº¿n**:
    -   **Mean Aggregator**: Láº¥y trung bÃ¬nh embedding cá»§a cÃ¡c hÃ ng xÃ³m (tÆ°Æ¡ng tá»± vá»›i GCN).
    -   **Pooling Aggregator**: Ãp dá»¥ng má»™t máº¡ng nÆ¡-ron nhá» (MLP) lÃªn embedding cá»§a má»—i hÃ ng xÃ³m, sau Ä‘Ã³ Ã¡p dá»¥ng Max Pooling hoáº·c Mean Pooling.
    -   **LSTM Aggregator**: Ãp dá»¥ng má»™t máº¡ng LSTM lÃªn má»™t hoÃ¡n vá»‹ ngáº«u nhiÃªn cá»§a cÃ¡c hÃ ng xÃ³m. Äiá»u nÃ y cho phÃ©p máº¡ng náº¯m báº¯t cÃ¡c máº«u phá»©c táº¡p hÆ¡n, nhÆ°ng khÃ´ng cÃ²n hoÃ¡n vá»‹ báº¥t biáº¿n hoÃ n toÃ n.
-   **CÃ´ng thá»©c (cho Mean Aggregator)**:
    -   Aggregate: $\mathbf{h}_{\mathcal{N}(v)}^{(k)} = \text{MEAN} \left( \{ \mathbf{h}_u^{(k)} \mid u \in \mathcal{N}(v) \} \right)$
    -   Update: $\mathbf{h}_v^{(k+1)} = \sigma \left( \mathbf{W}^{(k)} \cdot \text{CONCAT}(\mathbf{h}_v^{(k)}, \mathbf{h}_{\mathcal{N}(v)}^{(k)}) \right)$
-   **Lá»£i Ã­ch**:
    -   **Inductive Capability**: CÃ³ kháº£ nÄƒng táº¡o embeddings cho cÃ¡c Ä‘á»‰nh má»›i hoáº·c toÃ n bá»™ Ä‘á»“ thá»‹ má»›i chÆ°a tháº¥y trong training.
    -   **Scalability**: Nhá» chiáº¿n lÆ°á»£c láº¥y máº«u, cÃ³ thá»ƒ Ã¡p dá»¥ng cho cÃ¡c Ä‘á»“ thá»‹ ráº¥t lá»›n (hÃ ng tá»· Ä‘á»‰nh vÃ  cáº¡nh).
    -   **Linh hoáº¡t**: Cho phÃ©p tÃ¹y chá»‰nh cÃ¡c hÃ m tá»•ng há»£p.

### 6.3 Graph Attention Networks (GAT)

-   **TÆ° tÆ°á»Ÿng**: GAT giáº£i quyáº¿t má»™t háº¡n cháº¿ cá»§a cÃ¡c GNN trÆ°á»›c Ä‘Ã³ (nhÆ° GCN vÃ  GraphSAGE) lÃ  viá»‡c gÃ¡n táº§m quan trá»ng nhÆ° nhau cho táº¥t cáº£ cÃ¡c hÃ ng xÃ³m. Thay vÃ o Ä‘Ã³, GAT giá»›i thiá»‡u cÆ¡ cháº¿ **attention** Ä‘á»ƒ há»c má»™t cÃ¡ch linh hoáº¡t vá» má»©c Ä‘á»™ quan trá»ng cá»§a má»—i Ä‘á»‰nh hÃ ng xÃ³m Ä‘á»‘i vá»›i má»™t Ä‘á»‰nh trung tÃ¢m.
-   **CÃ¡ch hoáº¡t Ä‘á»™ng**:
    1.  **TÃ­nh toÃ¡n há»‡ sá»‘ Attention**: Äá»‘i vá»›i má»—i cáº·p Ä‘á»‰nh $(i, j)$ trong Ä‘Ã³ $j$ lÃ  hÃ ng xÃ³m cá»§a $i$, GAT tÃ­nh toÃ¡n má»™t há»‡ sá»‘ attention $e_{ij}$. Há»‡ sá»‘ nÃ y thá»ƒ hiá»‡n má»©c Ä‘á»™ liÃªn quan hoáº·c táº§m quan trá»ng cá»§a Ä‘á»‰nh $j$ Ä‘á»‘i vá»›i Ä‘á»‰nh $i$. Viá»‡c nÃ y thÆ°á»ng Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch Ã¡p dá»¥ng má»™t phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh vÃ  hÃ m kÃ­ch hoáº¡t LeakyReLU cho sá»± káº¿t há»£p cá»§a cÃ¡c embedding cá»§a $i$ vÃ  $j$.
    2.  **Chuáº©n hÃ³a Attention**: CÃ¡c há»‡ sá»‘ attention thÃ´ $e_{ij}$ Ä‘Æ°á»£c chuáº©n hÃ³a báº±ng hÃ m `softmax` trÃªn táº¥t cáº£ cÃ¡c hÃ ng xÃ³m cá»§a Ä‘á»‰nh $i$ Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c cÃ¡c trá»ng sá»‘ attention $\alpha_{ij}$ cÃ³ tá»•ng báº±ng 1.
    3.  **Tá»•ng há»£p thÃ´ng tin**: Biá»ƒu diá»…n má»›i cá»§a Ä‘á»‰nh $i$ ($h_i'$) Ä‘Æ°á»£c tÃ­nh báº±ng tá»•ng cÃ³ trá»ng sá»‘ cá»§a cÃ¡c biá»ƒu diá»…n (embedding) cá»§a cÃ¡c Ä‘á»‰nh hÃ ng xÃ³m, vá»›i trá»ng sá»‘ chÃ­nh lÃ  cÃ¡c há»‡ sá»‘ attention $\alpha_{ij}$ Ä‘Ã£ há»c.
-   **CÃ´ng thá»©c (cho má»™t head attention)**:
    -   TÃ­nh attention scores: $e_{ij} = \text{LeakyReLU}(\mathbf{a}^T [\mathbf{W}\mathbf{h}_i \, \Vert \, \mathbf{W}\mathbf{h}_j])$
    -   Chuáº©n hÃ³a attention: $\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}(i) \cup \{i\}} \exp(e_{ik})}$
    -   Update: $\mathbf{h}_i' = \sigma \left( \sum_{j \in \mathcal{N}(i) \cup \{i\}} \alpha_{ij} \mathbf{W}\mathbf{h}_j \right)$
    -   (LÆ°u Ã½: $\Vert$ lÃ  phÃ©p ná»‘i vector, $\mathbf{a}$ lÃ  vector trá»ng sá»‘ attention há»c Ä‘Æ°á»£c).
-   **Multi-head Attention**: Äá»ƒ tÄƒng cÆ°á»ng kháº£ nÄƒng biá»ƒu diá»…n vÃ  á»•n Ä‘á»‹nh quÃ¡ trÃ¬nh huáº¥n luyá»‡n, GAT thÆ°á»ng sá»­ dá»¥ng Multi-head Attention (tÆ°Æ¡ng tá»± Transformer), trong Ä‘Ã³ nhiá»u "head" attention Ä‘á»™c láº­p Ä‘Æ°á»£c tÃ­nh toÃ¡n vÃ  káº¿t quáº£ cá»§a chÃºng Ä‘Æ°á»£c ná»‘i (concatenate) hoáº·c láº¥y trung bÃ¬nh.
-   **Lá»£i Ã­ch**:
    -   **Kháº£ nÄƒng biá»ƒu diá»…n máº¡nh máº½**: Há»c Ä‘Æ°á»£c cÃ¡c má»‘i quan há»‡ phá»©c táº¡p hÆ¡n báº±ng cÃ¡ch gÃ¡n trá»ng sá»‘ khÃ¡c nhau cho cÃ¡c hÃ ng xÃ³m.
    -   **Giáº£i thÃ­ch Ä‘Æ°á»£c (Interpretability)**: CÃ¡c há»‡ sá»‘ attention cÃ³ thá»ƒ cung cáº¥p cÃ¡i nhÃ¬n vá» má»©c Ä‘á»™ quan trá»ng cá»§a cÃ¡c hÃ ng xÃ³m.
    -   **Inductive**: CÃ³ kháº£ nÄƒng Ã¡p dá»¥ng cho cÃ¡c cáº¥u trÃºc Ä‘á»“ thá»‹ chÆ°a tháº¥y (tÆ°Æ¡ng tá»± GraphSAGE).
    -   **KhÃ´ng yÃªu cáº§u ma tráº­n ká»**: ThÃ­ch há»£p cho cÃ¡c Ä‘á»“ thá»‹ Ä‘á»™ng (dynamic graphs) hoáº·c cÃ¡c Ä‘á»“ thá»‹ khÃ´ng rÃµ rÃ ng.
-   **NhÆ°á»£c Ä‘iá»ƒm**:
    -   Phá»©c táº¡p hÆ¡n vá» máº·t tÃ­nh toÃ¡n so vá»›i GCN.
    -   ÄÃ´i khi cÃ³ thá»ƒ gáº·p váº¥n Ä‘á» vá» bá»™ nhá»› vá»›i Ä‘á»“ thá»‹ lá»›n do viá»‡c tÃ­nh toÃ¡n cÃ¡c cáº·p attention.
## ğŸ¯ 6. BÃ i táº­p vÃ  Tham kháº£o

### 6.1 BÃ i táº­p thá»±c hÃ nh
1.  **Node Classification trÃªn Cora**: Sá»­ dá»¥ng bá»™ dá»¯ liá»‡u Cora (máº¡ng lÆ°á»›i trÃ­ch dáº«n khoa há»c), xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh GCN Ä‘á»ƒ phÃ¢n loáº¡i cÃ¡c bÃ i bÃ¡o khoa há»c vÃ o cÃ¡c lÄ©nh vá»±c khÃ¡c nhau.
2.  **Link Prediction**: XÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh GNN Ä‘á»ƒ dá»± Ä‘oÃ¡n cÃ¡c má»‘i quan há»‡ báº¡n bÃ¨ chÆ°a Ä‘Æ°á»£c thiáº¿t láº­p trong má»™t máº¡ng xÃ£ há»™i.
3.  **GraphSAGE vs. GAT**: Implement cáº£ hai mÃ´ hÃ¬nh trÃªn cÃ¹ng má»™t bá»™ dá»¯ liá»‡u vÃ  so sÃ¡nh hiá»‡u suáº¥t, thá»i gian huáº¥n luyá»‡n.

### 6.2 TÃ i liá»‡u tham kháº£o
-   **ThÆ° viá»‡n**: `PyTorch Geometric (PyG)`, `Deep Graph Library (DGL)`.
-   **KhÃ³a há»c**: Stanford CS224W: Machine Learning with Graphs.
-   **BÃ i bÃ¡o quan trá»ng**:
    -   "Semi-Supervised Classification with Graph Convolutional Networks" (GCN paper).
    -   "Inductive Representation Learning on Large Graphs" (GraphSAGE paper).
    -   "Graph Attention Networks" (GAT paper).

---
*ChÃºc báº¡n há»c táº­p hiá»‡u quáº£! ğŸš€*

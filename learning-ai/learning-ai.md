# üöÄ Learning AI ‚Äî L·ªô tr√¨nh chuy√™n gia Data Analyst, Data Science, AI/ML/LLM

> **M·ª•c ti√™u**: T·ª´ n·ªÅn t·∫£ng ƒë·∫øn tri·ªÉn khai h·ªá th·ªëng AI/LLM th·ª±c t·∫ø, theo chu·∫©n th·ª±c h√†nh c·ªßa ng√†nh. T·∫•t c·∫£ m·ª•c ƒë·ªÅu k√®m ngu·ªìn ch√≠nh th·ª©c ho·∫∑c h·ªçc li·ªáu uy t√≠n ƒë·ªÉ t·ª± h·ªçc c√≥ ƒë·ªãnh h∆∞·ªõng.

## üß† **Global Theory Integration & Academic Excellence Framework**

### **1. Theoretical Foundation Integration**

**Academic Excellence Standards:**
- **50% Theory / 50% Practice**: Balanced curriculum across all learning paths
- **Mathematical Rigor**: Deep understanding of mathematical foundations
- **Computer Science Theory**: Algorithm analysis, complexity theory, system design
- **Research Methodology**: Literature review, experimental design, evaluation frameworks
- **Industry Best Practices**: Production deployment, scalability, reliability engineering

**Research-Driven Learning Approach:**
- **Core Papers**: Foundational research papers in AI/ML/Data Science
- **Recent Advances**: State-of-the-art developments and breakthroughs
- **Implementation Guides**: Practical applications and best practices
- **Evaluation Metrics**: Standardized benchmarks and assessment criteria

### **2. Advanced Mathematical Theory Framework**

**Linear Algebra & Optimization:**
```python
class AdvancedMathematicalTheory:
    """Advanced mathematical theory framework cho AI/ML"""
    
    @staticmethod
    def explain_advanced_foundations():
        """Explain advanced mathematical foundations"""
        print("""
        **Advanced Mathematical Foundations for AI/ML:**
        
        1. **Advanced Linear Algebra:**
           - **Eigenvalue Theory**: Spectral decomposition, power iteration
           - **Matrix Factorizations**: SVD, QR, Cholesky, LU decomposition
           - **Linear Transformations**: Projections, rotations, scaling, shearing
           - **Applications**: PCA, LDA, recommendation systems, image processing
        
        2. **Advanced Calculus & Optimization:**
           - **Multivariate Calculus**: Gradients, Hessians, Jacobians, chain rule
           - **Convex Optimization**: Convexity, Lagrange multipliers, KKT conditions
           - **Numerical Methods**: Gradient descent variants, Newton's method, line search
           - **Applications**: Neural network training, parameter optimization, hyperparameter tuning
        
        3. **Advanced Probability & Statistics:**
           - **Bayesian Inference**: Prior selection, posterior computation, MCMC
           - **Statistical Learning Theory**: VC dimension, Rademacher complexity, generalization bounds
           - **Information Theory**: Entropy, mutual information, KL divergence, rate-distortion theory
           - **Applications**: Model uncertainty, decision theory, causal inference, reinforcement learning
        
        4. **Advanced Graph Theory & Algorithms:**
           - **Graph Representations**: Adjacency matrices, edge lists, incidence matrices
           - **Graph Algorithms**: Shortest path, minimum spanning tree, maximum flow
           - **Network Analysis**: Centrality measures, clustering coefficients, community detection
           - **Applications**: Social networks, recommendation systems, knowledge graphs, molecular biology
        """)
    
    @staticmethod
    def demonstrate_advanced_concepts():
        """Demonstrate advanced mathematical concepts"""
        
        import numpy as np
        import matplotlib.pyplot as plt
        from scipy import linalg, stats
        from scipy.optimize import minimize
        import seaborn as sns
        
        class AdvancedMathematicalAnalyzer:
            """Analyze advanced mathematical concepts in AI/ML context"""
            
            def __init__(self):
                self.results = {}
            
            def demonstrate_advanced_linear_algebra(self):
                """Demonstrate advanced linear algebra concepts"""
                
                print("**Advanced Linear Algebra Demonstration:**")
                
                # Create complex data matrix with structure
                np.random.seed(42)
                n_samples, n_features = 200, 15
                
                # Generate data with multiple clusters
                cluster_centers = np.array([
                    [2, 2, 2] + [0] * (n_features - 3),  # Cluster 1
                    [-2, -2, -2] + [0] * (n_features - 3),  # Cluster 2
                    [0, 0, 3] + [0] * (n_features - 3),  # Cluster 3
                ])
                
                data = np.random.randn(n_samples, n_features)
                cluster_assignments = np.random.choice(3, n_samples)
                
                for i, cluster in enumerate(cluster_assignments):
                    data[i] += cluster_centers[cluster] * 0.8
                
                # 1. Singular Value Decomposition (SVD)
                U, s, Vt = linalg.svd(data, full_matrices=False)
                
                # 2. Principal Component Analysis (PCA)
                data_centered = data - np.mean(data, axis=0)
                cov_matrix = np.cov(data_centered.T)
                eigenvalues, eigenvectors = linalg.eigh(cov_matrix)
                
                # Sort by eigenvalues (descending)
                sorted_indices = np.argsort(eigenvalues)[::-1]
                eigenvalues = eigenvalues[sorted_indices]
                eigenvectors = eigenvectors[:, sorted_indices]
                
                # Project data onto principal components
                data_pca = data_centered @ eigenvectors
                
                # 3. Linear Discriminant Analysis (LDA) approximation
                # Calculate between-class and within-class scatter matrices
                overall_mean = np.mean(data, axis=0)
                between_class_scatter = np.zeros((n_features, n_features))
                within_class_scatter = np.zeros((n_features, n_features))
                
                for cluster_id in range(3):
                    cluster_data = data[cluster_assignments == cluster_id]
                    cluster_mean = np.mean(cluster_data, axis=0)
                    n_cluster = len(cluster_data)
                    
                    # Between-class scatter
                    diff = cluster_mean - overall_mean
                    between_class_scatter += n_cluster * np.outer(diff, diff)
                    
                    # Within-class scatter
                    for sample in cluster_data:
                        diff = sample - cluster_mean
                        within_class_scatter += np.outer(diff, diff)
                
                # Solve generalized eigenvalue problem
                try:
                    eigenvals_lda, eigenvecs_lda = linalg.eigh(between_class_scatter, within_class_scatter)
                    # Sort by eigenvalues (descending)
                    sorted_indices_lda = np.argsort(eigenvals_lda)[::-1]
                    eigenvals_lda = eigenvals_lda[sorted_indices_lda]
                    eigenvecs_lda = eigenvecs_lda[:, sorted_indices_lda]
                    
                    # Project data onto LDA components
                    data_lda = data_centered @ eigenvecs_lda
                except:
                    print("LDA computation failed due to singular matrix")
                    data_lda = None
                
                print(f"Original data shape: {data.shape}")
                print(f"PCA data shape: {data_pca.shape}")
                print(f"Number of SVD singular values: {len(s)}")
                print(f"Top 5 singular values: {s[:5]}")
                print(f"Explained variance ratio (PCA): {eigenvalues[:5] / np.sum(eigenvalues)}")
                
                # Visualize results
                fig, axes = plt.subplots(2, 3, figsize=(18, 12))
                
                # Original data (first 2 dimensions)
                axes[0, 0].scatter(data[:, 0], data[:, 1], c=cluster_assignments, cmap='viridis', alpha=0.7)
                axes[0, 0].set_xlabel('Feature 1')
                axes[0, 0].set_ylabel('Feature 2')
                axes[0, 0].set_title('Original Data (First 2 Features)')
                axes[0, 0].grid(True, alpha=0.3)
                
                # PCA projection (first 2 components)
                axes[0, 1].scatter(data_pca[:, 0], data_pca[:, 1], c=cluster_assignments, cmap='viridis', alpha=0.7)
                axes[0, 1].set_xlabel('Principal Component 1')
                axes[0, 1].set_ylabel('Principal Component 2')
                axes[0, 1].set_title('PCA Projection')
                axes[0, 1].grid(True, alpha=0.3)
                
                # LDA projection (if available)
                if data_lda is not None:
                    axes[0, 2].scatter(data_lda[:, 0], data_lda[:, 1], c=cluster_assignments, cmap='viridis', alpha=0.7)
                    axes[0, 2].set_xlabel('LDA Component 1')
                    axes[0, 2].set_ylabel('LDA Component 2')
                    axes[0, 2].set_title('LDA Projection')
                    axes[0, 2].grid(True, alpha=0.3)
                else:
                    axes[0, 2].text(0.5, 0.5, 'LDA Failed\n(Singular Matrix)', 
                                   ha='center', va='center', transform=axes[0, 2].transAxes)
                    axes[0, 2].set_title('LDA Projection')
                
                # SVD singular values
                axes[1, 0].plot(range(1, len(s) + 1), s, 'bo-', linewidth=2)
                axes[1, 0].set_xlabel('Singular Value Index')
                axes[1, 0].set_ylabel('Singular Value')
                axes[1, 0].set_title('SVD Singular Values')
                axes[1, 0].set_yscale('log')
                axes[1, 0].grid(True, alpha=0.3)
                
                # PCA explained variance
                cumulative_variance = np.cumsum(eigenvalues) / np.sum(eigenvalues)
                axes[1, 1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'ro-', linewidth=2)
                axes[1, 1].set_xlabel('Principal Component')
                axes[1, 1].set_ylabel('Cumulative Explained Variance')
                axes[1, 1].set_title('PCA Cumulative Explained Variance')
                axes[1, 1].grid(True, alpha=0.3)
                
                # Feature importance (first principal component)
                feature_importance = np.abs(eigenvectors[:, 0])
                top_features_idx = np.argsort(feature_importance)[-10:]
                top_features = [f'F{i+1}' for i in top_features_idx]
                top_importance = feature_importance[top_features_idx]
                
                axes[1, 2].barh(range(len(top_features)), top_importance)
                axes[1, 2].set_yticks(range(len(top_features)))
                axes[1, 2].set_yticklabels(top_features)
                axes[1, 2].set_xlabel('Feature Importance (PC1)')
                axes[1, 2].set_title('Top Features by PC1')
                axes[1, 2].grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.show()
                
                return {
                    'data': data,
                    'cluster_assignments': cluster_assignments,
                    'svd': (U, s, Vt),
                    'pca': (eigenvalues, eigenvectors, data_pca),
                    'lda': (eigenvals_lda, eigenvecs_lda, data_lda) if data_lda is not None else None
                }
            
            def demonstrate_advanced_optimization(self):
                """Demonstrate advanced optimization concepts"""
                
                print("\n**Advanced Optimization Demonstration:**")
                
                # Define complex optimization problems
                def rosenbrock_3d(x):
                    """3D Rosenbrock function: f(x,y,z) = (1-x)¬≤ + 100(y-x¬≤)¬≤ + (1-y)¬≤ + 100(z-y¬≤)¬≤"""
                    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2 + (1 - x[1])**2 + 100 * (x[2] - x[1]**2)**2
                
                def himmelblau(x):
                    """Himmelblau function: f(x,y) = (x¬≤ + y - 11)¬≤ + (x + y¬≤ - 7)¬≤"""
                    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2
                
                def ackley(x):
                    """Ackley function: f(x,y) = -20*exp(-0.2*sqrt(0.5*(x¬≤+y¬≤))) - exp(0.5*(cos(2œÄx)+cos(2œÄy))) + e + 20"""
                    return -20 * np.exp(-0.2 * np.sqrt(0.5 * (x[0]**2 + x[1]**2))) - \
                           np.exp(0.5 * (np.cos(2 * np.pi * x[0]) + np.cos(2 * np.pi * x[1]))) + np.e + 20
                
                # Test different optimization algorithms
                optimization_problems = {
                    'Rosenbrock 3D': (rosenbrock_3d, [-1.5, -1.5, -1.5]),
                    'Himmelblau': (himmelblau, [-2, -2]),
                    'Ackley': (ackley, [2, 2])
                }
                
                results = {}
                
                for problem_name, (objective_func, start_point) in optimization_problems.items():
                    print(f"\nOptimizing {problem_name}...")
                    
                    # Try different optimization methods
                    methods = ['L-BFGS-B', 'CG', 'BFGS', 'Powell']
                    method_results = {}
                    
                    for method in methods:
                        try:
                            result = minimize(objective_func, start_point, method=method, 
                                            options={'maxiter': 1000})
                            method_results[method] = {
                                'success': result.success,
                                'x': result.x,
                                'fun': result.fun,
                                'nit': result.nit,
                                'nfev': result.nfev
                            }
                        except Exception as e:
                            method_results[method] = {'error': str(e)}
                    
                    results[problem_name] = method_results
                    
                    # Print results
                    print(f"  Results for {problem_name}:")
                    for method, result in method_results.items():
                        if 'error' not in result:
                            print(f"    {method}: x={result['x']}, f(x)={result['fun']:.6f}, "
                                  f"iterations={result['nit']}, evaluations={result['nfev']}")
                        else:
                            print(f"    {method}: Error - {result['error']}")
                
                # Visualize optimization landscapes
                fig, axes = plt.subplots(1, 3, figsize=(18, 6))
                
                # Himmelblau function
                x = np.linspace(-5, 5, 100)
                y = np.linspace(-5, 5, 100)
                X, Y = np.meshgrid(x, y)
                Z_himmelblau = himmelblau([X, Y])
                
                contour1 = axes[0].contour(X, Y, Z_himmelblau, levels=20, colors='black', alpha=0.5)
                axes[0].clabel(contour1, inline=True, fontsize=8)
                axes[0].set_xlabel('x')
                axes[0].set_ylabel('y')
                axes[0].set_title('Himmelblau Function')
                axes[0].grid(True, alpha=0.3)
                
                # Mark known minima
                known_minima = [(3, 2), (-2.81, 3.13), (-3.78, -3.28), (3.58, -1.85)]
                for x_min, y_min in known_minima:
                    axes[0].plot(x_min, y_min, 'r*', markersize=15, label=f'Min at ({x_min}, {y_min})')
                axes[0].legend()
                
                # Ackley function
                Z_ackley = ackley([X, Y])
                
                contour2 = axes[1].contour(X, Y, Z_ackley, levels=20, colors='black', alpha=0.5)
                axes[1].clabel(contour2, inline=True, fontsize=8)
                axes[1].set_xlabel('x')
                axes[1].set_ylabel('y')
                axes[1].set_title('Ackley Function')
                axes[1].grid(True, alpha=0.3)
                
                # Mark global minimum
                axes[1].plot(0, 0, 'r*', markersize=15, label='Global Min at (0, 0)')
                axes[1].legend()
                
                # Rosenbrock 3D (show 2D projection)
                def rosenbrock_2d(x, y):
                    return (1 - x)**2 + 100 * (y - x**2)**2
                
                Z_rosenbrock = rosenbrock_2d(X, Y)
                
                contour3 = axes[2].contour(X, Y, Z_rosenbrock, levels=20, colors='black', alpha=0.5)
                axes[2].clabel(contour3, inline=True, fontsize=8)
                axes[2].set_xlabel('x')
                axes[2].set_ylabel('y')
                axes[2].set_title('Rosenbrock Function (2D Projection)')
                axes[2].grid(True, alpha=0.3)
                
                # Mark global minimum
                axes[2].plot(1, 1, 'r*', markersize=15, label='Global Min at (1, 1)')
                axes[2].legend()
                
                plt.tight_layout()
                plt.show()
                
                return results
            
            def demonstrate_advanced_statistics(self):
                """Demonstrate advanced statistical concepts"""
                
                print("\n**Advanced Statistics Demonstration:**")
                
                # Generate complex synthetic data
                np.random.seed(42)
                n_samples = 2000
                
                # Multiple distributions with different characteristics
                data_distributions = {
                    'Normal': np.random.normal(0, 1, n_samples),
                    'Log-normal': np.random.lognormal(0, 1, n_samples),
                    'Exponential': np.random.exponential(1, n_samples),
                    'Gamma': np.random.gamma(2, 2, n_samples),
                    'Beta': np.random.beta(2, 5, n_samples)
                }
                
                # Statistical analysis
                statistical_summary = {}
                
                for name, data in data_distributions.items():
                    # Basic statistics
                    mean_val = np.mean(data)
                    std_val = np.std(data)
                    skew_val = stats.skew(data)
                    kurt_val = stats.kurtosis(data)
                    
                    # Percentiles
                    percentiles = np.percentile(data, [25, 50, 75])
                    
                    # Normality test
                    shapiro_stat, shapiro_p = stats.shapiro(data)
                    
                    statistical_summary[name] = {
                        'mean': mean_val,
                        'std': std_val,
                        'skewness': skew_val,
                        'kurtosis': kurt_val,
                        'percentiles': percentiles,
                        'shapiro_stat': shapiro_stat,
                        'shapiro_p': shapiro_p,
                        'is_normal': shapiro_p > 0.05
                    }
                
                # Print statistical summary
                print("Statistical Summary:")
                for name, stats_dict in statistical_summary.items():
                    print(f"\n{name} Distribution:")
                    print(f"  Mean: {stats_dict['mean']:.3f}")
                    print(f"  Std: {stats_dict['std']:.3f}")
                    print(f"  Skewness: {stats_dict['skewness']:.3f}")
                    print(f"  Kurtosis: {stats_dict['kurtosis']:.3f}")
                    print(f"  Q1: {stats_dict['percentiles'][0]:.3f}")
                    print(f"  Median: {stats_dict['percentiles'][1]:.3f}")
                    print(f"  Q3: {stats_dict['percentiles'][2]:.3f}")
                    print(f"  Shapiro-Wilk p-value: {stats_dict['shapiro_p']:.6f}")
                    print(f"  Normal distribution: {stats_dict['is_normal']}")
                
                # Advanced statistical tests
                print("\n**Advanced Statistical Tests:**")
                
                # ANOVA test between distributions
                f_stat, anova_p = stats.f_oneway(*data_distributions.values())
                print(f"ANOVA test (F-statistic: {f_stat:.3f}, p-value: {anova_p:.6f})")
                
                # Kruskal-Wallis test (non-parametric alternative)
                kw_stat, kw_p = stats.kruskal(*data_distributions.values())
                print(f"Kruskal-Wallis test (H-statistic: {kw_stat:.3f}, p-value: {kw_p:.6f})")
                
                # Effect size calculation (eta-squared approximation)
                def calculate_eta_squared(data_dict):
                    """Calculate eta-squared effect size"""
                    all_data = np.concatenate(list(data_dict.values()))
                    grand_mean = np.mean(all_data)
                    
                    # Between-group sum of squares
                    ss_between = 0
                    for name, data in data_dict.items():
                        group_mean = np.mean(data)
                        n_group = len(data)
                        ss_between += n_group * (group_mean - grand_mean)**2
                    
                    # Total sum of squares
                    ss_total = np.sum((all_data - grand_mean)**2)
                    
                    # Eta-squared
                    eta_squared = ss_between / ss_total
                    return eta_squared
                
                eta_squared = calculate_eta_squared(data_distributions)
                print(f"Effect size (eta-squared): {eta_squared:.4f}")
                
                # Visualize distributions
                fig, axes = plt.subplots(2, 3, figsize=(18, 12))
                
                # Histograms
                for i, (name, data) in enumerate(data_distributions.items()):
                    row = i // 3
                    col = i % 3
                    
                    axes[row, col].hist(data, bins=30, alpha=0.7, density=True, edgecolor='black')
                    axes[row, col].set_xlabel('Value')
                    axes[row, col].set_ylabel('Density')
                    axes[row, col].set_title(f'{name} Distribution')
                    axes[row, col].grid(True, alpha=0.3)
                    
                    # Add theoretical normal curve for comparison
                    if name == 'Normal':
                        x = np.linspace(data.min(), data.max(), 100)
                        y = stats.norm.pdf(x, np.mean(data), np.std(data))
                        axes[row, col].plot(x, y, 'r-', linewidth=2, label='Theoretical Normal')
                        axes[row, col].legend()
                
                # Q-Q plots for normality assessment
                for i, (name, data) in enumerate(data_distributions.items()):
                    row = i // 3
                    col = i % 3
                    
                    stats.probplot(data, dist="norm", plot=axes[row, col])
                    axes[row, col].set_title(f'{name} Q-Q Plot')
                    axes[row, col].grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.show()
                
                return {
                    'data_distributions': data_distributions,
                    'statistical_summary': statistical_summary,
                    'anova_test': (f_stat, anova_p),
                    'kruskal_wallis': (kw_stat, kw_p),
                    'effect_size': eta_squared
                }
        
        # Demonstrate advanced mathematical theory
        advanced_math_theory = AdvancedMathematicalTheory()
        advanced_math_theory.explain_advanced_foundations()
        
        # Demonstrate advanced mathematical analysis
        advanced_math_analyzer = AdvancedMathematicalAnalyzer()
        
        print("**Advanced Mathematical Theory Demonstration:**")
        
        # Demonstrate different advanced mathematical concepts
        linear_algebra_results = advanced_math_analyzer.demonstrate_advanced_linear_algebra()
        optimization_results = advanced_math_analyzer.demonstrate_advanced_optimization()
        statistics_results = advanced_math_analyzer.demonstrate_advanced_statistics()
        
        return advanced_math_analyzer, linear_algebra_results, optimization_results, statistics_results

# Demonstrate advanced mathematical theory
advanced_math_theory = AdvancedMathematicalTheory()
advanced_math_theory.explain_advanced_foundations()

# Demonstrate advanced mathematical analysis
advanced_math_analyzer, linear_algebra_results, optimization_results, statistics_results = advanced_math_theory.demonstrate_advanced_concepts()
```

### **3. Research Methodology & Evaluation Framework**

**Academic Research Standards:**
- **Literature Review**: Systematic review of existing research and methodologies
- **Experimental Design**: Proper experimental setup, control groups, randomization
- **Statistical Analysis**: Appropriate statistical tests, effect sizes, confidence intervals
- **Reproducibility**: Code sharing, data availability, methodology documentation
- **Peer Review**: Quality assessment, validation, constructive feedback

**Industry Best Practices:**
- **Production Deployment**: Scalability, reliability, monitoring, alerting
- **Performance Optimization**: Latency, throughput, resource utilization
- **Security & Privacy**: Data protection, access control, compliance
- **Continuous Improvement**: A/B testing, feedback loops, iterative development

**T√†i li·ªáu tham kh·∫£o chuy√™n s√¢u:**
- **Advanced Mathematics**: [Advanced Linear Algebra](https://linear.axler.net/), [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/)
- **Statistical Learning**: [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/), [Pattern Recognition](https://www.springer.com/gp/book/9780387310732)
- **Research Methods**: [Research Design](https://www.sagepub.com/sites/default/files/upm-binaries/23681_Chapter_1.pdf), [Experimental Design](https://www.wiley.com/en-us/Design+and+Analysis+of+Experiments-p-9781118146927)
- **Industry Practices**: [Site Reliability Engineering](https://sre.google/sre-book/), [Building Machine Learning Systems](https://www.oreilly.com/library/view/building-machine-learning/9781449369865/)

## üéØ **T·ªïng quan Learning Path**

## üéØ **T·ªïng quan Learning Path**

ƒê√¢y l√† l·ªô tr√¨nh h·ªçc t·∫≠p to√†n di·ªán ƒë·ªÉ tr·ªü th√†nh chuy√™n gia trong lƒ©nh v·ª±c AI/ML/Data Science, ƒë∆∞·ª£c thi·∫øt k·∫ø theo chu·∫©n th·ª±c h√†nh c·ªßa ng√†nh v·ªõi c√°c m·ª•c ti√™u r√µ r√†ng v√† c√≥ th·ªÉ ƒëo l∆∞·ªùng ƒë∆∞·ª£c.

### Mind map t·ªïng quan (c·∫≠p nh·∫≠t)

```mermaid
mindmap
  root((DA ‚Ä¢ DS ‚Ä¢ AI/ML ‚Ä¢ DL ‚Ä¢ LLM ‚Ä¢ MLOps))
    nen_tang(N·ªÅn t·∫£ng)
      Python
        OOP
        Packaging
        Testing
      toan_hoc(To√°n h·ªçc)
        ƒê·∫°i s·ªë tuy·∫øn t√≠nh
        Gi·∫£i t√≠ch & T·ªëi ∆∞u
        X√°c su·∫•t & Th·ªëng k√™
      sql(SQL)
        JOIN/CTE/Window
        Thi·∫øt k·∫ø schema
        Indexing
      truc_quan_hoa(Tr·ª±c quan h√≥a)
        Matplotlib/Seaborn
        Plotly
        Storytelling
      git_cli(Git & CLI)
    data_analyst(Data Analyst)
      Thu th·∫≠p & l√†m s·∫°ch
      EDA c√≥ h·ªá th·ªëng
      Dashboard & BI
      A/B testing
      Causal inference
    data_science_ml(Data Science / ML)
      Ti·ªÅn x·ª≠ l√Ω & FE
      H·ªçc c√≥ gi√°m s√°t
      H·ªçc kh√¥ng gi√°m s√°t
      Imbalance & sampling
      Explainability (SHAP)
      ƒê√°nh gi√° & ch·ªçn m√¥ h√¨nh
    time_series(Time Series)
      ARIMA/ETS/Prophet
      Cross-val theo th·ªùi gian
      ML/DL cho TS (N-BEATS)
    deep_learning(Deep Learning)
      Optimization & Regularization
      CNN/Detection/Segmentation
      RNN/Attention/Transformer
      pytorch(Pytorch Specialization)
        Tensors/Autograd
        nn.Module/Training Loop
        AMP/DDP
        TorchScript/ONNX/Serving
    nlp(NLP)
      Tokenization & Embedding
      BERT/T5/GPT
      ƒê√°nh gi√° (BLEU/ROUGE)
    llm_ung_dung(LLM & ·ª®ng d·ª•ng)
      Prompt engineering
      RAG & Vector DB
      Reranking
      Finetune & PEFT
      Ph·ª•c v·ª• (vLLM/Triton)
      Quantization (8/4-bit)
    mlops(MLOps)
      Tracking & Registry
      DVC & Pipelines
      CI/CD
      Monitoring & Drift
      B·∫£o m·∫≠t & Governance
    thuc_hanh_chat_luong(Th·ª±c h√†nh & Ch·∫•t l∆∞·ª£ng)
      Testing d·ªØ li·ªáu/m√¥ h√¨nh
      Reproducibility
      Clean code
      Portfolio/Interview
    stack_apps(·ª®ng d·ª•ng theo Stack)
      stack_python(Python)
        FastAPI
        Polars
        PyO3
      stack_rust(Rust)
        Axum/Actix
        Polars/Arrow
        Linfa/tch-rs
      stack_node(Node.js)
        Express/NestJS
        TensorFlow.js/Transformers.js
        NAPI/Neon
    interop(Interop)
      REST
      gRPC
      FFI (PyO3/NAPI/C-ABI)

```

![Learning AI Mindmap](assets/learning-ai-mindmap.svg)

![Learning AI Mindmap PNG](assets/learning-ai-mindmap.png)

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/learning-ai-mindmap.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/learning-ai-mindmap.png)**

**üìÅ [Xem file PNG tr·ª±c ti·∫øp](assets/learning-ai-mindmap.png)**

M·ªü r·ªông theo Mind map (ƒëi·ªÉm ch√≠nh v√† nƒÉng l·ª±c c·∫ßn ƒë·∫°t)
- N·ªÅn t·∫£ng: Python/To√°n/SQL/CLI ‚Üí m·ª•c ti√™u t·ª± ƒë·ªông ho√° t√°c v·ª•, ph√¢n t√≠ch d·ªØ li·ªáu c√≥ c·∫•u tr√∫c, l√†m ch·ªß typing/testing/packaging.
- Data Analyst: quy tr√¨nh EDA, tr·ª±c quan ho√°, storytelling, A/B testing, causal c∆° b·∫£n.
- Data Science/ML: pipeline h√≥a, ch·ªçn/ƒë√°nh gi√° m√¥ h√¨nh, explainability, x·ª≠ l√Ω imbalance.
- Time Series: backtest ƒë√∫ng chu·∫©n, ch·ªçn m√¥ h√¨nh theo seasonality, metrics ri√™ng TS.
- Deep Learning: c√¥ng th·ª©c train chu·∫©n (AMP/EMA/DDP), augmentation, t·ªëi ∆∞u.
- NLP/LLM: tokenization/Transformer, RAG/finetune, ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng v√† safety.
- MLOps: tracking/registry, pipelines/CI/CD, monitoring/rollback.
- Stack: tri·ªÉn khai Python/Rust/Node theo th·∫ø m·∫°nh t·ª´ng ng√¥n ng·ªØ; interop REST/gRPC/FFI.

## üß© Ghi ch√∫ 50/50 to√†n c·ª•c

- T·∫•t c·∫£ chuy√™n ƒë·ªÅ tu√¢n theo t·ª∑ l·ªá 50% l√Ω thuy·∫øt : 50% th·ª±c h√†nh
- M·ªói ch∆∞∆°ng ƒë·ªÅu c√≥: b·∫£ng ph√¢n b·ªï 50/50, rubric 100ƒë (30 l√Ω thuy·∫øt, 30 code, 30 k·∫øt qu·∫£, 10 b√°o c√°o)
- Weekly/Module split n√™u r√µ deliverables v√† ti√™u ch√≠ pass/fail

---

## üìã **M·ª•c l·ª•c nhanh (Clickable)**

- [üå± 1) N·ªÅn t·∫£ng b·∫Øt bu·ªôc](#1-n·ªÅn-t·∫£ng-b·∫Øt-bu·ªôc)
- [üìä 2) Data Analyst (DA)](#2-data-analyst-da)
- [ü§ñ 3) Data Science / Machine Learning](#3-data-science--machine-learning)
- [üìà 4) Time Series (TS)](#4-time-series-ts)
- [üß† 5) Deep Learning (DL)](#5-deep-learning-dl)
- [üí¨ 6) LLMs v√† ·ª©ng d·ª•ng th·ª±c t·∫ø](#6-llms-v√†-·ª©ng-d·ª•ng-th·ª±c-t·∫ø)
- [üöÄ 7) MLOps & S·∫£n xu·∫•t](#7-mlops--s·∫£n-xu·∫•t)
- [üèóÔ∏è 12) ·ª®ng d·ª•ng theo Stack](#12-·ª©ng-d·ª•ng-theo-stack-python-rust-nodejs-c√¥ng-c·ª•-v√≠-d·ª•-ngu·ªìn)

---

## üå± **1) N·ªÅn t·∫£ng b·∫Øt bu·ªôc**

- **Python**
  - Ch·ªß ƒë·ªÅ: c·∫•u tr√∫c d·ªØ li·ªáu, `itertools`, comprehension, OOP, typing, packaging (`pyproject.toml`), virtualenv/Poetry, logging, error handling, profiling, multiprocessing/async.
  - Ngu·ªìn: [Python Docs](https://docs.python.org/3/), [Real Python](https://realpython.com/), [Effective Python](https://effectivepython.com/)
  - Th·ª±c h√†nh: vi·∫øt th∆∞ vi·ªán nh·ªè, test b·∫±ng `pytest`, publish g√≥i l√™n TestPyPI.
- **NumPy, Pandas**
  - Ch·ªß ƒë·ªÅ: broadcasting, vectorization, `datetime`, `groupby`, `merge`, `categorical`, window ops, missing/outlier, performance (Arrow/Polars tham kh·∫£o).
  - Ngu·ªìn: [NumPy Docs](https://numpy.org/doc/stable/), [Pandas Docs](https://pandas.pydata.org/docs/), [Polars](https://pola.rs/)
- **Tr·ª±c quan h√≥a & Data storytelling**
  - Ch·ªß ƒë·ªÅ: bi·ªÉu ƒë·ªì ph√¢n ph·ªëi, t∆∞∆°ng quan, facet grid, interactive dashboards.
  - Ngu·ªìn: [Matplotlib](https://matplotlib.org/stable/), [Seaborn](https://seaborn.pydata.org/), [Plotly](https://plotly.com/python/)
- **SQL & Ki·∫øn tr√∫c d·ªØ li·ªáu**
  - Ch·ªß ƒë·ªÅ: JOIN/CTE/Window functions, chu·∫©n h√≥a d·ªØ li·ªáu, indexing, explain query, materialized views, time-partitioning.
  - Ngu·ªìn: [PostgreSQL Docs](https://www.postgresql.org/docs/current/), [Mode SQL Tutorial](https://mode.com/sql-tutorial/)
- **Git & CLI**
  - Ch·ªß ƒë·ªÅ: branching, PR workflow, resolving conflicts, `grep`, `sed`, `awk` c∆° b·∫£n.
  - Ngu·ªìn: [Pro Git](https://git-scm.com/book/en/v2), [SWC Unix Shell](https://swcarpentry.github.io/shell-novice/)
- **To√°n h·ªçc cho DS/AI**
  - ƒê·∫°i s·ªë tuy·∫øn t√≠nh: SVD, eigenvalues, PCA h√¨nh h·ªçc.
  - Gi·∫£i t√≠ch & t·ªëi ∆∞u: ƒë·∫°o h√†m, gradient descent, convexity.
  - X√°c su·∫•t & th·ªëng k√™: ph√¢n ph·ªëi, ∆∞·ªõc l∆∞·ª£ng, ki·ªÉm ƒë·ªãnh, kho·∫£ng tin c·∫≠y.
  - Ngu·ªìn: [3Blue1Brown‚ÄîLinear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr), [Khan Academy Statistics](https://www.khanacademy.org/math/statistics-probability), [MIT 18.06 OCW](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/)

### ‚úÖ **Checklist nhanh (N·ªÅn t·∫£ng)**

- **üêç Python**: Bi·∫øt `typing`, `pytest`, packaging, logging, profiling
- **üìä Data**: Th√†nh th·∫°o Pandas/Polars c∆° b·∫£n, SQL JOIN/CTE/Window
- **üîß CLI/Git**: Thao t√°c nhanh, branch/PR/resolve conflict
- **üßÆ To√°n**: N·∫Øm ch·∫Øc vector‚Äìma tr·∫≠n, gradient, x√°c su·∫•t c∆° b·∫£n

### üí° **Best Practices**

- **üìù Code Quality**: Vi·∫øt code c√≥ type hints, c·∫•u tr√∫c module r√µ r√†ng, test t·ªëi thi·ªÉu
- **üìã Logging**: Ghi log chu·∫©n (level, context); c·∫•u h√¨nh env qua `.env`
- **üîß Automation**: D√πng `pre-commit` (ruff/black/isort) ƒë·ªÉ gi·ªØ ch·∫•t l∆∞·ª£ng m√£

### ‚ö†Ô∏è **Pitfalls th∆∞·ªùng g·∫∑p**

- **‚è∞ Time Handling**: L·∫´n l·ªôn ki·ªÉu th·ªùi gian/timezone; ph√©p t√≠nh tr√™n c·ªôt Pandas kh√¥ng vector h√≥a
- **üóÑÔ∏è Database**: SQL kh√¥ng d√πng index/EXPLAIN d·∫´n t·ªõi ch·∫≠m; qu√™n x·ª≠ l√Ω NULL

### üìÅ **Template c·∫•u tr√∫c d·ª± √°n Python t·ªëi thi·ªÉu**

```
project/
‚îú‚îÄ‚îÄ pyproject.toml          # Project configuration
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ project/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_main.py
‚îî‚îÄ‚îÄ .env.example           # Environment variables template
```

---

## üìä **2) Data Analyst (DA)**

- Quy tr√¨nh: ƒë·∫∑t c√¢u h·ªèi kinh doanh ‚Üí thu th·∫≠p/l√†m s·∫°ch ‚Üí EDA ‚Üí gi·∫£ thuy·∫øt ‚Üí tr·ª±c quan h√≥a/dashboards ‚Üí khuy·∫øn ngh·ªã.
- K·ªπ nƒÉng c·ªët l√µi: SQL n√¢ng cao, EDA, th·ªëng k√™ suy lu·∫≠n, A/B testing, causal inference c∆° b·∫£n.
- C√¥ng c·ª•: Excel/Google Sheets, SQL (Postgres/BigQuery), BI (Tableau/Power BI/Looker), Python + Pandas/Seaborn/Plotly, Streamlit.
- Ngu·ªìn: [Tableau](https://help.tableau.com/current/guides/everybody-install/en-us/everybody_install_en-us.pdf), [Power BI](https://learn.microsoft.com/power-bi/), [Google Analytics](https://support.google.com/analytics/)

### ‚úÖ **Checklist DA**

- **üîç EDA Process**: Quy tr√¨nh EDA c√≥ checklist; x·ª≠ l√Ω missing/outlier; k·ªÉ chuy·ªán b·∫±ng bi·ªÉu ƒë·ªì
- **üóÑÔ∏è SQL Skills**: Vi·∫øt ƒë∆∞·ª£c CTE nhi·ªÅu t·∫ßng; window functions ph·ªï bi·∫øn
- **üìä Dashboard**: C√≥ filter, drill-down, di·ªÖn gi·∫£i insight

### üí° **Best Practices**

- **üìö Documentation**: L∆∞u c√°c truy v·∫•n th√†nh repo; chu·∫©n h√≥a metric ƒë·ªãnh nghƒ©a
- **üß™ Testing**: ƒê·∫∑t c√¢u h·ªèi kinh doanh th√†nh gi·∫£ thuy·∫øt ƒëo l∆∞·ªùng ƒë∆∞·ª£c; A/B v·ªõi power analysis

### ‚ö†Ô∏è **Pitfalls**

- **üîó Correlation vs Causation**: Nh·∫ßm nh√¢n qu·∫£ v√† t∆∞∆°ng quan; ch·ªçn chart kh√¥ng ph√π h·ª£p; sampling bias

### üìã **Template b√°o c√°o ng·∫Øn (1 trang)**

- **üéØ V·∫•n ƒë·ªÅ/KPI m·ª•c ti√™u** ‚Üí **üìä D·ªØ li·ªáu/Ph∆∞∆°ng ph√°p** ‚Üí **üìà K·∫øt qu·∫£ ch√≠nh (bi·ªÉu ƒë·ªì)** ‚Üí **üí° Khuy·∫øn ngh·ªã h√†nh ƒë·ªông**

---

## ü§ñ **3) Data Science / Machine Learning**

- Ti·ªÅn x·ª≠ l√Ω & Feature Engineering
  - Scaling/encoding, x·ª≠ l√Ω thi·∫øu/ngo·∫°i lai, target/mean encoding, feature selection (MI, permutation), leakage awareness.
  - Ngu·ªìn: [sklearn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)
- H·ªçc c√≥ gi√°m s√°t
  - Tuy·∫øn t√≠nh (Linear/Logistic), c√¢y quy·∫øt ƒë·ªãnh, SVM, kNN, ensemble (RF, XGBoost/LightGBM/CatBoost).
  - Ngu·ªìn: [scikit-learn Guide](https://scikit-learn.org/stable/user_guide.html), [XGBoost](https://xgboost.readthedocs.io/en/stable/), [LightGBM](https://lightgbm.readthedocs.io/), [CatBoost](https://catboost.ai/en/docs/)
- H·ªçc kh√¥ng gi√°m s√°t
  - Clustering (KMeans, GMM, DBSCAN), gi·∫£m chi·ªÅu (PCA, t-SNE/UMAP), ph√°t hi·ªán b·∫•t th∆∞·ªùng.
  - Ngu·ªìn: [sklearn Clustering](https://scikit-learn.org/stable/modules/clustering.html)
- Imbalance & sampling
  - Class weights, SMOTE, threshold moving, metrics ph√π h·ª£p (AUCPR, F1).
  - Ngu·ªìn: [Imbalanced-learn](https://imbalanced-learn.org/stable/)
- Explainability & fairness
  - SHAP/LIME, partial dependence, fairness checks.
  - Ngu·ªìn: [SHAP](https://shap.readthedocs.io/en/latest/), [AIF360](https://aif360.mybluemix.net/)
- ƒê√°nh gi√° & ch·ªçn m√¥ h√¨nh
  - Cross-validation, nested CV, calibration, uncertainty, ML system errors (data/model/infra).
  - Ngu·ªìn: [sklearn Model Evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)

Checklist DS/ML
- Pipeline t·ª´ preprocessing ‚Üí model ‚Üí evaluation c√≥ l∆∞u seed/version.
- D√πng CV ph√π h·ª£p; ch·ªçn metric theo b√†i to√°n; ki·ªÉm tra leakage.
- L∆∞u artifacts: params, metrics, confusion matrix/feature importance.

Best practices
- S·ª≠ d·ª•ng `Pipeline`/`ColumnTransformer`; chu·∫©n h√≥a ch·ªâ tr√™n train.
- Tune b·∫±ng `RandomizedSearchCV`/`Optuna` c√≥ early stop, log ƒë·∫ßy ƒë·ªß.

Pitfalls
- Leakage t·ª´ target encoding sai; so s√°nh metric kh√°c t·∫≠p; kh√¥ng fix random seed.

Template tracking th√≠ nghi·ªám (YAML)
```
experiment: name
data_version: v1
features: [cols]
model: xgboost
metrics: [auc, f1]
notes: short desc
```

---

### 4) Time Series (TS)

- M√¥ h√¨nh c·ªï ƒëi·ªÉn: ARIMA/SARIMA, ETS, Prophet; decomposition, seasonality.
- ML/DL cho TS: lags/rolling features, XGBoost/LightGBM; ki·∫øn tr√∫c N-BEATS, Temporal Convolutional Networks, Transformer cho TS.
- Cross-validation theo th·ªùi gian, backtesting; metrics: MAPE, sMAPE, MASE.
- Ngu·ªìn: [statsmodels](https://www.statsmodels.org/stable/index.html), [Prophet](https://facebook.github.io/prophet/), [darts](https://unit8co.github.io/darts/), [GluonTS](https://ts.gluon.ai/stable/)

Checklist TS
- Split theo th·ªùi gian; backtest rolling; tr√°nh leakage h·ªìi c·ªë.
- D√πng metrics ph√π h·ª£p (MASE, sMAPE); ki·ªÉm tra seasonality/holiday.

Best practices
- T·∫°o ƒë·∫∑c tr∆∞ng l·ªãch; s·ª≠ d·ª•ng cross-val theo th·ªùi gian; baseline r√µ r√†ng.

Pitfalls
- Shuffle d·ªØ li·ªáu theo th·ªùi gian; ch·ªçn horizon/lag kh√¥ng h·ª£p l√Ω.

---

### 5) Deep Learning (DL)

- N·ªÅn t·∫£ng: autograd, initialization (He/Xavier), activations (ReLU/GELU), batch norm, dropout, regularization, optimization (SGD/Adam/AdamW), lr schedules (cosine, OneCycle), early stopping, mixed precision, gradient clipping.
- C√¥ng c·ª• & framework: PyTorch, TensorFlow/Keras, PyTorch Lightning.
- Ngu·ªìn: [D2L.ai](https://d2l.ai/), [PyTorch Docs](https://pytorch.org/docs/stable/), [Keras](https://keras.io/), [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/)

Computer Vision (CV)
- Ph√¢n lo·∫°i, ph√°t hi·ªán (Faster R-CNN, YOLO), ph√¢n ƒëo·∫°n (U-Net, Mask R-CNN), augmentation (Albumentations), metrics (IoU, mAP), Grad-CAM.
- Ngu·ªìn: [torchvision](https://pytorch.org/vision/stable/), [OpenCV](https://docs.opencv.org/4.x/), [Albumentations](https://albumentations.ai/docs/)

Checklist DL
- T√°i l·∫≠p k·∫øt qu·∫£: seed, deterministic (n·∫øu c·∫ßn), log lr/scheduler.
- S·ª≠ d·ª•ng mixed precision, gradient clipping; theo d√µi loss/metrics/overfit.

Best practices
- DataLoader t·ªëi ∆∞u (num_workers, pin_memory); checkpoint theo metric.
- T√°ch c·∫•u h√¨nh (yaml) v·ªõi code; d√πng Lightning ƒë·ªÉ chu·∫©n h√≥a train loop.

Pitfalls
- B√πng n·ªï gradient; qu√™n eval mode/`torch.no_grad()` khi infer; leak VRAM do gi·ªØ graph.

Recipes hu·∫•n luy·ªán th·ª±c chi·∫øn (PyTorch)
- T·ªëi ∆∞u h√≥a
  - Optimizer: AdamW (Œ≤1=0.9, Œ≤2=0.999, weight_decay=1e-2 cho transformer; 1e-4 cho CNN).
  - L·ªãch h·ªçc: Cosine decay + warmup 3‚Äì5%; OneCycle cho CNN v·ª´a/nh·ªè.
  - AMP: `torch.cuda.amp.autocast` + `GradScaler`; gi·∫£m VRAM, tƒÉng t·ªëc ~1.3‚Äì2√ó.
- Regularization
  - Label smoothing (0.1) cho ph√¢n lo·∫°i; Mixup/CutMix cho CNN; dropout 0.1‚Äì0.3.
  - Stochastic Depth/DropPath cho ResNet/ViT.
- Augmentation (CV)
  - Train: RandAugment/AutoAugment; ColorJitter; RandomResizedCrop; Cutout.
  - Eval: Ten-crop/center-crop chu·∫©n; chu·∫©n h√≥a theo th·ªëng k√™ dataset.
- Checkpointing & EMA
  - L∆∞u best theo metric val; EMA weights 0.999‚Äì0.9999 ƒë·ªÉ ·ªïn ƒë·ªãnh inference.
- DDP/ZeRO
  - DDP (native) cho multi-GPU; ki·ªÉm tra seed/`torch.backends.cudnn.benchmark`.
  - ZeRO (DeepSpeed) cho m√¥ h√¨nh l·ªõn; gradient checkpointing ƒë·ªÉ gi·∫£m VRAM.
- Theo d√µi hu·∫•n luy·ªán
  - Log loss/lr/throughput, th·ªùi gian m·ªói step; c·∫£nh b√°o NaN; profile `torch.profiler`.

M·∫´u code loop v·ªõi AMP + Grad Accumulation
```python
for step, (x, y) in enumerate(loader):
    with torch.cuda.amp.autocast():
        logits = model(x)
        loss = loss_fn(logits, y) / grad_accum
    scaler.scale(loss).backward()
    if (step + 1) % grad_accum == 0:
        scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)
        scheduler.step()
```

Chu·∫©n h√≥a b√°o c√°o DL
- B·∫£ng: params, FLOPs, throughput (img/s), latency (ms), VRAM peak, top-1/top-5, mAP/IoU.
- Bi·ªÉu ƒë·ªì: loss/accuracy theo epoch; LR; confusion matrix; Precision-Recall.

Super-Resolution 4√ó (Real-ESRGAN) ‚Äî CLI Python & Rust
- Y√™u c·∫ßu: ph√≥ng to ·∫£nh 4√ó, ch·ªëng v·ª°/nh√≤e/gi·∫£ ƒëi·ªÉm ·∫£nh.
- Python (PyTorch, Real-ESRGAN):
  - C√†i ƒë·∫∑t (train/infer):
```bash
pip install realesrgan basicsr facexlib gfpgan torch torchvision
# Inference nhanh v·ªõi pretrain (x4):
python -m realesrgan.inference_realesrgan -n RealESRGAN_x4plus -i input.jpg -o output_x4.jpg

# Train (v√≠ d·ª• t·ªëi gi·∫£n):
git clone https://github.com/xinntao/Real-ESRGAN && cd Real-ESRGAN
pip install -r requirements.txt
# Chu·∫©n b·ªã dataset: ƒë·∫∑t ·∫£nh train v√†o datasets/; c·∫•u h√¨nh YAML theo m·∫´u options/train_realesrnet_x4plus.yml
python -m basicsr.train -opt options/train_realesrnet_x4plus.yml
```
- Python CLI t·ªëi gi·∫£n (g√≥i l·∫°i inference):
```bash
python - << 'PY'
import argparse, subprocess
parser = argparse.ArgumentParser()
parser.add_argument('-i','--input', required=True)
parser.add_argument('-o','--output', required=True)
parser.add_argument('-m','--model', default='RealESRGAN_x4plus')
args = parser.parse_args()
subprocess.check_call(['python','-m','realesrgan.inference_realesrgan','-n',args.model,'-i',args.input,'-o',args.output])
PY
```

- Rust CLI (∆∞u ti√™n: `realesrgan-ncnn-vulkan` ‚Äî hi·ªáu nƒÉng cao, kh√¥ng c·∫ßn CUDA):
```bash
cargo init --bin rsrgan && cd rsrgan
cat > Cargo.toml <<'TOML'
[package]
name = "rsrgan"
version = "0.1.0"
edition = "2021"

[dependencies]
clap = { version = "4", features = ["derive"] }
anyhow = "1"
TOML
cat > src/main.rs <<'RS'
use clap::Parser; use std::process::Command; use anyhow::Result;

#[derive(Parser)]
struct Args { #[arg(short, long)] input: String, #[arg(short, long)] output: String, #[arg(long, default_value_t=4)] scale: u32 }

fn main() -> Result<()> {
    let args = Args::parse();
    // G·ªçi realesrgan-ncnn-vulkan (c·∫ßn c√†i s·∫µn trong PATH)
    let status = Command::new("realesrgan-ncnn-vulkan")
        .args(["-i", &args.input, "-o", &args.output, "-s", &args.scale.to_string(), "-n", "realesrgan-x4plus"])
        .status()?;
    if !status.success() { anyhow::bail!("realesrgan-ncnn-vulkan failed") };
    Ok(())
}
RS
cargo build --release
./target/release/rsrgan -i input.jpg -o output_x4.jpg --scale 4
```

Ghi ch√∫ c√†i ƒë·∫∑t `realesrgan-ncnn-vulkan`:
- Linux/macOS: t·∫£i binary ph√°t h√†nh t·ª´ repo `nihui/realesrgan-ncnn-vulkan`, `chmod +x` v√† th√™m v√†o PATH.
- Windows: d√πng b·∫£n `.exe` ph√°t h√†nh, ho·∫∑c add v√†o PATH v√† g·ªçi tr·ª±c ti·∫øp t·ª´ CLI Rust.

G·ª£i √Ω ch·ªçn Rust backend:
- T·ªëc ƒë·ªô/tri·ªÉn khai ƒë∆°n gi·∫£n: `realesrgan-ncnn-vulkan` (khuy√™n d√πng cho CLI s·∫£n xu·∫•t, ƒëa n·ªÅn t·∫£ng, kh√¥ng c·∫ßn CUDA).
- T√≠ch h·ª£p pipeline ONNX s·∫µn c√≥: d√πng `ort` v·ªõi model ONNX Real-ESRGAN x4 t∆∞∆°ng th√≠ch.

Benchmark: Python (PyTorch Real-ESRGAN) vs Rust (realesrgan-ncnn-vulkan)
- M·ª•c ti√™u: so s√°nh th·ªùi gian x·ª≠ l√Ω 1 ·∫£nh 4K v√† throughput batch (N ·∫£nh), c√πng ch·∫•t l∆∞·ª£ng (PSNR/SSIM) n·∫øu c√≥ ground truth.
- Thi·∫øt l·∫≠p:
  - M√°y: c√πng CPU/GPU, t·∫Øt turbo n·∫øu c·∫ßn; Python d√πng Real-ESRGAN x4 pretrained; Rust d√πng `realesrgan-ncnn-vulkan -n realesrgan-x4plus`.
  - D·ªØ li·ªáu: b·ªô ·∫£nh 4K c·ªë ƒë·ªãnh (v√≠ d·ª• 10 ·∫£nh), ƒëo warmup (1 ·∫£nh) r·ªìi ch·∫°y ch√≠nh.
- L·ªánh ƒëo th·ªùi gian:
```bash
# Python (th·ªùi gian n·ªôi suy trung b√¨nh m·ªói ·∫£nh)
python - << 'PY'
import time, glob, subprocess
imgs = sorted(glob.glob('imgs4k/*.jpg'))
start = time.time()
for i,p in enumerate(imgs):
  subprocess.check_call(['python','-m','realesrgan.inference_realesrgan','-n','RealESRGAN_x4plus','-i',p,'-o',f'out_py/{i}.jpg'], stdout=subprocess.DEVNULL)
dt = (time.time() - start)/len(imgs)
print('python_avg_s_per_img', dt)
PY

# Rust (ncnn)
python - << 'PY'
import time, glob, subprocess, os
imgs = sorted(glob.glob('imgs4k/*.jpg'))
start = time.time()
for i,p in enumerate(imgs):
  subprocess.check_call(['./target/release/rsrgan','-i',p,'-o',f'out_rs/{i}.jpg','--scale','4'], stdout=subprocess.DEVNULL)
dt = (time.time() - start)/len(imgs)
print('rust_avg_s_per_img', dt)
PY
```
- Thu th·∫≠p t√†i nguy√™n:
  - RSS/CPU: `pidstat -rud 1 -p <PID>` trong qu√° tr√¨nh ch·∫°y; t·ªïng h·ª£p trung b√¨nh.
  - GPU (n·∫øu d√πng PyTorch GPU build): `nvidia-smi dmon -s pucm -d 1`.
- Ch·∫•t l∆∞·ª£ng (t√πy ch·ªçn, n·∫øu c√≥ ground truth): t√≠nh PSNR/SSIM gi·ªØa output v√† ·∫£nh chu·∫©n.
- K·ª≥ v·ªçng xu h∆∞·ªõng:
  - CPU-only: `realesrgan-ncnn-vulkan` th∆∞·ªùng nhanh h∆°n PyTorch inference m·∫∑c ƒë·ªãnh v√† RSS th·∫•p h∆°n; ch√™nh l·ªách p95 nh·ªè h∆°n (√≠t jitter).
  - GPU: n·∫øu Python d√πng GPU v√† ncnn d√πng Vulkan, k·∫øt qu·∫£ ph·ª• thu·ªôc driver/GPU; th∆∞·ªùng Rust (ncnn) v·∫´n ·ªïn ƒë·ªãnh p95, nh∆∞ng p50 c√≥ th·ªÉ t∆∞∆°ng ƒë∆∞∆°ng ho·∫∑c ch·∫≠m h∆°n so GPU PyTorch.

Bi·ªÉu ƒë·ªì b·∫Øt bu·ªôc:
- Bar: th·ªùi gian trung b√¨nh/·∫£nh (s) Python vs Rust.
- Violin/box: ph√¢n ph·ªëi th·ªùi gian t·ª´ng ·∫£nh.
- Stack bar: th·ªùi gian I/O vs x·ª≠ l√Ω (n·∫øu t√°ch ƒë∆∞·ª£c) ƒë·ªÉ th·∫•y overhead.

V√≠ d·ª• plotting nhanh:
```python
import matplotlib.pyplot as plt
langs=['Python','Rust']; avg=[0.42,0.28]
plt.bar(langs, avg); plt.ylabel('s/·∫£nh'); plt.title('Real-ESRGAN x4 @4K'); plt.show()
```

ƒêo theo nhi·ªÅu ƒë·ªô ph√¢n gi·∫£i (720p ‚Üí 1080p ‚Üí 1440p ‚Üí 4K ‚Üí 8K)
- M·ª•c ti√™u: ƒë√°nh gi√° scaling theo s·ªë pixel ƒë·∫ßu v√†o, quan s√°t ƒë∆∞·ªùng cong th·ªùi gian v√† b·ªô nh·ªõ.
- Sinh b·ªô test t·ª´ m·ªôt ·∫£nh g·ªëc ch·∫•t l∆∞·ª£ng cao:
```bash
mkdir -p bench_imgs/{720p,1080p,1440p,4k,8k}
python - << 'PY'
from PIL import Image; import sys
src='input_highres.jpg'; im=Image.open(src)
sizes = {
  '720p': (1280, 720), '1080p': (1920,1080), '1440p': (2560,1440),
  '4k': (3840,2160), '8k': (7680,4320)
}
for k,(w,h) in sizes.items(): im.resize((w,h), Image.LANCZOS).save(f'bench_imgs/{k}/img.jpg')
PY
```
- Benchmark Python vs Rust theo t·ª´ng ƒë·ªô ph√¢n gi·∫£i:
```bash
python - << 'PY'
import time, subprocess
cases=['720p','1080p','1440p','4k','8k']
res_py=[]; res_rs=[]
for c in cases:
  t0=time.time(); subprocess.run(['python','-m','realesrgan.inference_realesrgan','-n','RealESRGAN_x4plus','-i',f'bench_imgs/{c}/img.jpg','-o',f'out_py_{c}.jpg'], stdout=subprocess.DEVNULL); res_py.append(time.time()-t0)
  t0=time.time(); subprocess.run(['./target/release/rsrgan','-i',f'bench_imgs/{c}/img.jpg','-o',f'out_rs_{c}.jpg','--scale','4'], stdout=subprocess.DEVNULL); res_rs.append(time.time()-t0)
print('py',res_py); print('rs',res_rs)
PY
```
- Ghi nh·∫≠n RSS/CPU (ch·∫°y ri√™ng t·ª´ng case):
  - `pidstat -rud 1 -p <PID>` trong l√∫c ch·∫°y, l·∫•y trung b√¨nh.
- Bi·ªÉu ƒë·ªì khuy·∫øn ngh·ªã:
  - ƒê∆∞·ªùng th·ªùi gian vs ƒë·ªô ph√¢n gi·∫£i (ms/log-scale n·∫øu c·∫ßn) cho Python/Rust tr√™n c√πng ƒë·ªì th·ªã.
  - Bi·ªÉu ƒë·ªì c·ªôt RSS vs ƒë·ªô ph√¢n gi·∫£i cho Python/Rust.
- V√≠ d·ª• v·∫Ω ƒë∆∞·ªùng cong (gi·∫£ l·∫≠p s·ªë li·ªáu):
```python
import matplotlib.pyplot as plt
cases=['720p','1080p','1440p','4k','8k']
py=[0.05,0.11,0.20,0.42,1.75]
rs=[0.03,0.08,0.14,0.28,1.10]
plt.plot(cases, py, '-o', label='Python'); plt.plot(cases, rs, '-o', label='Rust')
plt.ylabel('s/·∫£nh'); plt.title('Scaling Real-ESRGAN x4 theo ƒë·ªô ph√¢n gi·∫£i'); plt.legend(); plt.grid(True); plt.show()
```

Natural Language Processing (NLP)
- Ti·ªÅn x·ª≠ l√Ω (tokenization, BPE), embeddings (word2vec/GloVe), RNN/LSTM/GRU, attention, Transformer (BERT/T5/GPT), seq2seq.
- ƒê√°nh gi√°: BLEU, ROUGE, METEOR, F1, exact match.
- Ngu·ªìn: [Hugging Face‚ÄîTransformers](https://huggingface.co/docs/transformers/index), [spaCy](https://spacy.io/usage)

---

### 6) LLMs v√† ·ª©ng d·ª•ng th·ª±c t·∫ø

- Prompt engineering: zeroshot, fewshot, chain-of-thought, self-consistency, tool-use.
  - Ngu·ªìn: [Prompting Guide](https://www.promptingguide.ai/)
- RAG ki·∫øn tr√∫c: chunking (size/overlap), embeddings, index (FAISS/Milvus/Weaviate/Chroma), retrievers (BM25, dense), reranking (Cross-Encoder), caching, evaluation.
  - Ngu·ªìn: [Hugging Face‚ÄîRAG](https://huggingface.co/docs/transformers/main/en/generation_strategies#retrieval-augmented-generation), [Weaviate Docs](https://weaviate.io/developers/weaviate), [FAISS Wiki](https://github.com/facebookresearch/faiss/wiki)
- Fine-tuning & PEFT: LoRA/QLoRA, adapters, p-tuning; ch·ªçn dtype (bf16/fp16), memory-efficient attention.
  - Ngu·ªìn: [PEFT Docs](https://huggingface.co/docs/peft/index), [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes), [Accelerate](https://huggingface.co/docs/accelerate/index)
- Ph·ª•c v·ª• & T·ªëi ∆∞u suy lu·∫≠n: vLLM, TGI, TensorRT-LLM, quantization (8/4-bit: GPTQ/AWQ), KV cache, continuous batching; ƒëo l∆∞·ªùng throughput/latency/TTFT.
  - Ngu·ªìn: [vLLM](https://docs.vllm.ai/en/latest/), [TGI](https://huggingface.co/docs/text-generation-inference/index), [TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/)
- ƒê√°nh gi√° LLM: gold set, human eval, LLM-as-a-judge (r·ªßi ro), hallucination, safety.
  - Ngu·ªìn: [HELM](https://crfm.stanford.edu/helm/latest/), [MMLU](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)

Checklist LLM app
- Prompt > RAG > rerank > guardrails; log trace v√† latency.
- ƒê√°nh gi√°: gold set + human review; theo d√µi hallucination/toxicity.

Best practices
- Cache embedding/k·∫øt qu·∫£; d√πng reranker khi corpus nhi·ªÖu; streaming output.
- Ch·ªçn serving ph√π h·ª£p (vLLM/TGI) v√† ƒëo TTFT/throughput.

Pitfalls
- Overfit prompt; RAG chunking k√©m; thi·∫øu ƒë√°nh gi√° offline.

Hu·∫•n luy·ªán/ƒêi·ªÅu ch·ªânh LLM n√¢ng cao
- SFT (Supervised Fine-Tuning)
  - D·ªØ li·ªáu: c·∫∑p (instruction, output) s·∫°ch, ch·ªëng r√≤ r·ªâ; l·ªçc ƒë·ªôc h·∫°i.
  - C·∫•u h√¨nh: LoRA r=8‚Äì16, Œ±‚âà16‚Äì32, dropout 0.05‚Äì0.1; lr 2e-4‚Äì1e-4; batch hi·ªáu d·ª•ng 128‚Äì512; cosine + warmup 3%.
- Preference Optimization
  - DPO: c·∫∑p (chosen, rejected); loss tr·ª±c ti·∫øp tr√™n x√°c su·∫•t; lr 5e-6‚Äì1e-5.
  - PPO: c·∫ßn reward model + KL penalty; theo d√µi stability, clip range; t·ªën t√†i nguy√™n h∆°n.
- K·ªπ thu·∫≠t ti·∫øt ki·ªám b·ªô nh·ªõ
  - 8-bit/4-bit quantized training (QLoRA) + paged optimizers; gradient checkpointing.
- ƒê√°nh gi√° & Safety
  - Eval: GSM8K, MMLU, TruthfulQA, HellaSwag; pass@k cho coding; b√™n c·∫°nh human eval.
  - Safety: l·ªçc prompt ƒë·ªôc h·∫°i; policy RLHF guardrails; red-teaming.

M·∫´u c·∫•u h√¨nh LoRA (HF PEFT YAML gi·∫£ l·∫≠p)
```
base_model: meta-llama/Llama-3-8b-instruct
precision: bf16
lora:
  r: 16
  alpha: 32
  dropout: 0.05
train:
  lr: 0.0002
  batch_effective: 256
  max_steps: 2000
  warmup_ratio: 0.03
  scheduler: cosine
eval:
  tasks: [gsm8k, mmlu]
```

RAG n√¢ng cao
- Chunking ƒë·ªông theo entropy; hybrid retrieval (BM25 + dense); reranker cross-encoder.
- Caching retrieval; dedup; citation b·∫Øt bu·ªôc; ki·ªÉm tra factuality t·ª± ƒë·ªông.

---

### 7) MLOps & S·∫£n xu·∫•t

- Qu·∫£n l√Ω th√≠ nghi·ªám: MLflow/W&B; model registry.
- DVC & Data versioning; feature store (Feast/Databricks FS) khi c·∫ßn.
- Orchestration & pipelines: Airflow, Prefect, Dagster; CI/CD (GitHub Actions/GitLab CI) cho train/serve.
- Tri·ªÉn khai: ƒë√≥ng g√≥i v·ªõi Docker, API b·∫±ng FastAPI; batch/bulk scoring, streaming; canary/blue-green.
- Monitoring: ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu, concept drift, performance drift; alerting; feedback loop.
- B·∫£o m·∫≠t & governance: qu·∫£n l√Ω PII, quy·ªÅn truy c·∫≠p d·ªØ li·ªáu, audit, reproducibility.
- Ngu·ªìn: [MLflow](https://mlflow.org/docs/latest/index.html), [DVC](https://dvc.org/doc), [Airflow](https://airflow.apache.org/docs/), [Prefect](https://docs.prefect.io/), [Dagster](https://docs.dagster.io/), [FastAPI](https://fastapi.tiangolo.com/), [Docker Docs](https://docs.docker.com/), [Evidently](https://docs.evidentlyai.com/)

Checklist MLOps
- Experiment tracking + model registry; CI cho train + serve; data versioning.
- Monitoring data/perform drift; alerting; rollback chi·∫øn l∆∞·ª£c.

Best practices
- IaC cho h·∫° t·∫ßng; d√πng feature store khi c·∫ßn d√πng l·∫°i online/offline.
- Zero-downtime deploy (blue-green/canary); secrets qu·∫£n l√Ω t·∫≠p trung.

Pitfalls
- Thi·∫øu reproducibility; l·ªá thu·ªôc th·ªß c√¥ng; kh√¥ng c√≥ SLO/SLA r√µ r√†ng.

MLOps cho DL/LLM (tri·ªÉn khai, t·ªëi ∆∞u, gi√°m s√°t)
- Deploy
  - DL: TorchServe/TensorRT; autoscaling theo QPS/latency; canary.
  - LLM: vLLM/TGI; KV cache; continuous batching; pin CPU affinity.
- T·ªëi ∆∞u ho√° m√¥ h√¨nh
  - Quantize: int8 (PTQ), int4 (AWQ/GPTQ) cho inference; c√¢n b·∫±ng ch·∫•t l∆∞·ª£ng/latency.
  - Prune/Distill: c·∫Øt b·ªõt head/layer; distill sang k√≠ch th∆∞·ªõc nh·ªè h∆°n; ƒëo ch·ªâ s·ªë tr∆∞·ªõc/sau.
- Monitoring
  - DL: latency, throughput, l·ªói; drift input/feature; canary alarms.
  - LLM: toxicity/hallucination rate; refusal rate; feedback loop; cost per token.
- Quy tr√¨nh CI/CD
  - Build images reproducible; unit + e2e tests; benchmark gate (p95 kh√¥ng v∆∞·ª£t ng∆∞·ª°ng).
  - Rollback t·ª± ƒë·ªông n·∫øu metric x·∫•u; gi·ªØ m√¥ h√¨nh c≈© t·ªëi thi·ªÉu N ng√†y.

---

### 8) D·ª± √°n th·ª±c h√†nh (c√≥ ti√™u ch√≠ ch·∫•m ƒëi·ªÉm)

- DA: Ph√¢n t√≠ch b√°n l·∫ª
  - Ti√™u ch√≠: SQL ‚â• 10 truy v·∫•n d√πng window; EDA c√≥ checklist; dashboard c√≥ drill-down; b√°o c√°o insight c√≥ k·∫øt lu·∫≠n h√†nh ƒë·ªông.
  - D·ªØ li·ªáu: [Kaggle](https://www.kaggle.com/), [Google Dataset Search](https://datasetsearch.research.google.com/)
- DS/ML: D·ª± b√°o nhu c·∫ßu
  - Ti√™u ch√≠: cross-val theo th·ªùi gian; baseline vs ML/DL; b√°o c√°o l·ªói (MAE/MAPE/sMAPE); gi·∫£i th√≠ch top features.
  - D·ªØ li·ªáu: [M5 Forecasting](https://www.kaggle.com/competitions/m5-forecasting-accuracy)
- CV: Ph√¢n lo·∫°i ·∫£nh
  - Ti√™u ch√≠: transfer learning, augmentation, Grad-CAM; so s√°nh 2 backbone; mAP/accuracy target; inference script reproducible.
  - D·ªØ li·ªáu: [Food-101](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)
- NLP/LLM: H·ªá Q&A v·ªõi RAG
  - Ti√™u ch√≠: chi·∫øn l∆∞·ª£c chunking; evaluation EM/F1; guardrails; latency < 2s/1000 tokens ·ªü m√°y local.
  - C√¥ng c·ª•: [FAISS](https://github.com/facebookresearch/faiss/wiki), [Chroma](https://docs.trychroma.com/), [Transformers](https://huggingface.co/docs/transformers/index)

M·ªü r·ªông theo ch·ªß ƒë·ªÅ Project (ƒë·∫ßu ra & rubric)
- DA: S·ªï tay ph√¢n t√≠ch b√°n l·∫ª n√¢ng cao
  - ƒê·∫ßu ra: notebook EDA c√≥ checklist; dashboard t∆∞∆°ng t√°c; b√°o c√°o 1 trang.
  - Rubric: d·ªØ li·ªáu s·∫°ch (20%), ph√¢n t√≠ch c√≥ c·∫•u tr√∫c (30%), insight h√†nh ƒë·ªông (30%), tr√¨nh b√†y (20%).
- DS/ML: D·ª± b√°o nhu c·∫ßu theo c·ª≠a h√†ng/tu·∫ßn
  - ƒê·∫ßu ra: pipeline, backtest th·ªùi gian, b√°o c√°o l·ªói/uncertainty.
  - Rubric: baseline vs model (25%), quy tr√¨nh CV ƒë√∫ng (25%), c·∫£i thi·ªán ƒë·ªãnh l∆∞·ª£ng (25%), explainability (25%).
- CV: Ph√¢n lo·∫°i/Detection s·∫£n xu·∫•t nh·∫π
  - ƒê·∫ßu ra: script train, export ONNX/TensorRT, benchmark inference (latency/RSS).
  - Rubric: ch·∫•t l∆∞·ª£ng (40%), t·ªëi ∆∞u suy lu·∫≠n (40%), t√°i l·∫≠p/ƒë√≥ng g√≥i (20%).
- NLP/LLM: RAG + ƒë√°nh gi√° t·ª± ƒë·ªông
  - ƒê·∫ßu ra: service RAG, eval offline + human sample, guardrails.
  - Rubric: ƒë·ªô ch√≠nh x√°c (40%), ·ªïn ƒë·ªãnh latency (30%), an to√†n (15%), b√°o c√°o (15%).

---

### 9) L·ªô tr√¨nh 12 tu·∫ßn (c·∫≠p nh·∫≠t, c√≥ ƒë·∫ßu ra/ƒë√°nh gi√°)

- Tu·∫ßn 1: Python n·ªÅn t·∫£ng ‚Äî [Python Docs](https://docs.python.org/3/), [Real Python](https://realpython.com/)
  - ƒê·∫ßu ra: 3 b√†i t·∫≠p file/csv/json; mini-package + `pytest`.
- Tu·∫ßn 2: NumPy & Pandas ‚Äî [NumPy](https://numpy.org/doc/stable/), [Pandas](https://pandas.pydata.org/docs/)
  - ƒê·∫ßu ra: Notebook EDA 1 dataset; checklist ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu.
- Tu·∫ßn 3: SQL & Visualization ‚Äî [PostgreSQL](https://www.postgresql.org/docs/current/), [Seaborn](https://seaborn.pydata.org/)
  - ƒê·∫ßu ra: 10 truy v·∫•n v·ªõi window; report h√¨nh ·∫£nh c√≥ insight.
- Tu·∫ßn 4: Th·ªëng k√™ & A/B ‚Äî [Khan Academy](https://www.khanacademy.org/math/statistics-probability)
  - ƒê·∫ßu ra: mini-report A/B; code power analysis.
- Tu·∫ßn 5: ML c∆° b·∫£n ‚Äî [sklearn Guide](https://scikit-learn.org/stable/user_guide.html)
  - ƒê·∫ßu ra: pipeline + benchmark 3 model; b√°o c√°o metrics.
- Tu·∫ßn 6: FE & Tuning ‚Äî [sklearn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)
  - ƒê·∫ßu ra: c·∫£i thi·ªán ‚â• x% so baseline; gi·∫£i th√≠ch feature.
- Tu·∫ßn 7: Time Series ‚Äî [statsmodels](https://www.statsmodels.org/stable/index.html), [Prophet](https://facebook.github.io/prophet/)
  - ƒê·∫ßu ra: backtest; so s√°nh ARIMA vs ML.
- Tu·∫ßn 8: DL c∆° b·∫£n ‚Äî [D2L.ai](https://d2l.ai/), [PyTorch Tutorials](https://pytorch.org/tutorials/)
  - ƒê·∫ßu ra: MLP/CNN nh·ªè; mixed precision.
- Tu·∫ßn 9: CV ho·∫∑c NLP ‚Äî [torchvision](https://pytorch.org/vision/stable/), [Transformers](https://huggingface.co/docs/transformers/index)
  - ƒê·∫ßu ra: model + inference script; report l·ªói.
- Tu·∫ßn 10: LLM & RAG ‚Äî [HF RAG](https://huggingface.co/docs/transformers/main/en/generation_strategies#retrieval-augmented-generation), [Weaviate](https://weaviate.io/developers/weaviate)
  - ƒê·∫ßu ra: prototype QA; ƒë√°nh gi√° EM/F1.
- Tu·∫ßn 11: Tri·ªÉn khai ‚Äî [FastAPI](https://fastapi.tiangolo.com/), [Docker](https://docs.docker.com/)
  - ƒê·∫ßu ra: API + Docker; test load nh·ªè.
- Tu·∫ßn 12: MLOps & Monitoring ‚Äî [MLflow](https://mlflow.org/docs/latest/index.html), [Evidently](https://docs.evidentlyai.com/)
  - ƒê·∫ßu ra: E2E pipeline + monitoring dashboard.

---

### 10) Competency matrix (rubric t√≥m t·∫Øt)

| NƒÉng l·ª±c | Level 1 (Beginner) | Level 2 (Junior) | Level 3 (Mid/Senior) | Level 4 (Expert) |
|---|---|---|---|---|
| Python | Vi·∫øt script c∆° b·∫£n | OOP, typing, `pytest` | Packaging, profiling, async | Thi·∫øt k·∫ø lib, hi·ªáu nƒÉng |
| SQL | CRUD c∆° b·∫£n | JOIN/CTE/Window | T·ªëi ∆∞u, indexing | Thi·∫øt k·∫ø m√¥ h√¨nh d·ªØ li·ªáu |
| EDA/Viz | Bi·ªÉu ƒë·ªì c∆° b·∫£n | EDA c√≥ c·∫•u tr√∫c | Storytelling, dashboard | Truy·ªÅn th√¥ng k·∫øt qu·∫£ v·ªõi TL kinh doanh |
| ML | Fit/predict | Pipeline, CV, metrics | FE, tuning, explainability | Thi·∫øt k·∫ø h·ªá th·ªëng ML |
| DL | MLP/CNN c∆° b·∫£n | Train pipeline chu·∫©n | CV/NLP n√¢ng cao | Ki·∫øn tr√∫c & t·ªëi ∆∞u training |
| NLP/LLM | Tokenization | BERT fine-tune | RAG/PEFT | Tri·ªÉn khai LLM t·ªëi ∆∞u (vLLM) |
| MLOps | Ch·∫°y local | MLflow/DVC c∆° b·∫£n | Pipelines/CI/CD/Registry | Monitoring/Drift/SLA |
| Soft/PM | Ghi ch√©p | User story, PRD c∆° b·∫£n | ∆Øu ti√™n & giao ti·∫øp | D·∫´n d·∫Øt roadmap/mentoring |

G·ª£i √Ω ƒë√°nh gi√°: b√†i t·∫≠p ti√™u chu·∫©n h√≥a; rubric theo ti√™u ch√≠ ƒëo l∆∞·ªùng (accuracy/MAE/latency/chi ph√≠), reproducibility, ch·∫•t l∆∞·ª£ng b√°o c√°o.

---

### L√Ω thuy·∫øt chuy√™n s√¢u (Deep Theory)

T·ªëi ∆∞u ho√° trong DL
- SGD v√† bi·∫øn th·ªÉ: Momentum/Nesterov; Adam/AdamW (gi·∫£i th√≠ch weight decay chu·∫©n vs L2); Lion/Adafactor.
- L·ªãch h·ªçc: cosine/OneCycle; warmup ƒë·ªÉ ·ªïn ƒë·ªãnh ƒë·∫ßu h·ªçc; cyclical LR.
- Generalization gap: early stopping; flat minima vs sharp minima; SAM.

Regularization v√† kh√°i qu√°t ho√°
- Data augmentation nh∆∞ prior; label smoothing nh∆∞ Bayesian marginalization.
- Dropout nh∆∞ trung b√¨nh m√¥ h√¨nh; stochastic depth trong ResNet/Transformers.
- Mixup/CutMix c·∫£i thi·ªán t√≠nh tuy·∫øn t√≠nh c·ª•c b·ªô c·ªßa decision boundary.

Transformer to√°n h·ªçc
- Self-attention: Q,K,V; scaled dot-product; multi-head; residual/LayerNorm.
- V·ªã tr√≠: sinusoidal/roPE/ALiBi; ·∫£nh h∆∞·ªüng t·ªõi extrapolation ƒë·ªô d√†i.
- Complexity: O(n^2) v√† bi·∫øn th·ªÉ efficient attention (Linformer, Performer, FlashAttention, MQA/GQA).

L√Ω thuy·∫øt RAG/IR hi·ªán ƒë·∫°i
- Trade-off dense vs lexical; hybrid search; effect c·ªßa reranking (cross-encoder) l√™n precision@k.
- Error modes: lexical mismatch, semantic drift, knowledge gaps; k·ªπ thu·∫≠t triage v√† cache.

ƒê√°nh gi√° m√¥ h√¨nh ·ªü quy m√¥ s·∫£n xu·∫•t
- Thi·∫øt k·∫ø gold set ƒë·∫°i di·ªán; canary eval li√™n t·ª•c; th·ªëng k√™ drift.
- Risk & safety: prompt injection, jailbreak; mitigations (guardrails, content filter, tool-use allowlist).

### 11) B·ªô c√¥ng c·ª• & m√¥i tr∆∞·ªùng ƒë·ªÅ xu·∫•t

- Python 3.11+, Poetry ho·∫∑c pip/venv, JupyterLab/VS Code.
- Th∆∞ vi·ªán: `numpy`, `pandas`, `matplotlib`, `seaborn`, `plotly`, `scikit-learn`, `statsmodels`, `pytorch` ho·∫∑c `tensorflow`, `pytorch-lightning`, `transformers`, `datasets`, `faiss-cpu`, `weaviate-client`/`pymilvus`, `bitsandbytes`, `peft`, `accelerate`, `mlflow`, `dvc`, `fastapi`, `uvicorn`, `evidently`.

---

> T√†i li·ªáu ∆∞u ti√™n ngu·ªìn ch√≠nh th·ª©c v√† h·ªçc li·ªáu m·ªü; ƒë·ªß ƒë·ªÉ t·ª± h·ªçc t·ª´ n·ªÅn t·∫£ng ƒë·∫øn tri·ªÉn khai h·ªá th·ªëng AI/LLM th·ª±c t·∫ø ·ªü m·ª©c chuy√™n nghi·ªáp.

---

### T√¨m ki·∫øm ng·ªØ nghƒ©a t√†i li·ªáu (DL-powered search)

M·ª•c ti√™u: T·∫°o c√¥ng c·ª• t√¨m ki·∫øm ng·ªØ nghƒ©a (semantic search) tr√™n to√†n b·ªô t√†i li·ªáu n√†y, tr·∫£ v·ªÅ ƒëo·∫°n li√™n quan c√πng anchor ƒë·ªÉ nh·∫£y ƒë·∫øn ƒë√∫ng m·ª•c.

Pipeline
- T√°ch t√†i li·ªáu: chia theo heading/m·ª•c, t·∫°o anchor t·ª´ ti√™u ƒë·ªÅ (slug GitHub).
- Nh√∫ng (embedding): d√πng SentenceTransformers `all-MiniLM-L6-v2` (nh·∫π, nhanh) ho·∫∑c `bge-small`/`multilingual` n·∫øu c·∫ßn ƒëa ng√¥n ng·ªØ.
- L·∫≠p ch·ªâ m·ª•c: FAISS IndexFlatIP (cosine similarity b·∫±ng vector ƒë√£ chu·∫©n h√≥a) ho·∫∑c HNSW cho t·ªëc ƒë·ªô cao.
- Truy v·∫•n: encode c√¢u h·ªèi ‚Üí t√¨m k ƒëo·∫°n li√™n quan ‚Üí tr·∫£ v·ªÅ ti√™u ƒë·ªÅ, tr√≠ch d·∫´n v√† anchor.

CLI Python t·ªëi gi·∫£n
```python
# file: search_md.py
import re, json, argparse, unicodedata
import faiss, numpy as np
from sentence_transformers import SentenceTransformer

def slugify(s):
    s = unicodedata.normalize('NFKD', s).encode('ascii', 'ignore').decode('ascii')
    s = re.sub(r'[^a-zA-Z0-9\-\s]', '', s).strip().lower().replace(' ', '-')
    return re.sub(r'-+', '-', s)

def split_sections(md):
    sections = []
    current = {'title': 'intro', 'anchor': '', 'content': []}
    for line in md.splitlines():
        m = re.match(r'^(#{2,6})\s+(.*)', line)
        if m:
            if current['content']:
                current['text'] = '\n'.join(current['content']).strip()
                sections.append(current)
            title = m.group(2).strip()
            anchor = '#' + slugify(title)
            current = {'title': title, 'anchor': anchor, 'content': []}
        else:
            current['content'].append(line)
    if current['content']:
        current['text'] = '\n'.join(current['content']).strip()
        sections.append(current)
    return sections

def build_index(sections, model_name='sentence-transformers/all-MiniLM-L6-v2'):
    model = SentenceTransformer(model_name)
    texts = [s['title'] + '\n' + s.get('text','')[:2000] for s in sections]
    emb = model.encode(texts, normalize_embeddings=True)
    index = faiss.IndexFlatIP(emb.shape[1])
    index.add(emb.astype('float32'))
    return model, index, np.array(emb), texts

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('-f','--file', default='learning-ai.md')
    ap.add_argument('-q','--query', required=True)
    ap.add_argument('-k','--topk', type=int, default=5)
    args = ap.parse_args()
    md = open(args.file, 'r', encoding='utf-8').read()
    secs = split_sections(md)
    model, index, emb, texts = build_index(secs)
    qv = model.encode([args.query], normalize_embeddings=True).astype('float32')
    D, I = index.search(qv, args.topk)
    results = []
    for idx, score in zip(I[0], D[0]):
        s = secs[int(idx)]
        results.append({'title': s['title'], 'anchor': s['anchor'], 'score': float(score), 'preview': s.get('text','')[:300]})
    print(json.dumps(results, ensure_ascii=False, indent=2))

if __name__ == '__main__':
    main()
```

V√≠ d·ª• d√πng
```bash
pip install sentence-transformers faiss-cpu
python search_md.py -q "Real-ESRGAN x4 CLI Rust"
python search_md.py -q "So s√°nh ResNet50 onnxruntime vs ort"
```

M·ªü r·ªông
- D√πng HNSW (faiss.IndexHNSWFlat) cho ch·ªâ m·ª•c l·ªõn.
- T·∫°o th√™m sub-sections theo ti√™u ƒë·ªÅ c·∫•p 3‚Äì4 ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c.
- Tr·∫£ v·ªÅ link ƒë·∫ßy ƒë·ªß `https://github.com/<repo>/blob/master/learning-ai.md{anchor}` ƒë·ªÉ click t·ª´ b·∫•t k·ª≥ ƒë√¢u.

### 12) ·ª®ng d·ª•ng theo Stack: Python, Rust, Node.js (c√¥ng c·ª•, v√≠ d·ª•, ngu·ªìn)

Python cho DA/DS/ML/LLM
- C√¥ng c·ª•: FastAPI, Polars, PyTorch/TF, Transformers, Sentence-Transformers, FAISS, MLflow, Airflow/Prefect, Pydantic, Uvicorn, Ray.
- Ngu·ªìn: [FastAPI](https://fastapi.tiangolo.com/), [Polars](https://pola.rs/), [PyTorch](https://pytorch.org/), [Transformers](https://huggingface.co/docs/transformers), [Sentence-Transformers](https://www.sbert.net/), [FAISS](https://github.com/facebookresearch/faiss), [MLflow](https://mlflow.org/)
- V√≠ d·ª• FastAPI + RAG t·ªëi gi·∫£n (Python): xem ph·∫ßn ph·ª• l·ª•c RAG ·ªü tr√™n; thay `FastAPI` route tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi t·ª´ retriever.

Rust cho hi·ªáu nƒÉng & h·ªá th·ªëng d·ªØ li·ªáu/ML
- C√¥ng c·ª•: Axum/Actix-web (API), Tokio (async), Polars/Arrow (x·ª≠ l√Ω b·∫£ng c·ªôt), ndarray, linfa (ML c·ªï ƒëi·ªÉn), `tch-rs` (binding PyTorch), `ort` (ONNX Runtime), `candle` (Rust DL), Serde (serde_json), anyhow/thiserror (error), tracing (observability).
- Ngu·ªìn: [Axum](https://docs.rs/axum/latest/axum/), [Actix-web](https://actix.rs/), [Tokio](https://tokio.rs/), [Polars Rust](https://pola-rs.github.io/polars-book/), [Arrow](https://arrow.apache.org/docs/), [linfa](https://rust-ml.github.io/linfa/), [tch-rs](https://github.com/LaurentMazare/tch-rs), [ONNX Runtime](https://github.com/microsoft/onnxruntime), [candle](https://github.com/huggingface/candle)
- V√≠ d·ª• API Axum infer ONNX (pseudocode):

```rust
use axum::{routing::post, Router, Json};
use serde::Deserialize;

#[derive(Deserialize)]
struct Features { x: Vec<f32> }

#[tokio::main]
async fn main() {
    let app = Router::new().route("/predict", post(predict));
    axum::Server::bind(&"0.0.0.0:8000".parse().unwrap()).serve(app.into_make_service()).await.unwrap();
}

async fn predict(Json(feat): Json<Features>) -> Json<f32> {
    // G·ªçi ONNX Runtime ho·∫∑c tch-rs ƒë·ªÉ suy lu·∫≠n
    Json(feat.x.iter().sum())
}
```

Node.js cho API, realtime v√† t√≠ch h·ª£p JS/TS
- C√¥ng c·ª•: Express/NestJS (API), TypeScript, Zod (schema), Prisma/TypeORM (ORM), BullMQ (queue), Socket.IO (realtime), `@xenova/transformers` (Transformers.js), TF.js, LangChain.js, `@grpc/grpc-js`.
- Ngu·ªìn: [NestJS](https://docs.nestjs.com/), [Express](https://expressjs.com/), [Transformers.js](https://huggingface.co/docs/transformers.js/index), [TensorFlow.js](https://www.tensorflow.org/js), [LangChain.js](https://js.langchain.com/docs/)
- V√≠ d·ª• NestJS route g·ªçi Transformers.js:

```ts
import { Controller, Get, Query } from '@nestjs/common';
import { pipeline } from '@xenova/transformers';

@Controller('nlp')
export class NlpController {
  @Get('sentiment')
  async sentiment(@Query('q') q: string) {
    const clf = await pipeline('text-classification', 'Xenova/distilbert-base-uncased-finetuned-sst-2-english');
    return await clf(q);
  }
}
```

So s√°nh nhanh
- Python: h·ªá sinh th√°i ML/LLM phong ph√∫, t·ªëc ƒë·ªô ph√°t tri·ªÉn nhanh, d·ªÖ th·ª≠ nghi·ªám.
- Rust: hi·ªáu nƒÉng cao, memory safety, build d·ªãch v·ª• inference t·ªëi ∆∞u.
- Node.js: t√≠ch h·ª£p web nhanh, TypeScript, h·ªá sinh th√°i frontend, prototyping API.

#### ƒê√°nh gi√° m·ª©c ƒë·ªô ph√°t tri·ªÉn Rust so v·ªõi Python (ƒë√£ l√†m/ch∆∞a l√†m)

- ƒê√£ l√†m ƒë∆∞·ª£c (Rust):
  - H·ªá sinh th√°i web/backend ·ªïn ƒë·ªãnh: Axum/Actix, Tokio, tower, tracing.
  - X·ª≠ l√Ω d·ªØ li·ªáu hi·ªáu nƒÉng: Polars (native), Arrow, parquet, IPC.
  - ML c·ªï ƒëi·ªÉn c∆° b·∫£n: `linfa` (logistic/regression, SVM, KMeans, v.v.) ƒë·ªß cho th·ª≠ nghi·ªám.
  - Suy lu·∫≠n m√¥ h√¨nh: binding `tch-rs` (PyTorch C++), `ort` (ONNX Runtime), `tract`, `candle` (ƒëang ph√°t tri·ªÉn nhanh), `ggml` bindings.
  - Dev tooling t·ªët: Cargo, clippy, rustfmt, cross-compilation, WASM, security/memory-safety.
  - Interop m·∫°nh: PyO3/maturin ƒë·ªÉ nh√∫ng Rust v√†o Python cho hot path, N-API (`napi-rs`) cho Node.
- Ch∆∞a m·∫°nh b·∫±ng Python:
  - Th∆∞ vi·ªán ML/DL s·∫£n xu·∫•t: ch∆∞a c√≥ t∆∞∆°ng ƒë∆∞∆°ng ƒë·∫ßy ƒë·ªß v·ªõi scikit-learn, PyTorch/TensorFlow ecosystem, Lightning, Hugging Face Trainer.
  - M√¥ h√¨nh, dataset, tutorial: √≠t t√†i nguy√™n, √≠t notebook m·∫´u; c·ªông ƒë·ªìng DS/LLM nh·ªè h∆°n ƒë√°ng k·ªÉ.
  - GPU/Accelerator: h·ªó tr·ª£ c√≤n ph√¢n m·∫£nh (CUDA/ROCm qua bindings), tooling profiling/visualization k√©m ti·ªán so v·ªõi Python.
  - Viz/Notebook: thi·∫øu stack data-viz v√† notebook thu·∫≠n ti·ªán nh∆∞ matplotlib/seaborn/plotly/Jupyter.
  - MLOps: √≠t SDK/clients ch√≠nh th·ª©c; t√≠ch h·ª£p MLflow/Weights & Biases, feature store c√≤n h·∫°n ch·∫ø.
- Khi n√™n ch·ªçn Rust:
  - D·ªãch v·ª• inference/ETL c·∫ßn hi·ªáu nƒÉng, ƒë·ªô tr·ªÖ th·∫•p, ki·ªÉm so√°t b·ªô nh·ªõ, ƒë√≥ng g√≥i nh·ªã ph√¢n.
  - Vi·∫øt extension tƒÉng t·ªëc cho pipeline Python (via PyO3) ho·∫∑c tri·ªÉn khai th√†nh microservice.
- Khi n√™n ∆∞u ti√™n Python:
  - Nghi√™n c·ª©u, th·ª≠ nghi·ªám nhanh, prototyping ML/LLM, hu·∫•n luy·ªán v√† ph√¢n t√≠ch d·ªØ li·ªáu.

#### D√πng Rust l√†m ch√≠nh, "b√π" ph·∫ßn thi·∫øu b·∫±ng Python nh∆∞ th·∫ø n√†o?

- C√°ch 1 ‚Äî Nh√∫ng Python in-process (PyO3):
  - Ph√π h·ª£p: c·∫ßn g·ªçi nhanh m·ªôt v√†i h√†m Python (numpy/sklearn/transformers) trong ƒë∆∞·ªùng n√≥ng nh·ªè.
  - L∆∞u √Ω: GIL h·∫°n ch·∫ø song song; qu·∫£n l√Ω m√¥i tr∆∞·ªùng Python (venv/conda/docker) ch·∫∑t; warmup interpreter.
- C√°ch 2 ‚Äî T√°ch microservice Python (HTTP/gRPC):
  - Ph√π h·ª£p: m√¥-ƒëun ML/LLM l·ªõn, c·∫ßn GPU, mu·ªën scale ƒë·ªôc l·∫≠p v√† quan s√°t t·ªët.
  - L∆∞u √Ω: th√™m ƒë·ªô tr·ªÖ m·∫°ng/serialization; c·∫ßn h·ª£p ƒë·ªìng API r√µ (OpenAPI/gRPC), theo d√µi p50/p95.
- C√°ch 3 ‚Äî Subprocess/CLI (JSON/Arrow):
  - Ph√π h·ª£p: batch/offline, cron, pipeline ETL ƒë∆°n gi·∫£n.
  - L∆∞u √Ω: latency kh·ªüi ƒë·ªông, qu·∫£n l√Ω timeout/retry.

- Thay th·∫ø Python ·ªü ƒë√¢u ƒë∆∞·ª£c ngay trong Rust:
  - Inference: `onnxruntime` (`ort`), `tch-rs` (libtorch), `candle` (n·∫øu m√¥ h√¨nh h·ªó tr·ª£).
  - X·ª≠ l√Ω d·ªØ li·ªáu: `polars`/Arrow, parquet.
  - ML c∆° b·∫£n: `linfa` n·∫øu ƒë·ªß nhu c·∫ßu.

- Ki·∫øn tr√∫c khuy·∫øn ngh·ªã:
  - Rust l√†m API/orchestrator hi·ªáu nƒÉng cao; Python t√°ch th√†nh service ML/LLM khi c·∫ßn GPU/h·ªá sinh th√°i.
  - N·∫øu y√™u c·∫ßu ƒë·ªô tr·ªÖ si√™u th·∫•p, lo·∫°i Python kh·ªèi hot path b·∫±ng ONNX/tch-rs/candle; ch·ªâ d√πng Python ·ªü n·ªÅn ph·ª• tr·ª£.


---

### 13) Interop ƒëa ng√¥n ng·ªØ: REST, gRPC, FFI

REST
- Chu·∫©n h√≥a schema b·∫±ng OpenAPI. Python (FastAPI) v√† Node (NestJS) ƒë·ªÅu generate spec t·ª± ƒë·ªông.
- Ngu·ªìn: [OpenAPI](https://www.openapis.org/)

gRPC
- ƒê·ªãnh nghƒ©a `.proto` ‚Üí generate client/server ƒëa ng√¥n ng·ªØ; ph√π h·ª£p n·ªôi b·ªô, low latency.
- Python: `grpcio`; Node: `@grpc/grpc-js`; Rust: `tonic`.
- Ngu·ªìn: [gRPC](https://grpc.io/), [tonic](https://docs.rs/tonic/latest/tonic/)

FFI
- Python ‚Üî Rust: PyO3/maturin export module Rust d√πng nh∆∞ package Python.
- Node ‚Üî Rust: N-API qua `napi-rs` ho·∫∑c Neon ƒë·ªÉ vi·∫øt native addon.
- Ngu·ªìn: [PyO3](https://pyo3.rs/), [maturin](https://www.maturin.rs/), [napi-rs](https://napi.rs/), [Neon](https://neon-bindings.com/)

V√≠ d·ª• FFI Rust ‚Üí Python (PyO3)

```rust
use pyo3::prelude::*;

#[pyfunction]
fn add(x: i32, y: i32) -> i32 { x + y }

#[pymodule]
fn myrustlib(_py: Python<'_>, m: &PyModule) -> PyResult<()> {
    m.add_function(wrap_pyfunction!(add, m)?)?;
    Ok(())
}
```

---

### 14) So s√°nh hi·ªáu nƒÉng ·ª©ng d·ª•ng (Python vs Rust vs Node.js)

M·ª•c ti√™u: ƒëo ƒë·ªô tr·ªÖ ph·∫£n h·ªìi p50/p95 v√† throughput cho c√πng t√°c v·ª• (hello-json, CPU-bound nh·ªè, I/O gi·∫£ l·∫≠p) tr√™n nhi·ªÅu c·∫•u h√¨nh VPS.

C·∫•u h√¨nh VPS (profile)
- 1 CPU: 1 GB RAM (P1), 2 GB (P2), 4 GB (P3)
- 2 CPU: 2 GB (P4), 4 GB (P5), 8 GB (P6)
- 4 CPU: 4 GB (P7), 8 GB (P8), 16 GB (P9)

Workloads chu·∫©n h√≥a
- W1: Hello JSON (tr·∫£ v·ªÅ `{status: ok}`) ‚Äî network + serialization.
- W2: CPU nh·ªè (t√≠nh t·ªïng b√¨nh ph∆∞∆°ng 1..N, N=1e5) ‚Äî CPU-bound.
- W3: I/O gi·∫£ l·∫≠p (sleep 5ms) ‚Äî concurrency/async.

Thi·∫øt l·∫≠p m√°y ch·ªß
- Python: FastAPI + Uvicorn (workers = s·ªë CPU), `uvicorn app:app --workers $CPU --loop uvloop --http httptools`.
- Rust: Axum (tokio multi-thread, `RUSTFLAGS='-C target-cpu=native'`), build `--release`.
- Node.js: NestJS/Express (node 20+), cluster (PM2/Node cluster, workers = s·ªë CPU).

Sinh t·∫£i (kh√¥ng tr·∫°ng th√°i)
- D√πng `oha` ho·∫∑c `wrk`/`bombardier`. V√≠ d·ª•: `oha -z 30s -c 100 http://HOST:PORT/w1`.
- L·∫∑p v·ªõi 3 c·ª° batch: 100, 1_000, 1_000_000 requests (c√≥ th·ªÉ c·∫ßn th·ªùi gian d√†i/chia l√¥ cho 1_000_000).

Endpoint m·∫´u ƒë·ªìng nh·∫•t

Python (FastAPI)
```python
from fastapi import FastAPI
import time
app = FastAPI()

@app.get('/w1')
def w1():
    return {'status': 'ok'}

@app.get('/w2')
def w2():
    return {'sum': sum(i*i for i in range(100_000))}

@app.get('/w3')
async def w3():
    time.sleep(0.005)
    return {'done': True}
```

Rust (Axum)
```rust
use axum::{routing::get, Router};
use std::time::Duration;
use std::thread::sleep;

async fn w1() -> axum::Json<serde_json::Value> {
    axum::Json(serde_json::json!({"status":"ok"}))
}

async fn w2() -> axum::Json<serde_json::Value> {
    let s: u64 = (0..100_000).map(|i| i*i).sum();
    axum::Json(serde_json::json!({"sum": s}))
}

async fn w3() -> axum::Json<serde_json::Value> {
    sleep(Duration::from_millis(5));
    axum::Json(serde_json::json!({"done": true}))
}

#[tokio::main]
async fn main() {
    let app = Router::new().route("/w1", get(w1)).route("/w2", get(w2)).route("/w3", get(w3));
    axum::Server::bind(&"0.0.0.0:8000".parse().unwrap()).serve(app.into_make_service()).await.unwrap();
}
```

Node.js (Express)
```js
const express = require('express');
const app = express();

app.get('/w1', (_, res) => res.json({ status: 'ok' }));
app.get('/w2', (_, res) => {
  let s = 0; for (let i = 0; i < 100000; i++) s += i*i; res.json({ sum: s });
});
app.get('/w3', async (_, res) => { setTimeout(() => res.json({ done: true }), 5); });

app.listen(8000);
```

C√°ch ƒëo v√† b√°o c√°o
- Ch·∫°y 3 l·∫ßn/ƒëi·ªÅu ki·ªán, l·∫•y trung b√¨nh p50/p95 latency v√† req/s (throughput).
- Ghi nh·∫≠n CPU/RAM (top, vmstat/iostat), netstat (k·∫øt n·ªëi) v√† l·ªói (5xx).
- B√°o c√°o theo ma tr·∫≠n: (ng√¥n ng·ªØ √ó profile √ó workload √ó batch-size).

K·ª≥ v·ªçng k·∫øt qu·∫£ tham kh·∫£o (c√≥ th·ªÉ kh√°c t√πy tri·ªÉn khai)
- W1 (hello-json): Rust th∆∞·ªùng c√≥ latency th·∫•p nh·∫•t (p50 ~ 0.4‚Äì1.2 ms P1), Node/Python nh·ªânh h∆°n (Node p50 ~ 0.8‚Äì2.0 ms; Python p50 ~ 1.0‚Äì3.0 ms). Throughput tƒÉng g·∫ßn tuy·∫øn t√≠nh theo CPU n·∫øu kh√¥ng b·ªã gi·ªõi h·∫°n m·∫°ng.
- W2 (CPU-bound): Rust d·∫´n ƒë·∫ßu r√µ r·ªát; Node/Python b·ªã GIL/JS engine ·∫£nh h∆∞·ªüng; Python c·∫£i thi·ªán khi d√πng ƒëa ti·∫øn tr√¨nh (gunicorn/uvicorn workers) nh∆∞ng v·∫´n k√©m Rust. p95 Python/Node tƒÉng nhanh khi y√™u c·∫ßu tƒÉng.
- W3 (I/O 5 ms): Node v√† Python async c√≥ th·ªÉ ti·ªám c·∫≠n nhau; Rust gi·ªØ ·ªïn ƒë·ªãnh p95 h∆°n khi c·∫°nh tranh t√†i nguy√™n. Throughput ch·ªß y·∫øu ph·ª• thu·ªôc v√†o th·ªùi gian sleep v√† concurrency.

L·ªánh v√≠ d·ª• v·ªõi `oha`
```bash
oha -z 30s -c 100 http://HOST:8000/w1
oha -z 30s -c 200 http://HOST:8000/w2
oha -z 30s -c 500 http://HOST:8000/w3
```

Ngu·ªìn & c√¥ng c·ª• benchmark tham kh·∫£o: [oha](https://github.com/hatoo/oha), [wrk](https://github.com/wg/wrk), [bombardier](https://github.com/codesenberg/bombardier), [hey](https://github.com/rakyll/hey).

#### Benchmark: ResNet50 Inference (ONNX) ‚Äî Python (onnxruntime) vs Rust (`ort`)

- M·ª•c ti√™u: so s√°nh p50/p95 latency, throughput (RPS), RSS memory, CPU% (v√† GPU n·∫øu d√πng GPU provider) cho c√πng m√¥ h√¨nh ONNX.
- Thi·∫øt l·∫≠p chung: c√πng m√°y, c√πng phi√™n b·∫£n ONNX Runtime, batch=1, ·∫£nh/seed c·ªë ƒë·ªãnh, warmup 100 req.
- Python: FastAPI + `onnxruntime` (CPU/GPU); Rust: Axum + crate `ort` (bindings ONNX Runtime).
- Ti·ªÅn x·ª≠ l√Ω chu·∫©n ResNet50: resize 256 ‚Üí center-crop 224, normalize mean/std, NCHW fp32.
- L·ªánh ƒëo: d√πng `oha` v·ªõi body l√† JSON `{image_b64: ...}` v√†o `/infer` c·ªßa m·ªói server; theo d√µi t√†i nguy√™n b·∫±ng `pidstat`, `top`, `nvidia-smi dmon`.
- K·ª≥ v·ªçng: CPU ch√™nh l·ªách ch·ªß y·∫øu do HTTP stack/JSON/ti·ªÅn x·ª≠ l√Ω; GPU th∆∞·ªùng s√°t nhau v√¨ bottleneck ·ªü runtime/GPU.

M·∫´u tri·ªÉn khai t·ªëi thi·ªÉu ƒë√£ g·ª£i √Ω ·ªü ph·∫ßn tr·∫£ l·ªùi: "ƒê·ªÅ xu·∫•t d·ª± √°n so s√°nh: ResNet50 Inference (ONNX) ‚Äî Python vs Rust".

Bi·ªÉu ƒë·ªì & tr·ª±c quan ho√° b·∫Øt bu·ªôc:
- Bi·ªÉu ƒë·ªì c·ªôt: so s√°nh p50/p95 latency (ms) gi·ªØa Python vs Rust.
- Bi·ªÉu ƒë·ªì c·ªôt: so s√°nh throughput (RPS).
- Bi·ªÉu ƒë·ªì ƒë∆∞·ªùng/violin: ph√¢n ph·ªëi latency, highlight p50/p95/p99.
- B·∫£ng ph·ª•: RSS (MB), CPU% trung b√¨nh, GPU util/mem (n·∫øu d√πng GPU).

V√≠ d·ª• plotting (Python, matplotlib):
```python
import matplotlib.pyplot as plt

data = {
    'Python': {'p50_ms': 2.4, 'p95_ms': 4.8, 'rps': 12000, 'rss_mb': 210},
    'Rust':   {'p50_ms': 1.9, 'p95_ms': 3.7, 'rps': 13500, 'rss_mb': 160},
}
langs = list(data.keys())
p50 = [data[k]['p50_ms'] for k in langs]
p95 = [data[k]['p95_ms'] for k in langs]
rps = [data[k]['rps'] for k in langs]

fig, axs = plt.subplots(1, 2, figsize=(8,3))
axs[0].bar(langs, p50, label='p50')
axs[0].bar(langs, [p95[i]-p50[i] for i in range(len(p50))], bottom=p50, label='p50‚Üíp95')
axs[0].set_title('Latency (ms)'); axs[0].legend()
axs[1].bar(langs, rps, color=['tab:orange','tab:green']); axs[1].set_title('RPS')
plt.tight_layout(); plt.show()
```

#### Benchmark 4K Image Processing (Super-Resolution/Detection) ‚Äî Python vs Rust

- B√†i to√°n: x·ª≠ l√Ω ·∫£nh 4K (3840√ó2160) ‚Äî so s√°nh hai k·ªãch b·∫£n ph·ªï bi·∫øn:
  - Super-resolution (ESRGAN/Real-ESRGAN/FSRCNN) d√πng ONNX Runtime.
  - Object detection (YOLOv5/YOLOv8 ONNX) tr√™n ·∫£nh 4K, c√≥ th·ªÉ chia tile ƒë·ªÉ t·ªëi ∆∞u b·ªô nh·ªõ.
- C√¥ng c·ª•:
  - Python: `onnxruntime`, `opencv-python`, `numpy`, (tu·ª≥ ch·ªçn) `onnxruntime-gpu`.
  - Rust: `ort` (ONNX Runtime), `image`/`opencv` crate, `rayon` cho song song h√≥a tile, `ndarray`.
- Thi·∫øt k·∫ø benchmark:
  - ·∫¢nh ƒë·∫ßu v√†o 4K c·ªë ƒë·ªãnh; ƒëo 2 pipeline: (a) full-frame; (b) tile-based (v√≠ d·ª• 4√ó4 ho·∫∑c 8√ó8 tile ch·ªìng l·ªÅ 16 px ƒë·ªÉ tr√°nh seam) r·ªìi gh√©p.
  - ƒêo p50/p95 latency, throughput (batch=1), RSS, CPU%, n·∫øu GPU: GPU util/mem.
  - Th·ª≠ nhi·ªÅu c·ª° tile (512, 640, 960) ƒë·ªÉ t√¨m ƒëi·ªÉm c√¢n b·∫±ng gi·ªØa b·ªô nh·ªõ v√† chi ph√≠ gh√©p.
- T·ªëi ∆∞u ho√° g·ª£i √Ω:
  - Python: b·∫≠t `intra_op_num_threads` t∆∞∆°ng ·ª©ng s·ªë core; d√πng numpy/opencv thao t√°c m·∫£ng; v·ªõi GPU: ch·ªçn provider CUDA/DirectML.
  - Rust: build `--release`, `RUSTFLAGS='-C target-cpu=native'`; d√πng `rayon` ƒë·ªÉ ti·ªÅn x·ª≠ l√Ω/gh√©p tile song song; tr√°nh copy kh√¥ng c·∫ßn thi·∫øt.
  - I/O: d√πng `mmap`/stream decode khi c·∫ßn; ∆∞u ti√™n ƒë·ªãnh d·∫°ng n√©n nh·∫π ƒë·ªÉ gi·∫£m decode time n·∫øu I/O l√† bottleneck.
- K·ª≥ v·ªçng:
  - CPU: Rust c√≥ l·ª£i th·∫ø v·ªÅ ki·ªÉm so√°t b·ªô nh·ªõ v√† song song ho√° tile tinh g·ªçn ‚Üí p95 ·ªïn ƒë·ªãnh h∆°n ·ªü ƒë·ªô ph√¢n gi·∫£i 4K; Python s√°t n√∫t n·∫øu ph·∫ßn l·ªõn th·ªùi gian n·∫±m trong ONNX Runtime/C++ op.
  - GPU: g·∫ßn t∆∞∆°ng ƒë∆∞∆°ng; kh√°c bi·ªát ch·ªß y·∫øu do b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω/h·∫≠u x·ª≠ l√Ω v√† pipeline gh√©p tile.

G·ª£i √Ω m√¥ h√¨nh ONNX:
- Super-resolution: Real-ESRGAN/FSRCNN ONNX (nhi·ªÅu repo c√≥ weights ONNX c√¥ng khai).
- Detection: YOLOv5s/v8s ONNX (ƒë·ªß nh·∫π cho CPU, GPU c√†ng t·ªët).

Bi·ªÉu ƒë·ªì & tr·ª±c quan ho√° b·∫Øt bu·ªôc:
- Bi·ªÉu ƒë·ªì c·ªôt nh√≥m: FPS v√† latency p50/p95 cho c√°c c·∫•u h√¨nh (full-frame, tile 2√ó2, tile 4√ó4) theo t·ª´ng ng√¥n ng·ªØ.
- Bi·ªÉu ƒë·ªì ƒë∆∞·ªùng: m·ªëi quan h·ªá k√≠ch th∆∞·ªõc tile (px) ‚Üî latency/RSS.
- Stack bar: ph√¢n r√£ th·ªùi gian theo giai ƒëo·∫°n (decode, preprocess, infer, postprocess, encode).
- B·∫£ng ph·ª•: dropped frames, jitter (p99 - p50), GPU util/mem.

V√≠ d·ª• plotting ph√¢n r√£ th·ªùi gian:
```python
import numpy as np, matplotlib.pyplot as plt
stages = ['decode','preproc','infer','postproc','encode']
py = np.array([3.1, 2.0, 8.4, 1.2, 1.0])
rs = np.array([2.6, 1.4, 8.1, 0.8, 0.9])
width = 0.35
x = np.arange(len(stages))
fig, ax = plt.subplots(figsize=(7,3))
ax.bar(x - width/2, py, width, label='Python')
ax.bar(x + width/2, rs, width, label='Rust')
ax.set_xticks(x); ax.set_xticklabels(stages)
ax.set_ylabel('ms/frame'); ax.set_title('Per-stage time @4K Tile 2√ó2')
ax.legend(); plt.tight_layout(); plt.show()
```

V√≠ d·ª• FFI Rust ‚Üí Node (N-API)

```rust
use napi::bindgen_prelude::*;

#[napi]
pub fn dot(a: Vec<f32>, b: Vec<f32>) -> f32 {
  a.iter().zip(b.iter()).map(|(x,y)| x*y).sum()
}
```

### Ph·ª• l·ª•c: V√≠ d·ª• c·ª• th·ªÉ cho t·ª´ng ch·ªß ƒë·ªÅ (runnable snippets)

Python c∆° b·∫£n

```python
from dataclasses import dataclass

@dataclass
class Order:
    id: int
    total: float
    is_vip: bool = False

orders = [Order(i, total=i * 10.0, is_vip=(i % 2 == 0)) for i in range(1, 6)]
vip_totals = [o.total for o in orders if o.is_vip]
assert sum(vip_totals) == 20.0 + 40.0
```

pytest t·ªëi thi·ªÉu

```python
def add(a, b):
    return a + b

def test_add():
    assert add(2, 3) == 5
```

NumPy: broadcasting v√† vector h√≥a

```python
import numpy as np
x = np.arange(9).reshape(3, 3)  # [[0,1,2],[3,4,5],[6,7,8]]
w = np.array([0.2, 0.3, 0.5])
y = (x * w).sum(axis=1)
assert y.shape == (3,)
```

Pandas: groupby, resample, merge

```python
import pandas as pd
df = pd.DataFrame({
    'date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-08']),
    'store': ['A', 'A', 'A'],
    'sales': [10, 20, 30]
})
weekly = df.set_index('date').groupby('store').resample('W')['sales'].sum().reset_index()

users = pd.DataFrame({'user_id': [1, 2], 'segment': ['A', 'B']})
events = pd.DataFrame({'user_id': [1, 1, 2], 'amount': [5, 7, 3]})
agg = events.groupby('user_id', as_index=False)['amount'].sum()
joined = agg.merge(users, on='user_id', how='left')
```

Tr·ª±c quan h√≥a: trung b√¨nh theo nh√≥m + CI

```python
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
tips = sns.load_dataset('tips')
sns.barplot(data=tips, x='day', y='total_bill', estimator=np.mean, errorbar=('ci', 95))
plt.tight_layout()
```

SQL: CTE + window function (cumulative sum)

```sql
WITH orders AS (
  SELECT 1 AS user_id, DATE '2025-01-01' AS dt, 100 AS amount UNION ALL
  SELECT 1, DATE '2025-01-02', 50 UNION ALL
  SELECT 2, DATE '2025-01-02', 70
),
daily AS (
  SELECT user_id, dt, SUM(amount) AS daily_amount
  FROM orders
  GROUP BY user_id, dt
)
SELECT user_id,
       dt,
       daily_amount,
       SUM(daily_amount) OVER (PARTITION BY user_id ORDER BY dt
         ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cum_amount
FROM daily
ORDER BY user_id, dt;
```

Th·ªëng k√™: ki·ªÉm ƒë·ªãnh t ƒë·ªôc l·∫≠p (Welch)

```python
from scipy import stats
a = [10, 11, 9, 12, 10]
b = [12, 13, 11, 14, 13]
t, p = stats.ttest_ind(a, b, equal_var=False)
assert p < 0.1
```

Gradient descent tuy·∫øn t√≠nh t·ªëi gi·∫£n

```python
import numpy as np
X = np.c_[np.ones(3), [1, 2, 3]]
y = np.array([2, 4, 6])
theta = np.zeros(2)
alpha = 0.1
for _ in range(1000):
    grad = X.T @ (X @ theta - y) / len(y)
    theta -= alpha * grad
assert np.allclose(theta, np.array([0.0, 2.0]), atol=1e-2)
```

DA: t√≠nh c·ª° m·∫´u A/B (x·∫•p x·ªâ)

```python
from scipy import stats
from statsmodels.stats.power import NormalIndPower
p1 = 0.10
lift = 0.05
effect = stats.proportion_effectsize(p1, p1 + lift)
n_per_group = NormalIndPower().solve_power(effect_size=effect, power=0.8, alpha=0.05, ratio=1.0)
```

Sklearn: pipeline ti·ªÅn x·ª≠ l√Ω + Logistic Regression

```python
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

X = pd.DataFrame({'age': [20, 30, 40, 25], 'city': ['HCM', 'HN', 'DN', 'HCM']})
y = [0, 1, 1, 0]
pre = ColumnTransformer([
    ('num', StandardScaler(), ['age']),
    ('cat', OneHotEncoder(handle_unknown='ignore'), ['city'])
])
clf = Pipeline([('pre', pre), ('model', LogisticRegression(max_iter=2000))]).fit(X, y)
```

ƒê√°nh gi√°: cross-validation c√≥ stratify

```python
from sklearn.model_selection import StratifiedKFold, cross_val_score
scores = cross_val_score(clf, X, y, cv=StratifiedKFold(5, shuffle=True, random_state=42), scoring='roc_auc')
print(scores.mean())
```

Unsupervised: PCA + KMeans

```python
import numpy as np
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
Xu = np.random.RandomState(0).rand(200, 5)
X2 = PCA(n_components=2, random_state=0).fit_transform(Xu)
labels = KMeans(n_clusters=3, n_init=10, random_state=0).fit_predict(X2)
```

Imbalance: SMOTE

```python
from imblearn.over_sampling import SMOTE
X_res, y_res = SMOTE().fit_resample(X2, labels)
```

Explainability: SHAP v·ªõi XGBoost

```python
import numpy as np
import shap, xgboost as xgb
Xb = np.random.rand(300, 4)
yb = (Xb[:, 0] + Xb[:, 1] > 1.0).astype(int)
model = xgb.XGBClassifier(n_estimators=50, max_depth=3, eval_metric='logloss').fit(Xb, yb)
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(Xb[:5])
```

Time Series: backtest ƒë∆°n gi·∫£n v·ªõi ARIMA

```python
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
s = pd.Series([100, 120, 130, 140, 160], index=pd.date_range('2024-01-01', periods=5, freq='D'))
preds = []
for t in range(3, len(s)):
    train = s.iloc[:t]
    m = ARIMA(train, order=(1, 1, 0)).fit()
    preds.append(m.forecast(1).iloc[0])
```

PyTorch: MLP t·ªëi thi·ªÉu

```python
import torch
import torch.nn as nn
import torch.optim as optim
X = torch.randn(256, 20)
y = (X[:, 0] + X[:, 1] > 0).long()
model = nn.Sequential(nn.Linear(20, 64), nn.ReLU(), nn.Linear(64, 2))
opt = optim.Adam(model.parameters(), lr=1e-3)
loss_fn = nn.CrossEntropyLoss()
for _ in range(100):
    opt.zero_grad()
    loss = loss_fn(model(X), y)
    loss.backward()
    opt.step()
```

CV: Transfer learning ResNet18

```python
import torch
from torchvision import models
model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
for p in model.parameters():
    p.requires_grad = False
model.fc = torch.nn.Linear(model.fc.in_features, 10)  # fine-tune head
```

NLP: Inference sentiment v·ªõi Transformers

```python
from transformers import pipeline
clf = pipeline('text-classification', model='distilbert-base-uncased-finetuned-sst-2-english')
print(clf('This course is great!'))
```

RAG t·ªëi gi·∫£n: FAISS + Sentence-Transformers + generation (Flan-T5)

```python
from sentence_transformers import SentenceTransformer
import faiss, numpy as np
from transformers import pipeline

docs = [
    'AI uses vector search for retrieval.',
    'Pandas is a Python library for data analysis.',
    'FAISS enables efficient similarity search over embeddings.'
]
emb = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
vecs = emb.encode(docs, normalize_embeddings=True)
index = faiss.IndexFlatIP(vecs.shape[1])
index.add(vecs)
query = 'Which tool is for similarity search?'
qv = emb.encode([query], normalize_embeddings=True)
D, I = index.search(qv, k=2)
context = ' '.join(docs[i] for i in I[0])
gen = pipeline('text2text-generation', model='google/flan-t5-small')
prompt = f'Answer using only the context. Context: {context} Question: {query}'
print(gen(prompt, max_new_tokens=50)[0]['generated_text'])
```

FastAPI: ph·ª•c v·ª• model sklearn

```python
from fastapi import FastAPI
import joblib, pandas as pd
app = FastAPI()
model = joblib.load('model.joblib')

@app.post('/predict')
def predict(features: dict):
    X = pd.DataFrame([features])
    proba = float(model.predict_proba(X)[:, 1][0])
    return {'probability': proba}
```

MLflow: log tham s·ªë v√† metric

```python
import mlflow
with mlflow.start_run():
    mlflow.log_param('model', 'logreg')
    mlflow.log_metric('auc', 0.91)
```

Evidently: b√°o c√°o drift nhanh

```python
import pandas as pd
from evidently.report import Report
from evidently.metrics import DataDriftPreset
ref = pd.DataFrame({'x': [1, 2, 3, 4]})
cur = pd.DataFrame({'x': [4, 5, 6, 7]})
report = Report(metrics=[DataDriftPreset()])
report.run(reference_data=ref, current_data=cur)
report.save_html('drift_report.html')
```

